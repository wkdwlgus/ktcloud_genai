{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuRpYQ3c15Ke+oXxB5X9pD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wkdwlgus/ktcloud_genai/blob/main/251021_%EA%B3%A0%EA%B8%89_%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8_%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81_%EC%9E%A5%EC%A7%80%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **프롬프트 엔지니어링**"
      ],
      "metadata": {
        "id": "yp-AQkTjxp64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **프롬프트 엔지니어링 개요**\n",
        "\n",
        "- **텍스트 표현**에 초점을 맞춘 모델\n",
        "    - BERT나 BERT 파생 모델\n",
        "- **텍스트 생성**에 초점을 맞춘 모델\n",
        "    - **GPT 계열** 모델\n",
        "    - 사용자의 **프로프트(prompt)에 대한 응답으로 텍스트 생성**\n",
        "- **프롬프트 엔지니어링**(Prompt Engineering)\n",
        "    - <mark>**생성된 텍스트의 품질을 향상시키기 위해 프롬프트를 설계하는 방법**\n",
        "\n",
        "- **목표**:\n",
        "    - <mark>프롬프트를 세심하게 설계하여 LLM이 원하는 응답을 하도록 유도할 수 있다.\n",
        "- **기억할 점**\n",
        "    - 프롬프트 최적화는 반복적인 과정이며 실험이 필요하다.\n",
        "    - 완벽한 프롬프트는 설계는 없으며 앞으로도 가능하지 않다.\n"
      ],
      "metadata": {
        "id": "B4ULIQMFRYF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **프롬프트의 기본 구성 요소**\n",
        "\n",
        "1. **지시사항/명령어** (Instruction)\n",
        "    - 프롬프트의 핵심으로, AI에게 무엇을 해야 하는지 명확하게 전달합니다\n",
        "    - 모호한 표현보다는 구체적이고 직접적인 동사를 사용하는 것이 효과적입니다\n",
        "    - \"해줘\", \"만들어줘\", \"분석해줘\", \"요약해줘\" 등의 명확한 동사 사용\n",
        "2. **입력 데이터** (Input Data)\n",
        "    - 실제로 처리되어야 할 원본 데이터입니다\n",
        "    - 구조화된 데이터(JSON, CSV)나 비구조화된 데이터(텍스트, 이미지) 모두 가능합니다\n",
        "3. **출력 지시어** (Output Format)\n",
        "    - 응답의 구조를 미리 지정하면 일관성 있는 결과를 얻을 수 있습니다\n",
        "    - 특히 프로그래밍에서 파싱이 필요한 경우 매우 중요합니다"
      ],
      "metadata": {
        "id": "WL_3HbQRHyxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **지시 기반 프롬프트**\n",
        "\n",
        "- 지시 기반 프롬프트란?\n",
        "    - 특정 질문에 답변하거나 특정 작업을 해결하기 위해 사용하는 프롬프트\n",
        "    - 작업마다 다른 지시가 필요하지만 출력 품질을 향상시키기 위한 프롬프트 기술에는 공통점이 있다.\n",
        "\n",
        "- **출력 품질 향상을 위한 기술**\n",
        "    - **구체성** : 원하는 바를 정확히/구체적으로 기술하세요.\n",
        "    - **환각 대응책** : LLM에게 답변을 알 때만 생성하라고 요청할 수 있습니다. 답을 알지 못하면 `답을 모른다`라고 출력할하라고 구체적으로 지시한다.\n",
        "    - **순서** : 프롬프트 시작이나 끝에 지시 사항을 전달한다. 특히, 긴 프롬프트의 경우 중간에 있는 정보가 잊힐 수 있다."
      ],
      "metadata": {
        "id": "6Cs4_8STKFUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **생성 모델 사용하기**"
      ],
      "metadata": {
        "id": "UldzT7hWLhUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "# 1. 일반적인 Python 경고(DeprecationWarning 등) 숨기기\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# 2. Transformers 라이브러리 로그 수준 조절 (Warning 이하는 숨기기)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "UwHRcv9Zde3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **필요 라이브러리 설치**"
      ],
      "metadata": {
        "id": "EpV2uvnPlgkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이썬 버전 확인(3.12)\n",
        "!python --version"
      ],
      "metadata": {
        "id": "6jBSq5FKdprY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07cfe37-13ad-4f7a-bb24-01e511ebcb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA 버전 확인 (12.5)\n",
        "!nvcc --version | grep cuda_"
      ],
      "metadata": {
        "id": "5ZcvURivdpyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2737fb2d-f6c0-4c15-8d63-9732a15f6e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- llama-cpp-python\n",
        "    - https://github.com/abetlen/llama-cpp-python/releases\n",
        "    - llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl\n",
        "- transformers\n",
        "- 사용 모델\n",
        "    - microsoft/Phi-3-mini-4k-instruct\n"
      ],
      "metadata": {
        "id": "uSpY0d9RlaGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRhw6oeA-yro"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# 사용하는 파이썬과 CUDA 버전에 맞는 llama-cpp-python 패키지를 설치하세요.\n",
        "# !pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl\n",
        "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUKDSu2MLEKe",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee32200-26bb-4620-8868-e614ec8ab360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.3)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.10.5)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.48.3\n"
          ]
        }
      ],
      "source": [
        "# Phi-3 모델과 호환성 때문에 transformers 4.48.3 버전을 사용합니다.\n",
        "!pip install transformers==4.48.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate tokenizers datasets safetensors"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-BUWHelAhvWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f41c418-27f1-46ec-9737-eade4d54f3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.21.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
        "import os\n",
        "import tqdm\n",
        "from transformers.utils import logging\n",
        "\n",
        "# tqdm 비활성화\n",
        "tqdm.tqdm = lambda *args, **kwargs: iter([])\n",
        "tqdm.auto.tqdm = lambda *args, **kwargs: iter([])\n",
        "tqdm.notebook.tqdm = lambda *args, **kwargs: iter([])\n",
        "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "CsRRyH_f8jTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **텍스트 생성 모델 선택하기**"
      ],
      "metadata": {
        "id": "Nw5Msk5JXcAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **AI 모델 분류**\n",
        "- **AI 모델 분류 발전사***\n",
        "|시기|주요 사건|사용된 용어|배경|\n",
        "|---|---|---|---|\n",
        "| 2012-2017| 딥러닝 초기| \"Proprietary Model\"| 기업들이 자체 모델 개발|\n",
        "| 2018-2019| BERT, GPT-2 등장| \"Open vs Closed\"| OpenAI가 GPT-2 단계적 공개|\n",
        "| 2020-2022| **GPT-3, Claude**| \"Closed-source Model\"| **API 접근만 제공하는 모델** 증가|\n",
        "| 2023-현재| **LLaMA, Mistral 등**| \"Open-weight Model\"| **가중치는 공개, 데이터는 비공개**|\n",
        "- 참고\n",
        "    - OpenAI GPT-2 Release Strategy (2019): https://openai.com/research/gpt-2-1-5b-release\n",
        "    - Meta LLaMA Release (2023): https://ai.meta.com/llama/\n",
        "\n"
      ],
      "metadata": {
        "id": "S8bLRDZOSl_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **모델 분류 유형**\n",
        "|용어|한국어|의미|공개 범위|예시|\n",
        "|---|---|---|---|---|\n",
        "|Closed-source Model|폐쇄형 모델|모델 가중치, 아키텍처, 학습 데이터 모두 비공개|API만 제공|GPT-4, Claude, Gemini|\n",
        "|Proprietary Model|독점 모델|상업적 소유권이 있는 모델 (일부는 가중치 공개 가능)|다양함|GPT-4, Claude (완전 비공개)|\n",
        "|Open-source Model|오픈소스 모델|코드, 가중치, 학습 방법 모두 공개|완전 공개|Pythia, OLMo, BLOOM|\n",
        "|Open-weight Model|오픈 가중치 모델|가중치는 공개, 학습 데이터/방법은 비공개|가중치만 공개|LLaMA, Mistral, Qwen|\n",
        "|Open API Model|오픈 API 모델|API로만 접근 가능하지만 무료 또는 저렴|API만|GPT-3.5 (무료 버전)"
      ],
      "metadata": {
        "id": "mDP4HQswWhZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 각 **모델 유형의 특징**\n",
        "|특징|Open-source|Open-weight|Closed-source|\n",
        "|---|---|---|---|\n",
        "| 모델 가중치| ✅ 공개| ✅ 공개| ❌ 비공개|\n",
        "| 학습 코드| ✅ 공개| ⚠️ 부분 공개| ❌ 비공개|\n",
        "| 학습 데이터| ✅ 공개| ❌ 비공개| ❌ 비공개|\n",
        "| 아키텍처| ✅ 공개| ✅ 공개| ⚠️ 부분 공개|\n",
        "| 로컬 실행| ✅ 가능| ✅ 가능| ❌ 불가능|\n",
        "| 상업적 사용| ✅ 가능 (라이선스에 따라)| ⚠️ 제한적| ❌ API 요금|\n",
        "| 모델 수정| ✅ 가능| ✅ 가능| ❌ 불가능|\n",
        "| 재현 가능성| ✅ 높음| ⚠️ 중간| ❌ 불가능|\n"
      ],
      "metadata": {
        "id": "Xuv88yOtVQj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **(오픈 소스 모델)파운데이션 모델**(Foundation model)\n",
        "    - 이런 LLM은 대규모 덱스트 데이터에서 사전 훈련되며 특정 애플리케이션을 위해 미세 튜닝된다.\n",
        "    - 미세 튜닝된 모델이 수백개 이상 됨\n",
        "    - 특정 작업에 잘 맞\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ibK55HC56skczaF9saU9FKlIR6BiZdqT\" width=\"80%\">"
      ],
      "metadata": {
        "id": "h3j7Yr1gTMk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **예제 : 라이선스별 사용 가능 범위 비교**"
      ],
      "metadata": {
        "id": "n12c7GTOft9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 예제: 라이선스 비교 및 적합성 판단\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"⚖️ AI 모델 라이선스 비교\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class LicenseChecker:\n",
        "    \"\"\"라이선스 적합성 검사기\"\"\"\n",
        "\n",
        "    licenses = {\n",
        "        \"Apache 2.0\": {\n",
        "            \"commercial\": True,\n",
        "            \"modify\": True,\n",
        "            \"redistribute\": True,\n",
        "            \"restrictions\": [],\n",
        "            \"models\": [\"Mistral 7B\", \"Qwen 2.5\", \"SOLAR\"]\n",
        "        },\n",
        "        \"LLaMA 2 License\": {\n",
        "            \"commercial\": True,\n",
        "            \"modify\": True,\n",
        "            \"redistribute\": True,\n",
        "            \"restrictions\": [\"MAU > 700M requires special license\"],\n",
        "            \"models\": [\"LLaMA 2\", \"LLaMA 3\"]\n",
        "        },\n",
        "        \"Gemma License\": {\n",
        "            \"commercial\": True,\n",
        "            \"modify\": True,\n",
        "            \"redistribute\": True,\n",
        "            \"restrictions\": [\"Cannot compete with Google services\"],\n",
        "            \"models\": [\"Gemma\", \"Gemma 2\"]\n",
        "        },\n",
        "        \"GPT-4 API\": {\n",
        "            \"commercial\": True,\n",
        "            \"modify\": False,\n",
        "            \"redistribute\": False,\n",
        "            \"restrictions\": [\"API usage only\", \"Rate limits\", \"Cost per token\"],\n",
        "            \"models\": [\"GPT-4\", \"GPT-4 Turbo\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def check_use_case(license_name, use_case):\n",
        "        \"\"\"사용 사례에 대한 라이선스 적합성 검사\"\"\"\n",
        "\n",
        "        license_info = LicenseChecker.licenses.get(license_name)\n",
        "        if not license_info:\n",
        "            return \"Unknown license\"\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # 상업적 사용\n",
        "        if \"commercial\" in use_case.lower():\n",
        "            if license_info[\"commercial\"]:\n",
        "                results.append(\"✅ 상업적 사용 가능\")\n",
        "            else:\n",
        "                results.append(\"❌ 상업적 사용 불가\")\n",
        "\n",
        "        # 모델 수정\n",
        "        if \"modify\" in use_case.lower() or \"fine-tun\" in use_case.lower():\n",
        "            if license_info[\"modify\"]:\n",
        "                results.append(\"✅ 모델 수정/파인튜닝 가능\")\n",
        "            else:\n",
        "                results.append(\"❌ 모델 수정 불가\")\n",
        "\n",
        "        # 재배포\n",
        "        if \"redistribute\" in use_case.lower():\n",
        "            if license_info[\"redistribute\"]:\n",
        "                results.append(\"✅ 모델 재배포 가능\")\n",
        "            else:\n",
        "                results.append(\"❌ 모델 재배포 불가\")\n",
        "\n",
        "        # 제약사항\n",
        "        if license_info[\"restrictions\"]:\n",
        "            results.append(f\"⚠️  제약사항:\")\n",
        "            for restriction in license_info[\"restrictions\"]:\n",
        "                results.append(f\"   - {restriction}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "# 사용 사례 테스트\n",
        "use_cases = [\n",
        "    (\"Apache 2.0\", \"스타트업에서 상업적 서비스 개발 및 모델 파인튜닝\"),\n",
        "    (\"LLaMA 2 License\", \"대기업(MAU 10억)에서 상업적 서비스 개발\"),\n",
        "    (\"GPT-4 API\", \"상업적 챗봇 서비스에 API 통합\"),\n",
        "    (\"Gemma License\", \"구글 검색 경쟁 서비스 개발\"),\n",
        "]\n",
        "\n",
        "print(\"\\n다양한 사용 사례에 대한 라이선스 적합성:\\n\")\n",
        "\n",
        "for license_name, use_case in use_cases:\n",
        "    print(f\"【 {license_name} 】\")\n",
        "    print(f\"사용 사례: {use_case}\")\n",
        "    print(f\"대표 모델: {', '.join(LicenseChecker.licenses[license_name]['models'])}\")\n",
        "    print()\n",
        "\n",
        "    results = LicenseChecker.check_use_case(license_name, use_case)\n",
        "\n",
        "    if isinstance(results, list):\n",
        "        for result in results:\n",
        "            print(f\"  {result}\")\n",
        "    else:\n",
        "        print(f\"  {results}\")\n",
        "\n",
        "    print(\"-\"*70)\n",
        "\n",
        "print(\"\\n💡 여러분의 프로젝트에 적합한 라이선스를 찾아보세요!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6F574Pe7fwB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d645a58f-e8ed-4b66-92ab-06bbb083e9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "⚖️ AI 모델 라이선스 비교\n",
            "======================================================================\n",
            "\n",
            "다양한 사용 사례에 대한 라이선스 적합성:\n",
            "\n",
            "【 Apache 2.0 】\n",
            "사용 사례: 스타트업에서 상업적 서비스 개발 및 모델 파인튜닝\n",
            "대표 모델: Mistral 7B, Qwen 2.5, SOLAR\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "【 LLaMA 2 License 】\n",
            "사용 사례: 대기업(MAU 10억)에서 상업적 서비스 개발\n",
            "대표 모델: LLaMA 2, LLaMA 3\n",
            "\n",
            "  ⚠️  제약사항:\n",
            "     - MAU > 700M requires special license\n",
            "----------------------------------------------------------------------\n",
            "【 GPT-4 API 】\n",
            "사용 사례: 상업적 챗봇 서비스에 API 통합\n",
            "대표 모델: GPT-4, GPT-4 Turbo\n",
            "\n",
            "  ⚠️  제약사항:\n",
            "     - API usage only\n",
            "     - Rate limits\n",
            "     - Cost per token\n",
            "----------------------------------------------------------------------\n",
            "【 Gemma License 】\n",
            "사용 사례: 구글 검색 경쟁 서비스 개발\n",
            "대표 모델: Gemma, Gemma 2\n",
            "\n",
            "  ⚠️  제약사항:\n",
            "     - Cannot compete with Google services\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "💡 여러분의 프로젝트에 적합한 라이선스를 찾아보세요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **예제: 모델 비용 계산기**\n",
        "- 팀 프로젝트시 반드시 해야할 과제임!"
      ],
      "metadata": {
        "id": "5zvIhHaoffla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 예제 : Open vs Closed 모델 비용 비교\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"💰 Open-weight vs Closed-source 비용 비교\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class ModelCostCalculator:\n",
        "    \"\"\"모델 사용 비용 계산기\"\"\"\n",
        "\n",
        "    # API 모델 가격 (2024-2025 기준, USD)\n",
        "    api_pricing = {\n",
        "        \"GPT-4\": {\n",
        "            \"input\": 0.03,  # per 1K tokens\n",
        "            \"output\": 0.06,\n",
        "            \"type\": \"closed-source\"\n",
        "        },\n",
        "        \"GPT-3.5 Turbo\": {\n",
        "            \"input\": 0.0005,\n",
        "            \"output\": 0.0015,\n",
        "            \"type\": \"closed-source\"\n",
        "        },\n",
        "        \"Claude Opus\": {\n",
        "            \"input\": 0.015,\n",
        "            \"output\": 0.075,\n",
        "            \"type\": \"closed-source\"\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # 오픈 모델 인프라 비용 (월 기준)\n",
        "    open_model_costs = {\n",
        "        \"LLaMA 2 7B\": {\n",
        "            \"gpu\": \"T4\",\n",
        "            \"gpu_cost_per_hour\": 0.35,  # Google Cloud\n",
        "            \"monthly_cost\": 0.35 * 24 * 30,\n",
        "            \"type\": \"open-weight\"\n",
        "        },\n",
        "        \"Mistral 7B\": {\n",
        "            \"gpu\": \"T4\",\n",
        "            \"gpu_cost_per_hour\": 0.35,\n",
        "            \"monthly_cost\": 0.35 * 24 * 30,\n",
        "            \"type\": \"open-weight\"\n",
        "        },\n",
        "        \"LLaMA 2 70B\": {\n",
        "            \"gpu\": \"A100\",\n",
        "            \"gpu_cost_per_hour\": 2.93,\n",
        "            \"monthly_cost\": 2.93 * 24 * 30,\n",
        "            \"type\": \"open-weight\"\n",
        "        },\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_api_cost(model_name, input_tokens, output_tokens):\n",
        "        \"\"\"API 모델 비용 계산\"\"\"\n",
        "\n",
        "        pricing = ModelCostCalculator.api_pricing.get(model_name)\n",
        "        if not pricing:\n",
        "            return None\n",
        "\n",
        "        input_cost = (input_tokens / 1000) * pricing[\"input\"]\n",
        "        output_cost = (output_tokens / 1000) * pricing[\"output\"]\n",
        "        total_cost = input_cost + output_cost\n",
        "\n",
        "        return {\n",
        "            \"input_cost\": input_cost,\n",
        "            \"output_cost\": output_cost,\n",
        "            \"total_cost\": total_cost\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def compare_costs(monthly_requests, avg_input_tokens, avg_output_tokens):\n",
        "        \"\"\"월간 비용 비교\"\"\"\n",
        "\n",
        "        print(f\"\\n사용량 가정:\")\n",
        "        print(f\"  월간 요청 수: {monthly_requests:,}회\")\n",
        "        print(f\"  평균 입력 토큰: {avg_input_tokens:,} tokens\")\n",
        "        print(f\"  평균 출력 토큰: {avg_output_tokens:,} tokens\")\n",
        "        print(f\"  총 토큰/월: {(monthly_requests * (avg_input_tokens + avg_output_tokens)):,} tokens\")\n",
        "        print()\n",
        "\n",
        "        # API 모델 비용\n",
        "        print(\"【 Closed-source (API) 모델 비용 】\")\n",
        "        for model_name, pricing in ModelCostCalculator.api_pricing.items():\n",
        "            cost_per_request = ModelCostCalculator.calculate_api_cost(\n",
        "                model_name, avg_input_tokens, avg_output_tokens\n",
        "            )\n",
        "\n",
        "            if cost_per_request:\n",
        "                monthly_cost = cost_per_request[\"total_cost\"] * monthly_requests\n",
        "                print(f\"  {model_name:20s}: ${monthly_cost:,.2f} / 월\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # 오픈 모델 비용\n",
        "        print(\"【 Open-weight 모델 비용 (인프라) 】\")\n",
        "        for model_name, cost_info in ModelCostCalculator.open_model_costs.items():\n",
        "            monthly_cost = cost_info[\"monthly_cost\"]\n",
        "            gpu_type = cost_info[\"gpu\"]\n",
        "            print(f\"  {model_name:20s}: ${monthly_cost:,.2f} / 월 ({gpu_type} 24/7 가동)\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # 손익분기점 계산\n",
        "        print(\"【 손익분기점 분석 】\")\n",
        "\n",
        "        # GPT-4 vs LLaMA 2 7B\n",
        "        gpt4_per_request = ModelCostCalculator.calculate_api_cost(\n",
        "            \"GPT-4\", avg_input_tokens, avg_output_tokens\n",
        "        )[\"total_cost\"]\n",
        "\n",
        "        llama_monthly = ModelCostCalculator.open_model_costs[\"LLaMA 2 7B\"][\"monthly_cost\"]\n",
        "\n",
        "        breakeven_requests = llama_monthly / gpt4_per_request\n",
        "\n",
        "        print(f\"  GPT-4 vs LLaMA 2 7B:\")\n",
        "        print(f\"    월 {breakeven_requests:,.0f}회 이상 사용 시 LLaMA 2가 경제적\")\n",
        "        print(f\"    현재 사용량 ({monthly_requests:,}회): \", end=\"\")\n",
        "\n",
        "        if monthly_requests > breakeven_requests:\n",
        "            print(\"✅ Open-weight 모델이 유리\")\n",
        "        else:\n",
        "            print(\"💰 API 모델이 유리\")\n",
        "\n",
        "# 시나리오 테스트\n",
        "scenarios = [\n",
        "    {\n",
        "        \"name\": \"소규모 스타트업\",\n",
        "        \"monthly_requests\": 10000,\n",
        "        \"avg_input\": 500,\n",
        "        \"avg_output\": 300\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"중규모 서비스\",\n",
        "        \"monthly_requests\": 1000000,\n",
        "        \"avg_input\": 500,\n",
        "        \"avg_output\": 300\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"대규모 서비스\",\n",
        "        \"monthly_requests\": 10000000,\n",
        "        \"avg_input\": 500,\n",
        "        \"avg_output\": 300\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, scenario in enumerate(scenarios, 1):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"시나리오 {i}: {scenario['name']}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    ModelCostCalculator.compare_costs(\n",
        "        scenario[\"monthly_requests\"],\n",
        "        scenario[\"avg_input\"],\n",
        "        scenario[\"avg_output\"]\n",
        "    )\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"💡 여러분의 프로젝트 규모에 맞는 모델을 선택해보세요!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wwqz9t0VfREF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fbf2a8-6225-4caa-9476-fbafeb0aec82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "💰 Open-weight vs Closed-source 비용 비교\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "시나리오 1: 소규모 스타트업\n",
            "======================================================================\n",
            "\n",
            "사용량 가정:\n",
            "  월간 요청 수: 10,000회\n",
            "  평균 입력 토큰: 500 tokens\n",
            "  평균 출력 토큰: 300 tokens\n",
            "  총 토큰/월: 8,000,000 tokens\n",
            "\n",
            "【 Closed-source (API) 모델 비용 】\n",
            "  GPT-4               : $330.00 / 월\n",
            "  GPT-3.5 Turbo       : $7.00 / 월\n",
            "  Claude Opus         : $300.00 / 월\n",
            "\n",
            "【 Open-weight 모델 비용 (인프라) 】\n",
            "  LLaMA 2 7B          : $252.00 / 월 (T4 24/7 가동)\n",
            "  Mistral 7B          : $252.00 / 월 (T4 24/7 가동)\n",
            "  LLaMA 2 70B         : $2,109.60 / 월 (A100 24/7 가동)\n",
            "\n",
            "【 손익분기점 분석 】\n",
            "  GPT-4 vs LLaMA 2 7B:\n",
            "    월 7,636회 이상 사용 시 LLaMA 2가 경제적\n",
            "    현재 사용량 (10,000회): ✅ Open-weight 모델이 유리\n",
            "\n",
            "======================================================================\n",
            "시나리오 2: 중규모 서비스\n",
            "======================================================================\n",
            "\n",
            "사용량 가정:\n",
            "  월간 요청 수: 1,000,000회\n",
            "  평균 입력 토큰: 500 tokens\n",
            "  평균 출력 토큰: 300 tokens\n",
            "  총 토큰/월: 800,000,000 tokens\n",
            "\n",
            "【 Closed-source (API) 모델 비용 】\n",
            "  GPT-4               : $33,000.00 / 월\n",
            "  GPT-3.5 Turbo       : $700.00 / 월\n",
            "  Claude Opus         : $30,000.00 / 월\n",
            "\n",
            "【 Open-weight 모델 비용 (인프라) 】\n",
            "  LLaMA 2 7B          : $252.00 / 월 (T4 24/7 가동)\n",
            "  Mistral 7B          : $252.00 / 월 (T4 24/7 가동)\n",
            "  LLaMA 2 70B         : $2,109.60 / 월 (A100 24/7 가동)\n",
            "\n",
            "【 손익분기점 분석 】\n",
            "  GPT-4 vs LLaMA 2 7B:\n",
            "    월 7,636회 이상 사용 시 LLaMA 2가 경제적\n",
            "    현재 사용량 (1,000,000회): ✅ Open-weight 모델이 유리\n",
            "\n",
            "======================================================================\n",
            "시나리오 3: 대규모 서비스\n",
            "======================================================================\n",
            "\n",
            "사용량 가정:\n",
            "  월간 요청 수: 10,000,000회\n",
            "  평균 입력 토큰: 500 tokens\n",
            "  평균 출력 토큰: 300 tokens\n",
            "  총 토큰/월: 8,000,000,000 tokens\n",
            "\n",
            "【 Closed-source (API) 모델 비용 】\n",
            "  GPT-4               : $330,000.00 / 월\n",
            "  GPT-3.5 Turbo       : $7,000.00 / 월\n",
            "  Claude Opus         : $300,000.00 / 월\n",
            "\n",
            "【 Open-weight 모델 비용 (인프라) 】\n",
            "  LLaMA 2 7B          : $252.00 / 월 (T4 24/7 가동)\n",
            "  Mistral 7B          : $252.00 / 월 (T4 24/7 가동)\n",
            "  LLaMA 2 70B         : $2,109.60 / 월 (A100 24/7 가동)\n",
            "\n",
            "【 손익분기점 분석 】\n",
            "  GPT-4 vs LLaMA 2 7B:\n",
            "    월 7,636회 이상 사용 시 LLaMA 2가 경제적\n",
            "    현재 사용량 (10,000,000회): ✅ Open-weight 모델이 유리\n",
            "\n",
            "======================================================================\n",
            "💡 여러분의 프로젝트 규모에 맞는 모델을 선택해보세요!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaB0B1LdMcPM"
      },
      "source": [
        "### **텍스트 생성 모델 로드하기**\n",
        "\n",
        "- 사용 모델 :\n",
        "    - microsoft/Phi-3-mini-4k-instruct\n",
        "        - 8GN VRAM  장치에서 실행 가능\n",
        "    - \"skt/KoGPT2-base-v2\"\n",
        "        - SKT가 개발한, 텍스트 생성(Generative)이 가능한 한국어 GPT-2 모델의 기본(base) 버전\n",
        "        - 약 1억 2500만 개(125M)의 파라미터\n",
        "\n",
        "코드 짜는거 외우기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNkbw28oMeAM",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a11ac9bb1626480a9bcd258bcf4c8e3e",
            "df9ba77f3b3e41f8a23f5ee8e7a7bdb5",
            "564c4b5d1b214fc48f0b24b455fa70b8",
            "2999811fdbfa4daa924f52faf7c2e9a2",
            "56b3a8676df641fe9ba7dc17bc330f8e",
            "e9f31c7c329148a0b5b0f0e9b2448ee5",
            "b0d71dff8db44063a4df053c2d09ed79",
            "9972a75896b74919b4c470d21fbd5ffb",
            "48e41165a1584d808f0ae08b8dd060d6",
            "6b2194fd84084768928ee3f5378b0e66",
            "f086d3e3c1864b06b363231e82f66ca7"
          ]
        },
        "outputId": "3d952b9e-27b7-4106-ec3d-8b032d88dd30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a11ac9bb1626480a9bcd258bcf4c8e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# AutoModelForCausalLM: 생성모델\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# 모델과 토크나이절르 로드합니다.\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "# model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"  # 어텐션 구현 방식을 'eager' (기본)로 명시\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# 파이프라인을 만듭니다.\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False, # 프롬프트를 제외하고 생성된 부분만 반환\n",
        "    max_new_tokens=500,     # 단, KoGPT2를 사용한다면 4k 토큰을 처리하지 못하므로 500은 너무 깁니다. (150 정도로 조절)\n",
        "    do_sample=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **호랑이와 관련된 농담을 만들어 달라고 요청하기**"
      ],
      "metadata": {
        "id": "tCRbsr2-2qkG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZIMVL0Q3g8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28818b4e-60d9-4c71-b5fc-d7fbc61e9cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 호랑이와 관련된 재미난 이야기:\n",
            "\n",
            "\n",
            "저는 호랑이와 관련된 이야기를 생각해 보았습니다. 호랑이는 예술적으로 매우 유명합니다. 그 이유는 그들의 멋진 형태와 느낌입니다. 호랑이는 대형 몸무게, 발길이가 넓은 발, 반지 등 특이한 형태를 가지고 있습니다. 그들은 매우 강력하고 잠실이 되어 있어 예술적으로 높은 감정을 주며 작품을 작성하는 데 도움이 됩니다.\n",
            "\n",
            "\n",
            "한편, 호랑이는 예술적으로도 잘 작용합니다. 그들의 몸과 몸집을 통해 다양한 예술 방법을 사용합니다. 예를 들어, 그들은 물건을 싸고 있거나 물건을 쓰는 것으로 작품을 만들고 있습니다. 이��\n"
          ]
        }
      ],
      "source": [
        "# 프롬프트\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"호랑이와 관련된 재미난 이야기를 만들어줘.\"}\n",
        "] # system, user, assistent 3개가 있음. 프롬프트 구조 공부\n",
        "\n",
        "# 출력을 생성합니다.\n",
        "output = pipe(messages)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **템플릿 적용하기**"
      ],
      "metadata": {
        "id": "Rt0yDdgh3D55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOGpAY8w4odt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5933936c-976f-4219-f60a-3cb0a8a6f847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "호랑이와 관련된 재미난 이야기를 만들어줘.<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "# 프롬프트 템플릿을 적용합니다.\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **모델 출력 제어하기**"
      ],
      "metadata": {
        "id": "gV6bHxKb3lky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **모델 매개변수를 조정하여 출력 제어하기**\n",
        "    - (참고) PyTorch의 언어 모델 생성 함수에서 사용되는 주요 매개변수.ipynb\n",
        "    - **temperature** :텍스트 생성의 무작위성(창의성) 조절\n",
        "    - **do_sample** : 샘플링 방식 선택 (False=Greedy)\n",
        "    - **top_p** : 뉴클리어 샘플링(nuclear sampling), LLM이 고려할 토큰 일부(뉴클리어스)를 제어하는 샘플링 기법, top_p에 지정한 누적 확률에 도달할 때까지 확률 크기 순으로 토큰을 모음(확률 높은 토큰의 랭킹)"
      ],
      "metadata": {
        "id": "kggLSohK3vDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLd_XXaR3i04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "7b9da57e-0451-4a49-fc40-1021e2d92954"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-146567363.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#       1 : 기본 대화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1.5-2.0 : 소설, 시 창작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m output = pipe(messages,\n\u001b[0m\u001b[1;32m      8\u001b[0m               \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# False면 그리디\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               temperature=0.0)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                     \u001b[0mchats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 🐈 🐈 🐈\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             )\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0;31m# 9. prepare logits processors and stopping criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m         prepared_logits_processor = self._get_logits_processor(\n\u001b[0m\u001b[1;32m   2149\u001b[0m             \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m             \u001b[0minput_ids_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_get_logits_processor\u001b[0;34m(self, generation_config, input_ids_seq_length, encoder_input_ids, prefix_allowed_tokens_fn, logits_processor, device, model_kwargs, negative_prompt_ids, negative_prompt_attention_mask)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;31m# all samplers can be found in `generation_utils_samplers.py`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m                 \u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTemperatureLogitsWarper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m                 processors.append(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, temperature)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mexcept_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" If you're looking for greedy decoding strategies, set `do_sample=False`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcept_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`."
          ]
        }
      ],
      "source": [
        "# 높은 temperature를 사용합니다.\n",
        "# temperature 값을 조정해 보세요. (0 ~ 2)\n",
        "#       0 : 수학문제\n",
        "# 0.3-0.5 : 보고서 작성\n",
        "#       1 : 기본 대화\n",
        "# 1.5-2.0 : 소설, 시 창작\n",
        "output = pipe(messages,\n",
        "              do_sample=True, # False면 그리디\n",
        "              temperature=0.0)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# temperature = 1.5\n",
        "output = pipe(messages,\n",
        "              do_sample=True, # False면 그리디\n",
        "              temperature=1.5)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH7QgPSKe9Fu",
        "outputId": "dd1f7032-a8aa-4aef-d53c-8e307eeda24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 월에같은 요정을 예가 기퀴 중단으로 만든 이유가 아닌 로스그뱃버들이 떠있었다. Roosgevrissen는 아직도 내부에 사의정의를 이룰수도, 나지만 죵중에 기퀴이 예가 터지듯이 산고가 소리를 내리는 것을 유명하게 홑하였다.\n",
            "\n",
            "\n",
            "그리고 한 구대화에서는 날카로이가 도모로나를 산보트로 보인 Roosgevrissen와 땀볕같은 바다를 한소리를 내며 말같이 한밟풍스츠를 해보았다. 반서에 동시에 호랑이가 들어오며, 근어내려마신사의 줄세를 들며 홀로도 도달한다.\n",
            "\n",
            "\n",
            "그에 따라 기퀴들이 바람에 내리도록 이바구니를 딱 연주하며 소란들에게의 마음보다 줍리라는 뮤지컬화서는 강하게 들던. 결국, 고유어들도 수직임, 무잠의 상호조화로 요원의 지식 사상을 인정하며 어머녀와 아들들도 서사쌈를 칭뜨겨다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz61Fvtk580U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6332ceec-262c-4d01-f48e-b6aa8fdbea0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 클래스 종교에 선물로 발송했다. 그래서 우리가 마감이 오기 전에 광고메시지를 텔레비전에 넣었다. 이렇게 한 방송은 어느정도 오셔도 잘들어요. 철수는 항상 좋을 수 있는 행복한 이야기에 잘 들었다. 또한, 우리가 도와주었던 후원자들에게는 사랑스러운 반올림이에요. 이제, 이야기를 찾아가자. 클래스의 아비는 회장이 도와주는 애인들과의 행운을 위해 작은 자리에 항결을 나타낸다. 이 암호를 발견한 후원자들은 모여나온 친구와 앞으로 그들의 기부금을 찾는 것에 집중하게 되었다. 때문에, 그들은 마침내 뒤틀어 대한 재산으로부터 다시 메모장을 벌어 주었다. 이제 윤리적인 대처를 다루고 있는 대학에서는 홍보\n"
          ]
        }
      ],
      "source": [
        "# 높은 top_p를 사용합니다.\n",
        "# top_p 값을 조정해 보세요.\n",
        "output = pipe(messages, do_sample=True, top_p=1)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[실습] 다양한 모델로 텍스트 생성하기**\n",
        "모델을 변경하여 텍스트를 생성해 보고 생성된 결과의 특징을 확인해 보세요.\n",
        "1. model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "2. model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "3. model_id = \"Qwen/Qwen2.5-3B-Instruct\""
      ],
      "metadata": {
        "id": "yvtGUZp0GbeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# AutoModelForCausalLM: 생성모델\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# 모델과 토크나이절르 로드합니다.\n",
        "model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "# model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"  # 어텐션 구현 방식을 'eager' (기본)로 명시\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# 파이프라인을 만듭니다.\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False, # 프롬프트를 제외하고 생성된 부분만 반환\n",
        "    max_new_tokens=500,     # 단, KoGPT2를 사용한다면 4k 토큰을 처리하지 못하므로 500은 너무 깁니다. (150 정도로 조절)\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# 프롬프트 템플릿을 적용합니다.\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(prompt)\n",
        "\n",
        "# 프롬프트\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"호랑이와 관련된 재미난 이야기를 만들어줘.\"}\n",
        "] # system, user, assistent 3개가 있음. 프롬프트 구조 공부\n",
        "\n",
        "# 출력을 생성합니다.\n",
        "output = pipe(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693,
          "referenced_widgets": [
            "1109e35f1b034b819e82e309fabe3d0f",
            "8a82aad7b1804b3a96be1bcae6ee4b79",
            "c75d75dc2b244d94914b5318c8ad4eab",
            "c9c9c660b17641aa95b283becfbccbe1",
            "287f887342504ad9ba7fb6a175c52365",
            "b3e05422411641c98f9cbb88159c2a5f",
            "93d4e1df3486426c89dbd8556ba14b88",
            "d5929f31726145f8ac0902c3042c528b",
            "e81531687a5a463580782494043b3feb",
            "85a297edbee34a3aa368f22cac06ef52",
            "4f0c2e77bba6489b8dd197b47b10bee8",
            "b17a67b10dba467e9d3682b42d0fa102",
            "fdbd41e7adcd46fa8af61c022aa072c9",
            "d7ef9026e79c42acb91e523b094e5d03",
            "a7bca91ac85644e09d8e5a616e027620",
            "bfd0bd3663e4404f9515f8288baa750e",
            "9c17ee94484e4baa890778827a746d9d",
            "5f047b963f2644239338e3dc36e72caf",
            "61415760ee9745b1965216dc4227c10d",
            "54b83e6e254941d48870fb1757569ce3",
            "dac8100f88ad4e039cff306c5e1917f2",
            "01dea7007ba34f89837703257871e7ed",
            "de980ce566094405acf1ed66f21c6b1c",
            "d9d9914bf3894148981bcfc5b49647d4",
            "66901e77a7de4a7b8c56e801babb7daf",
            "5662c77ee2d44562a761fcb237153288",
            "fadd1b06fb0d4b65901031def7398bf4",
            "96f967db66794954936d58194b422e0b",
            "eda897e164954d84986d057c04d7a101",
            "c278fd26ef4e4717a3e99c6e55c27716",
            "06eb2d7f667643bfaeda59940de08852",
            "4e72014ce50949d4a053f457df74907a",
            "5ca8334bfcfc4d94a7db914f5b2ad0ce",
            "067ccf1246824eba8b24b7db7c66d1f7",
            "e2543b773d0d4248ae21a4bbaab6ad86",
            "b9707e0614fe4476998bf55024b74cb9",
            "6af1bd76535641938cc39bbc243b103a",
            "8f3b187b73fd4f30b7809b36a2195866",
            "0d2b0dc868c242bba5ead2aedb516330",
            "f26cb839d03a41f2ad09afadaf593695",
            "619864f8feb1470f8726c4a1d0437448",
            "3771ef5790b9403d8e005755386da95b",
            "51d296cb9e844c6ab015f231c9a93270",
            "1aa40e8aaf3f49a891bf8b5031a31eae",
            "a74e7d9f237d49e39c6a6884cef1d83c",
            "005f415f4cfb41d997f89ed1c26c4718",
            "c24e7f043c7249bab2c6898c63e00150",
            "5f144c8fcfb64006a38318fa89d888ec",
            "9f292cfeb5744609bfe50577f4406b3f",
            "61d73a4cea11456a8bd582a9f5ac0dd0",
            "9d2c21cdefd04f1c82b0193933204482",
            "a5a895e5f89a4a2384d4429976553df4",
            "f28cc192d00546779ef1af9917007651",
            "b61656c0d22b48e3a98a2ce239c5c5a3",
            "35f832cd053d49d6af8b5a066637a3e6",
            "02ef5777462845689a6f3722b828138c",
            "a1e1e46b991b4bd4a343ea91c6e8fc43",
            "96bb40803325453b9f705e3c439faea8",
            "bf446c34fef1457b95680b2f3251251b",
            "3e0f22e4bcf742b49b7f77c9f0803018",
            "98056d17ec1e40d8ac1d0d3898c031f5",
            "ea07887628ce45898a2fe429c44a62c1",
            "16b189a3984f4db58396a8d387bd446b",
            "f1aa8a3ed8fb42169ba24472efe22fad",
            "114a63fe588d40cbac3cb9d86383d452",
            "147dc0fdb86b4863b014a467b1c8b5c8",
            "36880692b82743be82ce435715d9e97e",
            "3447c540b2574a5597f4e1c8389e9463",
            "cde2d317855d486ab7840b16e32da230",
            "2fa7766dc5c74b1195dd9bcb3d4795a1",
            "b2685543196d4d3789cf02f26ba9df1b",
            "fb91e9a212f8470181b76cefc284aa1b",
            "05862be2493c45d0900eb3895af6f878",
            "1e01d552f6614bceb052fc7a006f4f7e",
            "c94178aacff14a6895ca063a3aa7ccb1",
            "33ee6be299364796aa45a2245082fb62",
            "d5acef0aca264483bf3ba846f1868c1e",
            "e509c309750f4367986ffdf0b6aa5ab7",
            "d63f737b7f77473eb856ec35386b38a5",
            "930f6bb9053d4a5ea563fd5b286428bb",
            "8f0070de23b241a4a8b14848355760ee",
            "489a3a84fd02446eaf94272edb029248",
            "a2579953509947a58aa1bd00140ce7e4",
            "625dcdf6ddb14c4ba8324b6f11940a92",
            "933eeda023e148d295b0f8658d530076",
            "fa691d38b01541398f56e3eab9c5b760",
            "618d564814b241719e4adbbec0f78d0e",
            "3b193cf2cf7a403dbf02fb05b9ce2572",
            "f8b7ba6ef875407ea1dc590d0f148f18",
            "6b39227f5a194281beb947f194c13bc9",
            "94db82e06c4e4747a99aecaffe12bfa8",
            "37a8903b6e544b858e0802b5bd7c1ec2",
            "c562f6dd1bad4d419cc36456375a9544",
            "19663a7125d84716a88e81020eae477b",
            "35348d4b8d0d49d699a9b13010d0354a",
            "575273abd64c49e9a6f059483f23f724",
            "e60bd1a591bf4c789364d518e2897159",
            "3bf82348672c482cb0641a104706c324",
            "33d7bae8abe44a19b00a062862858e7a",
            "8341b59db50741b6bcc7cdcbc894f3ed",
            "0b5654c9ed5c4f9ca6ee7aeb333ebb71",
            "5ad9448aa8be478892663c49221ca487",
            "9e0ec03fefcd4187a0d8ee4720df72f3",
            "699fed99bc04464aac649ffc199eb14d",
            "7c013ce170d7400d94a355e8486b4817",
            "27edfee7d0cb48ceb0c2f65ac514a324",
            "2b8d5aae01134983a2e7073930903985",
            "fa2439318acd469da880cacbb72185ef",
            "fb98aa12ef7547cd908481aa521201f8",
            "c9d258f389b6463da7188c6bef5dd4d5",
            "b2f8bd14c1ad42a78ddc046b2ddd1305",
            "7721c2d0e3254e1884c1c71ea1278f5b",
            "6222bd99f75148c3a54acac7b0f4a7d6",
            "164a5572a8024ad0a90b843f758fe5c8",
            "dfb9963fcbb4419583dd4f24894b65ce",
            "16f14b6bf5734664a22130f2329d1b8f",
            "e05a947992014288a217395d5cec0071",
            "ffefd0c5bc4f429f8bbcd7ee0163e077",
            "bc437327e1ac402da948c3f49f20eb96",
            "7749039412c646ceadcff92de912df18",
            "90110df89fb14856bd5d4e9972c853e1",
            "2fee776143474e53ae0cba448bb95e59",
            "35e56865d6b44de6b7c61cf9ebd875e5",
            "495566b976fd4073a35f2a155cf15f54",
            "a2b88f94156245509274f73cebe5b352",
            "9a1bdad6cbaf4361b88afe4a74da348a",
            "4eac5697d0304056850631bd541191ac",
            "550c9e428cc64b46ae64e5a9aeb7404a",
            "b98bffab1ea14f3aac0c322c3996b152",
            "19d1f6fa3a57467bbf853f85ed9ac8de",
            "519d5fd7c4034d81a644279238d3c472",
            "2b8959826a4f4c6ebdf7e8802098056a",
            "c1f2082dad3048c5a070f542e9f80ca1",
            "f29465eeede14f83af42b1b5e003737e",
            "cd92776e42c84b45bc87d74375e59c6b",
            "61906dcc84c24a60930c516f9770f153",
            "6209e7336e2b49f4a5e1f925a857d9c9",
            "aba1c473739d49ae88fa9dd80f6c777e",
            "9c1fa58a38b546deb9b8de3ac3ad44fa",
            "8a329abed98d4590a61c0223d8e8ec32",
            "9e50398e224c4f9c9b9071ec6ac29ab5",
            "90dbf03b2eeb41d7a558ac19fc72168e",
            "3326cc1cf4c04b66a6873f7d6b47013d",
            "ed1b84e6b39d458ebc159896e52cbb48",
            "d6e292238e104dc0ba19cc6447cdfc03",
            "edefdf28fd9743fda9abb524c0ac4037",
            "1a85079916044b5c94f0db675b96e26a",
            "a5b4e29cbf534be0822294d3622b46f2",
            "cdb739f4ef744b77b6417f292e4d7a1e",
            "75be5b9521a345a6ba3df414edff5d82",
            "6b5efc53cb1e4938adeee332704f8308",
            "ea5fcdbdd0fe4c8fb51721fc4f4dadd3",
            "a57bca17170b4bc891ba22678546d95e",
            "54fd8910297b4ce3836e07d403e171ad"
          ]
        },
        "id": "2HogHCSyj1gq",
        "outputId": "a2b81db9-67bd-4f9a-a69e-2334aec8577c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1109e35f1b034b819e82e309fabe3d0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b17a67b10dba467e9d3682b42d0fa102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de980ce566094405acf1ed66f21c6b1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00006.safetensors:   0%|          | 0.00/3.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "067ccf1246824eba8b24b7db7c66d1f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a74e7d9f237d49e39c6a6884cef1d83c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00006.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02ef5777462845689a6f3722b828138c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36880692b82743be82ce435715d9e97e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00005-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e509c309750f4367986ffdf0b6aa5ab7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00006-of-00006.safetensors:   0%|          | 0.00/1.29G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8b7ba6ef875407ea1dc590d0f148f18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8341b59db50741b6bcc7cdcbc894f3ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2f8bd14c1ad42a78ddc046b2ddd1305"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fee776143474e53ae0cba448bb95e59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1f2082dad3048c5a070f542e9f80ca1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed1b84e6b39d458ebc159896e52cbb48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "호랑이와 관련된 재미난 이야기를 만들어줘.<|eot_id|>\n",
            "호랑이와 관련된 재미난 이야기를 만들어줘.assistant\n",
            "이번엔 1년 전과는 달리, 1년 후의 상황을 예측하는 데 집중했다. 1년 후의 상황을 예측하는 데는 1년 전의 상황을 예측하는 것보다 훨씬 더 어려운 일이다. 1년 전의 상황을 예측하는 데는 과거의 데이터를 활용할 수 있지만, 1년 후의 상황을 예측하는 데는 과거의 데이터를 활용할 수 없다. 1년 후의 상황을 예측하는 데는 과거의 데이터를 활용할 수 없기 때문에, 1년 후의 상황을 예측하는 것은 1년 전의 상황을 예측하는 것보다 훨씬 더 어려운 일이다. \n",
            " 1년 후의 상황을 예측하는 데는 과거의 데이터를 활용할 수 없기 때문에, 1년 후의 상황을 예측하는 것은 1년 전의 상황을 예측하는 것보다 훨씬 더 어려운 일이다. \n",
            " 1년 후의 상황을 예측하는 데는 과거의 데이터를 활용할 수 없기 때문에, 1년 후의 상황을 예측하는 것은 1년 전의 상황을 예측하는 것보다 훨씬 더 어려운 일이다. \n",
            " 1년 후의 상황을 예측하는 데는 과거의 데이터를 활용할 수 없기 때문에, 1년 후의 상황을 예측하는 것은 1년 전의 상황을 예측하는 것보다 훨씬 더 어려운 일이다. \n",
            " 1년 후의 상황을 예측하는 데는 과거의 데이터를 활용할 수 없기 때문에, 1년 후의 상황을 예측하는 것은 1년 전의 상황을 예측하는 것보다 훨씬 더 어려운 일이다. \n",
            " 1년 후의 상황을 예측하는 데는 과거의 데이터를 활용할 수 없기 때문에, 1년 후의 상황을 예측하는 것은 1년 전의 상황을 예측하는 것보다 훨씬 더 어려운 일이다. \n",
            " 1년 후의 상황을 예측하는 데는 과거의 데이터를 활용할 수 없기 때문에, 1년 후\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7Jr2TER1GG9f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnojy54u2uQa"
      },
      "source": [
        "## **고급 프롬프트 엔지니어링**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **고급 프롬프트 구성 요소**\n",
        "\n",
        "- **역할/페르소나**(정체성)\n",
        "    - 모델에게 “누구처럼 행동할지”를 명시합니다\n",
        "- **지시사항/명령어** (핵심 작업)\n",
        "    - 프롬프트의 핵심으로, AI에게 무엇을 해야 하는지 명확하게 전달합니다\n",
        "    - 모호한 표현보다는 구체적이고 직접적인 동사를 사용하는 것이 효과적입니다\n",
        "    - \"해줘\", \"만들어줘\", \"분석해줘\", \"요약해줘\" 등의 명확한 동사 사용\n",
        "- **문맥/맥락/컨텍스트** (추가 정보)\n",
        "    - AI가 작업을 수행할 때 필요한 추가 정보/배경 정보를 제공합니다\n",
        "    - 대상 독자, 목적, 상황 등을 포함하면 더 적절한 응답을 얻을 수 있습니다\n",
        "- **형식** (추가 정보)\n",
        "    - LLM이 생성한 텍스트를 출력하는 데 사용할 형식. 이를 지정하지 않으면 LLM이 스스로 형식을 결정하기 때문에 자동화된 시스템에서 문제가 됩니다.\n",
        "- **청중**\n",
        "    - 생성된 텍스트의 소비 대상, 생성된 출력의 수준도 기술합니다.\n",
        "    - 교육이 목적이라면 ELI5 (Explain Like I'm 5: \"5살 아이에게 설명하듯이 쉽게 설명해주세요\"라는 뜻)를 사용하는게 도움이 됩니다.\n",
        "- **어투**\n",
        "    - LLM이 생성된 텍스트에서 사용할 말투, 상사에게 업무 메일을 쓴다면 격식을 차린 어투가 필요할 것입니다.\n",
        "- **데이터**\n",
        "    - 작업 자체에 관련된 주요 데이터"
      ],
      "metadata": {
        "id": "40oeVa2XMQji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMF9p8qK58Ou"
      },
      "source": [
        "### **복잡한 프롬프트**\n",
        "\n",
        "- 최상의 프롬프트를 얻으려면 실험이 필수이다.\n",
        "- 프롬프트 구성은 기본적으로 반복적인 실험 과정이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEQ9ZnSH73yP"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"이전 게시물에서는 최신 딥러닝 모델에서 널리 사용되는 방법인 어텐션(Attention)을 살펴보았습니다. 어텐션은 신경망 기계 번역 애플리케이션의 성능 향상에 기여한 개념입니다. 이번 게시물에서는 어텐션을 사용하여 모델의 학습 속도를 높이는 모델인 트랜스포머(The Transformer)를 살펴보겠습니다. 트랜스포머는 특정 작업에서 구글 신경망 기계 번역 모델보다 우수한 성능을 보입니다. 하지만 가장 큰 장점은 트랜스포머가 병렬화에 적합하다는 점입니다. 실제로 구글 클라우드는 자사의 클라우드 TPU 솔루션을 사용하기 위해 트랜스포머를 참조 모델로 사용할 것을 권장합니다. 이제 모델을 분해하여 어떻게 작동하는지 살펴보겠습니다.\n",
        "트랜스포머는 \"Attention is All You Need\" 논문에서 제안되었습니다. 텐서플로우(TensorFlow) 기반 구현은 Tensor2Tensor 패키지의 일부로 제공됩니다. 하버드 대학교 자연어 처리 그룹에서는 PyTorch 구현을 사용하여 논문에 주석을 단 가이드를 작성했습니다. 이 글에서는 내용을 다소 단순화하고 개념을 하나씩 소개하여 해당 주제에 대한 심층적인 지식이 없는 사람들도 더 쉽게 이해할 수 있도록 하겠습니다.\n",
        "이 모델을 하나의 블랙박스로 생각해 보겠습니다. 기계 번역 애플리케이션에서 이 모델은 한 언어로 된 문장을 입력받아 다른 언어로 번역된 결과를 출력합니다.\n",
        "옵티머스 프라임처럼 생긴 이 모델을 열면 인코딩 구성 요소, 디코딩 구성 요소, 그리고 이들 간의 연결을 볼 수 있습니다.\n",
        "인코딩 구성 요소는 인코더의 스택입니다(이 논문에서는 인코더를 여섯 개씩 쌓아 올렸는데, 숫자 6이 마법 같은 것은 아니며, 다른 배열을 시도해 볼 수 있습니다). 디코딩 구성 요소는 같은 수의 디코더의 스택입니다.\n",
        "인코더는 모두 구조가 동일하지만 가중치는 공유하지 않습니다. 각 인코더는 두 개의 하위 계층으로 나뉩니다.\n",
        "인코더의 입력은 먼저 셀프 어텐션 계층을 통과합니다. 셀프 어텐션 계층은 인코더가 특정 단어를 인코딩할 때 입력 문장의 다른 단어들을 살펴보는 데 도움이 되는 계층입니다. 이 글의 후반부에서 셀프 어텐션에 대해 자세히 살펴보겠습니다.\n",
        "셀프 어텐션 계층의 출력은 피드포워드 신경망에 입력됩니다. 동일한 피드포워드 신경망이 각 위치에 독립적으로 적용됩니다.\n",
        "디코더는 두 계층을 모두 가지고 있지만, 두 계층 사이에는 디코더가 입력 문장의 관련 부분에 집중할 수 있도록 돕는 어텐션 계층이 있습니다(seq2seq 모델에서 어텐션이 하는 역할과 유사).\n",
        "이제 모델의 주요 구성 요소를 살펴보았으니, 다양한 벡터/텐서와 이러한 벡터/텐서가 구성 요소 사이를 어떻게 흐르면서 학습된 모델의 입력을 출력으로 변환하는지 살펴보겠습니다.\n",
        "일반적인 NLP 애플리케이션과 마찬가지로, 임베딩 알고리즘을 사용하여 각 입력 단어를 벡터로 변환하는 것으로 시작합니다.\n",
        "각 단어는 크기가 512인 벡터에 임베딩됩니다. 이러한 벡터를 다음과 같은 간단한 상자로 표현하겠습니다.\n",
        "임베딩은 가장 아래쪽 인코더에서만 수행됩니다. 모든 인코더에 공통적인 추상화는 각각 크기가 512인 벡터 목록을 받는다는 것입니다. 맨 아래 인코더에서는 단어 임베딩이 되고, 다른 인코더에서는 바로 아래에 있는 인코더의 출력이 됩니다. 이 목록의 크기는 우리가 설정할 수 있는 하이퍼파라미터로, 기본적으로 학습 데이터 세트에서 가장 긴 문장의 길이가 됩니다.\n",
        "입력 시퀀스에 단어를 임베딩한 후, 각 단어는 인코더의 두 계층을 각각 통과합니다.\n",
        "여기서 우리는 Transformer의 핵심 속성 중 하나를 발견하게 되는데, 각 위치의 단어는 인코더에서 자체 경로를 따라 흐른다는 것입니다. 셀프 어텐션 계층에서는 이러한 경로 간에 종속성이 있습니다. 그러나 피드포워드 계층에는 이러한 종속성이 없으므로, 피드포워드 계층을 통과하는 동안 다양한 경로가 병렬로 실행될 수 있습니다.\n",
        "다음으로, 예시를 더 짧은 문장으로 전환하여 인코더의 각 하위 계층에서 어떤 일이 일어나는지 살펴보겠습니다.\n",
        "이제 인코딩을 시작합니다!\n",
        "앞서 언급했듯이, 인코더는 벡터 목록을 입력으로 받습니다. 인코더는 이 벡터들을 '셀프 어텐션' 계층으로 전달한 후, 피드포워드 신경망으로 전달하여 목록을 처리한 후, 출력을 다음 인코더로 전송합니다.\"\"\"\n",
        "\n",
        "# 프롬프트 구성 요소\n",
        "persona = \"당신은 대규모 언어 모델 전문가입니다. 복잡한 논문을 이해하기 쉬운 요약으로 정리하는 데 능숙합니다.\\n\"\n",
        "instruction = \"제공된 논문의 주요 결과를 요약하세요.\\n\"\n",
        "context = \"귀하의 요약에서는 연구자들이 논문의 가장 중요한 정보를 빠르게 이해하는 데 도움이 되는 가장 중요한 요점을 추출해야 합니다.\\n\"\n",
        "data_format = \"방법을 간략하게 요약한 요점 요약을 작성하세요. 그 후 주요 결과를 요약하는 간결한 단락으로 마무리하세요.\\n\"\n",
        "audience = \"이 요약은 대규모 언어 모델의 최신 동향을 빠르게 파악해야 하는 바쁜 연구자들을 위해 고안되었습니다.\\n\"\n",
        "tone = \"톤은 전문적이고 명확해야 합니다.\\n\"\n",
        "text = \"요약할 내 텍스트\"  # Replace with your own text to summarize\n",
        "data = f\"요약할 텍스트: {text}\"\n",
        "\n",
        "# 전체 프롬프트 - 요소를 삭제하거나 추가하여 생성된 출력에 미치는 영향을 관찰하세요.\n",
        "query = persona + instruction + context + data_format + audience + tone + data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8hbXm9R-sr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761752ac-050f-42e5-a237-790fb0f9712d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "당신은 대규모 언어 모델 전문가입니다. 복잡한 논문을 이해하기 쉬운 요약으로 정리하는 데 능숙합니다.\n",
            "제공된 논문의 주요 결과를 요약하세요.\n",
            "귀하의 요약에서는 연구자들이 논문의 가장 중요한 정보를 빠르게 이해하는 데 도움이 되는 가장 중요한 요점을 추출해야 합니다.\n",
            "방법을 간략하게 요약한 요점 요약을 작성하세요. 그 후 주요 결과를 요약하는 간결한 단락으로 마무리하세요.\n",
            "이 요약은 대규모 언어 모델의 최신 동향을 빠르게 파악해야 하는 바쁜 연구자들을 위해 고안되었습니다.\n",
            "톤은 전문적이고 명확해야 합니다.\n",
            "요약할 텍스트: 요약할 내 텍스트<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "print(tokenizer.apply_chat_template(messages, tokenize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jO_uqNMTiWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6920c3a6-d5d2-4e36-a406-c7dd5d308024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 요약할 내 텍스트: 이 논문은 언어 모델의 학습 과정에 대한 더 잘 이해된 이론을 제시하고 있습니다. 논문은 언어 모델이 언어 표현을 학습하는 방식에 대한 더 잘 이해된 이론을 제시하고 있습니다. 이러한 이론은 언어 모델이 언어 표현을 학습하는 방식에 대한 더 잘 이해된 이론을 제시하고 있습니다. 본 논문은 언어 모델이 언어 표현을 학습하는 방식에 대한 더 잘 이해된 이론을 제시하고 있습니다. 이러한 이론은 언어 모델이 언어 표현을 학습하는 방식에 대한 더 잘 이해된 이론을 제시하고 있습니다. 본 논문은 언어 모델이 언어 표현을 학습하는 방식에 대한 더 잘 이해된 이론을 제시하고 있습니다. 이러한 이론은\n"
          ]
        }
      ],
      "source": [
        "# 출력을 생성합니다.\n",
        "outputs = pipe(messages)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "# 캐시된 메모리 해제\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 가비지 컬렉션 실행\n",
        "gc.collect()\n",
        "\n",
        "print(\"GPU 메모리 클리어 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW4EwSDcpEXE",
        "outputId": "c149546e-3d89-44c4-a216-3631adfade30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 메모리 클리어 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol5aCUfM9bjM"
      },
      "source": [
        "### **문맥 내 학습: 예시 제공**\n",
        "\n",
        "- **문맥 내 학습**(in-context learning) : 모델에게 올바른 예시를 제공하는 방법\n",
        "    - **제로샷**(zero-shot) 프롬프트 : 예시를 활용하지 않는다.\n",
        "    - **원샷**(one-shot) 프롬프트 : 한 개 예시 사용\n",
        "    - **퓨샷**(few-shot) 프롬프트 : 두 개 이상의 예시 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 제로샷: 예시 0개\n",
        "    - user\n",
        "- 원샷: 예시 1개\n",
        "    - user → assistant → user\n",
        "- 퓨샷: 예시 2개 이상\n",
        "    - (user → assistant) × N → user"
      ],
      "metadata": {
        "id": "yjjaeDjXUXo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne92BtrXU8k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17d42a9-1a82-4064-a1bf-d9bee2dc5629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "'기가무루'는 일본 악기의 한 종류입니다. '기가무루'라는 단어를 사용한 문장의 예는 다음과 같습니다.:<|end|>\n",
            "<|assistant|>\n",
            "삼촌이 선물해 주신 기가무루가 있어요. 집에서 하는 걸 좋아해요.:<|end|>\n",
            "<|user|>\n",
            "무언가를 'screeg'한다는 것은 칼을 휘두르는 것을 의미합니다. screeg라는 단어를 사용한 문장의 예는 다음과 같습니다.:<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "# 문장에 가상의 단어가 포함된 예시를 사용합니다.\n",
        "one_shot_prompt = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"'기가무루'는 일본 악기의 한 종류입니다. '기가무루'라는 단어를 사용한 문장의 예는 다음과 같습니다.:\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"삼촌이 선물해 주신 기가무루가 있어요. 집에서 하는 걸 좋아해요.:\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"무언가를 'screeg'한다는 것은 칼을 휘두르는 것을 의미합니다. screeg라는 단어를 사용한 문장의 예는 다음과 같습니다.:\"\n",
        "    }\n",
        "]\n",
        "print(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox3LeHW5JfaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f5f4c5-01c8-42ae-cc18-1a8787847f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 저는 칼을 휘두르는 것을 즐겨하고 있어요. 칼을 휘두르면 나무를 덮어 잡고 있어요. 칼을 휘두르는 것은 잘 잘 되어 있는 나무를 잡아 내리는 것이 매우 힘듭니다. 그래서 나는 칼을 휘두르는 것을 좋아합니다. 그러나 칼을 휘두르는 것은 약속을 못 외롭게 하거나 칼을 쓰는 것을 잊지 마셔야 합니다. 칼을 휘두르는 것은 잘 잘 잡아 내리는 것이 아니고 잘 잘 잡아 더러운 것입니다. 그래서 칼을 휘두르는 것은 잘 잘 잡아 더러운 나무를 잡아 내리는 것이 매우 힘듭니다. 그래서 나는 칼을 휘두르는 것을 좋아합니다. 그러나 칼을 ��������\n"
          ]
        }
      ],
      "source": [
        "# 출력을 생성합니다.\n",
        "outputs = pipe(one_shot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cugB9temhHJ0"
      },
      "source": [
        "### **프롬프트 체인: 문제 쪼개기**\n",
        "\n",
        "- 여러 프롬프트로 분할하기\n",
        "- **한 프롬프트의 출력을 다음 프롬프트의 입력으로 사용**하는 식으로 연속적인 상호작용 체인을 만들어 문제를 해결한다. --> **순차적인 파이프라인 생성**\n",
        "- 각각의 호출에 파이프 매개변수를 다르게 지정할 수 있다는 것이 장점\n",
        "-  사용 사례 예:\n",
        "    - **응답 유형성 검사** :\n",
        "        - 이전에 생성한 출력을 제확인하도록 LLM에게 요청한다.\n",
        "    - **병렬 프롬프트** :  \n",
        "        - 여러 개의 프롬프트를 병렬로 만들고 최종 단계에서 병합한다. 예를 들어 복수의 LLM에게 여러 개의 레시피를 병렬로 생성하도록 요청한다.\n",
        "        - 그 다음 이 결과를 합쳐서 쇼핑 목록을 만들도록 한다.\n",
        "    - **이야기 작성** :\n",
        "        - LLM을 활용하여 문제를 여러 요소로 나누는 방식을 사용해 책이나 이야기를 작성한다.\n",
        "        - 예를 들어, 먼저 요약을 작성하고, 캐릭터를 개발하고, 핵심 장면을 만들고, 그 다음 대화를 만드는 단계로 넘어간다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🆚 비교: 체인 vs 한 번에\n",
        "\n",
        "#### 한번에 (단점)\n",
        "-  모든 단계에 같은 매개변수 적용\n",
        "result = pipe(\n",
        "    \"아이디어 내고, 분석하고, 홍보 문구 만들어줘\",\n",
        "    temperature=0.7,  # 모든 단계에 동일!\n",
        "    max_new_tokens=500\n",
        ")\n",
        "\n",
        "→ 창의적인 부분은 부족하고, 정확해야 할 부분은 불안정\n",
        "\n",
        "#### 체인 (장점)\n",
        "- 각 단계마다 최적 매개변수\n",
        "idea = pipe(..., temperature=1.2)      # 창의적\n",
        "analysis = pipe(..., temperature=0.3)  # 정확\n",
        "promo = pipe(..., temperature=0.9)     # 다시 창의적\n",
        "\n",
        "→ 각 단계의 목적에 최적화!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 매개변수 선택 가이드\n",
        "\n",
        "| 작업 유형 | temperature | max_tokens | top_p |\n",
        "|----------|-------------|------------|-------|\n",
        "| **사실 확인** | 0.1-0.3 | 짧게 | 0.8 |\n",
        "| **분류/분석** | 0.2-0.4 | 짧게 | 0.85 |\n",
        "| **요약** | 0.3-0.5 | 짧게 | 0.9 |\n",
        "| **설명** | 0.5-0.7 | 중간 | 0.9 |\n",
        "| **창작/마케팅** | 0.8-1.2 | 길게 | 0.95 |\n",
        "| **브레인스토밍** | 1.0-1.5 | 중간 | 0.95 |\n",
        "\n",
        "---\n",
        "\n",
        "## 정리\n",
        "\n",
        "**매개변수 = LLM 생성 설정값**\n",
        "\n",
        "- `temperature`: 창의성\n",
        "- `max_new_tokens`: 길이\n",
        "- `top_p`: 품질/다양성\n",
        "- `do_sample`: 확률적/결정적\n",
        "\n",
        "**체인의 장점 = 단계마다 최적 설정 가능**\n",
        "\n",
        "- 창의적 단계 → temperature 높게\n",
        "\n",
        "- 정확한 단계 → temperature 낮게\n",
        "\n",
        "- 짧은 출력 → max_tokens 적게\n",
        "\n",
        "- 긴 출력 → max_tokens 많게"
      ],
      "metadata": {
        "id": "fmycfkJ7XESG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXYuFn9eG2t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638850cf-2560-48f9-e99e-537f2fb47da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 민지\n"
          ]
        }
      ],
      "source": [
        "# 제품 이름과 슬로건을 만듭니다.\n",
        "product_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"한국 여자 아이 이름 한 개를 지어줘.\"}\n",
        "]\n",
        "outputs = pipe(product_prompt)\n",
        "product_description = outputs[0][\"generated_text\"]\n",
        "print(product_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNYi3eDRG9Sk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a3f377-816c-46b3-fd26-85bc26431dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 민지 소개\n",
            "\n",
            "민지는 현대 기술에 맞춰 노력하는 인물입니다. 저는 컴퓨터 과학, 디자인, 및 프로그래밍 분야에서 많은 경력을 가지고 있습니다. 저의 경험은 컴퓨터 프로그래밍 업무에서 첫 번째로 작성한 코드 프로젝트에 대한 성공적인 경험을 있습니다. 또한, 저는 프로젝트 운영, 프로그래밍 실습, 그리고 프로젝트 프로토타입 등의 분야에서 많은 경험을 가지고 있습니다. 저의 목표는 새로운 기술을 개발하고, 기업에서 효율적인 프로그래밍 업무를 수행하는 데 도움을 줄 것입니다.\n"
          ]
        }
      ],
      "source": [
        "# 제품 이름과 슬로건을 바탕으로 홍보 문구를 생성합니다.\n",
        "sales_prompt = [\n",
        "    {\"role\": \"user\", \"content\": f\"다음 사람에 대한 홍보 문구를 생성합니다. : '{product_description}'\"}\n",
        "]\n",
        "outputs = pipe(sales_prompt)\n",
        "sales_pitch = outputs[0][\"generated_text\"]\n",
        "print(sales_pitch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SKyF7UGBUSz3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL1UpiV02V5O"
      },
      "source": [
        "## **생성 모델을 사용한 추론**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lQJhAnY2W3O"
      },
      "source": [
        "### **CoT**(**Chain-of-thought**): **응답하기 전에 생각하기**\n",
        "\n",
        "- **CoT(<mark>**Chain-of-thought**</mark>)의 목표**\n",
        "    - **생성 모델이 추론 과정 없이 바로 질문에 대답하지 않고 그 전에 생각하게 만드는 것**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnkxKmLyHGuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0992b447-82ac-4789-aa70-d6b2fbd88874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 식당에서 점심을 만들 때 사과를 사용했습니다. 총 사과 23개 중에서 사과를 20개 사용했습니다. 이 값을 식당에서 있던 사과 23개 빼면 남은 사과는 3개입니다. 그런데 식당에서 6개를 더 샀다면 이 3개가 6개로 증가할 것입니다. 따라서 식당에서 사과가 6개 더 있습니다.\n"
          ]
        }
      ],
      "source": [
        "# 추론없이 답변하기\n",
        "standard_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"로저는 테니스공 5개를 가지고 있습니다. 그는 테니스공 캔 두 개를 더 삽니다. 각 캔에는 테니스공이 3개씩 들어 있습니다. 이제 그는 테니스공을 몇 개 가지고 있습니까?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"11\"},\n",
        "    {\"role\": \"user\", \"content\": \"식당에는 사과 23개가 있었습니다. 점심을 만드는데 사과를 20개를 사용하고 6개를 더 샀다면 남은 사과는 몇 개입니까?\"}\n",
        "]\n",
        "\n",
        "# 출력을 생성합니다.\n",
        "outputs = pipe(standard_prompt) #standard_prompt : 추론 능력 x -> 오답 가능성 높음\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9jD3zPPEz_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b23967a-ac80-4689-c5d6-47e9f3461c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 식당에서 점심을 만들 때 사과를 사용했습니다. 총 사과 23개 중에서 사과를 20개 사용했습니다. 남은 사과는 23 - 20 = 3개입니다. 또한 6개를 더 샀기 때문에 3개 + 6개 = 9개가 남았습니다. 답은 9개입니다.\n"
          ]
        }
      ],
      "source": [
        "# CoT로 대답하기\n",
        "cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"로저는 테니스공 5개를 가지고 있습니다. 그는 테니스공 캔 두 개를 더 삽니다. 각 캔에는 테니스공이 3개씩 들어 있습니다. 이제 그는 테니스공을 몇 개 가지고 있습니까?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"로저는 공 5개로 시작했습니다. 테니스공 3개가 들어있는 캔 2개는 테니스공 6개입니다. 5 + 6 = 11입니다. 답은 11입니다.\"},\n",
        "    {\"role\": \"user\", \"content\": \"식당에는 사과 23개가 있었습니다. 점심을 만드는데 사과를 20개를 사용하고 6개를 더 샀다면 남은 사과는 몇 개입니까?\"}\n",
        "]\n",
        "\n",
        "# 출력을 생성합니다.\n",
        "outputs = pipe(cot_prompt) #cot_prompt : 생성모델이 생각하게 만드는 것. 추론 능력 덕분에 새로운 문제에도 대답 가능\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jVOOib72Z0P"
      },
      "source": [
        "### **제로-샷 CoT**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyHWAk2XKKGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807530dd-1484-499d-f35d-98d2d91b6804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. 식당에 있었던 사과의 수: 23개\n",
            "\n",
            "2. 점심을 만드는 사과의 수: 20개\n",
            "\n",
            "3. 새로운 사과 추가: 6개\n",
            "\n",
            "4. 총 사과 수: 20개 + 6개 = 26개\n",
            "\n",
            "5. 남은 사과 수: 23개 - 26개 = -3개\n",
            "\n",
            "\n",
            "따라서, 남은 사과는 음수가 나온 것으로 봤습니다. 이는 사과가 없는 상황을 나타냅니다. 즉, 사과가 없습니다.\n"
          ]
        }
      ],
      "source": [
        "# 제로-샷 CoT\n",
        "zeroshot_cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"식당에는 사과 23개가 있었습니다. 만약 20개를 점심을 만드는데 사용하고 6개를 더 샀다면, 남은 사과는 몇 개일까요? 단계별로 생각해 봅시다.\"}\n",
        "]\n",
        "\n",
        "# 출력을 생성합니다.\n",
        "outputs = pipe(zeroshot_cot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**자기 일관성**(**self-consistency**): 출력 샘플링\n",
        "  - 무작위성에 대응하고 생성 모델의 품질을 향상시키기 시키기 위해 자기 일관성 방법이 개발됨\n",
        "  - **생성 모델에 동일한 프롬프트를 여러 번 요청**하고 다수를 차지하는 결과를 최종 답변으로 내놓는 방법\n",
        "  - 이 과정에서 샘플링의 다양성을 증가시키기 위해 서로 다른 temperature와 top_p 값을 사용해 답변을 생성할 수 있다."
      ],
      "metadata": {
        "id": "6SXjQfc7wTg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "def format_messages(messages, tokenizer):\n",
        "    \"\"\"chat 템플릿 적용\"\"\"\n",
        "    try:\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception:\n",
        "        text = \"\"\n",
        "        for m in messages:\n",
        "            role = m.get(\"role\", \"user\")\n",
        "            content = m.get(\"content\", \"\")\n",
        "            text += f\"[{role.upper()}]\\n{content}\\n\\n\"\n",
        "        text += \"[ASSISTANT]\\n\"\n",
        "        return text\n",
        "\n",
        "def normalize_answer(ans):\n",
        "    \"\"\"답변 정규화\"\"\"\n",
        "    korean = {\"하나\": \"1\", \"둘\": \"2\", \"셋\": \"3\", \"넷\": \"4\",\n",
        "              \"다섯\": \"5\", \"여섯\": \"6\", \"일곱\": \"7\", \"여덟\": \"8\", \"아홉\": \"9\"}\n",
        "    for k, v in korean.items():\n",
        "        ans = ans.replace(k, v)\n",
        "\n",
        "    nums = re.findall(r'\\d+', ans.replace(\",\", \"\"))\n",
        "    return nums[0] if nums else ans.strip()\n",
        "\n",
        "num_pattern = re.compile(r\"(-?\\d+(?:[.,]\\d+)?)\")\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"생성 결과에서 답변 추출 + 정규화\"\"\"\n",
        "    # \"답은 X\" 패턴\n",
        "    m = re.search(r\"답\\s*[:은]\\s*([^\\n\\.]+)\", text)\n",
        "    if m:\n",
        "        cand = m.group(1).strip()\n",
        "        return normalize_answer(cand)\n",
        "\n",
        "    # 숫자 탐색\n",
        "    nums = num_pattern.findall(text.replace(\",\", \"\"))\n",
        "    if nums:\n",
        "        return normalize_answer(nums[-1])\n",
        "\n",
        "    # 마지막 줄\n",
        "    last_line = text.strip().splitlines()[-1]\n",
        "    return normalize_answer(last_line.strip())\n",
        "\n",
        "def remove_outliers(candidates):\n",
        "    \"\"\"이상치 제거 (선택사항)\"\"\"\n",
        "    try:\n",
        "        nums = [float(c) for c in candidates\n",
        "                if c.replace('.','').replace('-','').isdigit()]\n",
        "        if len(nums) > 3:\n",
        "            import statistics\n",
        "            mean = statistics.mean(nums)\n",
        "            stdev = statistics.stdev(nums)\n",
        "\n",
        "            filtered = []\n",
        "            for c in candidates:\n",
        "                if not c.replace('.','').replace('-','').isdigit():\n",
        "                    filtered.append(c)\n",
        "                elif abs(float(c) - mean) <= 2 * stdev:\n",
        "                    filtered.append(c)\n",
        "            return filtered\n",
        "    except:\n",
        "        pass\n",
        "    return candidates\n",
        "\n",
        "def self_consistent_generate(\n",
        "    messages,\n",
        "    n_samples=12,\n",
        "    temperatures=(0.4, 0.7, 0.9, 1.1),\n",
        "    top_ps=(0.7, 0.9, 0.95),\n",
        "    max_new_tokens=128,\n",
        "    seed=42,\n",
        "    use_outlier_removal=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    개선된 자기 일관성 생성\n",
        "    \"\"\"\n",
        "    prompt = format_messages(messages, tokenizer)\n",
        "    candidates = []\n",
        "    meta = []\n",
        "\n",
        "    combos = [(t, p) for t in temperatures for p in top_ps]\n",
        "    random.seed(seed)\n",
        "    chosen = [random.choice(combos) for _ in range(n_samples)]\n",
        "\n",
        "    for i, (t, p) in enumerate(chosen):\n",
        "        torch.manual_seed(seed + i)\n",
        "        out = pipe(\n",
        "            prompt,\n",
        "            do_sample=True,\n",
        "            temperature=float(t),\n",
        "            top_p=float(p),\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        ans = extract_answer(out)\n",
        "        candidates.append(ans)\n",
        "        meta.append({\n",
        "            \"temperature\": t,\n",
        "            \"top_p\": p,\n",
        "            \"raw\": out,\n",
        "            \"parsed\": ans\n",
        "        })\n",
        "\n",
        "    # 이상치 제거 (선택)\n",
        "    if use_outlier_removal:\n",
        "        candidates = remove_outliers(candidates)\n",
        "\n",
        "    # 다수결\n",
        "    tally = Counter(candidates)\n",
        "    best_ans, best_cnt = tally.most_common(1)[0]\n",
        "    confidence = best_cnt / n_samples\n",
        "\n",
        "    return {\n",
        "        \"majority_answer\": best_ans,\n",
        "        \"vote_count\": best_cnt,\n",
        "        \"confidence\": f\"{confidence:.1%}\",\n",
        "        \"confidence_level\": \"높음\" if confidence > 0.7 else \"중간\" if confidence > 0.5 else \"낮음\",\n",
        "        \"total_samples\": n_samples,\n",
        "        \"votes\": dict(tally),\n",
        "        \"samples_meta\": meta,\n",
        "    }\n",
        "\n",
        "# 개선된 프롬프트\n",
        "cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"로저는 테니스공 5개를 가지고 있습니다. 그는 테니스공 캔 두 개를 더 삽니다. 각 캔에는 테니스공이 3개씩 들어 있습니다. 이제 그는 테니스공을 몇 개 가지고 있습니까?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"로저는 공 5개로 시작했습니다. 테니스공 3개가 들어있는 캔 2개는 테니스공 6개입니다. 5 + 6 = 11입니다. 답은 11입니다.\"},\n",
        "    {\"role\": \"user\", \"content\": \"식당에는 사과 23개가 있었습니다. 점심을 만드는 데 20개를 사용하고 6개를 더 샀다면 사과는 몇 개입니까? 위 예시처럼 단계별로 계산하고 마지막에 '답은 X'로 답하세요.\"},\n",
        "     # 코틀릿(CoT) 유도 힌트: \"생각을 단계적으로 설명한 다음 '답은 X'로 끝내라\"\n",
        "    {\"role\": \"assistant\", \"content\": \"생각을 단계적으로 설명한 다음 마지막 줄에 '답은 (정답)' 형태로 끝내세요.\"}\n",
        "]\n",
        "\n",
        "result = self_consistent_generate(\n",
        "    messages=cot_prompt,\n",
        "    n_samples=15,\n",
        "    temperatures=(0.4, 0.7, 1.0),\n",
        "    top_ps=(0.85, 0.9, 0.95),\n",
        "    max_new_tokens=120,\n",
        "    seed=123,\n",
        "    use_outlier_removal=True\n",
        ")\n",
        "\n",
        "print(\"=== Self-Consistency 결과 (개선 버전) ===\")\n",
        "print(f\"최종 답: {result['majority_answer']}\")\n",
        "print(f\"득표: {result['vote_count']}/{result['total_samples']}회\")\n",
        "print(f\"신뢰도: {result['confidence']} ({result['confidence_level']})\")\n",
        "print(f\"득표 분포: {result['votes']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FNSgYoxweNe",
        "outputId": "e96ee423-6284-4664-bb47-550cdf91f593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Self-Consistency 결과 (개선 버전) ===\n",
            "최종 답: 6\n",
            "득표: 9/15회\n",
            "신뢰도: 60.0% (중간)\n",
            "득표 분포: {'9': 3, '3': 2, '6': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "def format_messages(messages, tokenizer):\n",
        "    \"\"\"\n",
        "    chat 템플릿이 있으면 사용, 없으면 단순 문자열 포맷으로 fallback.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception:\n",
        "        # 간단 포맷: role과 content를 순서대로 이어붙임\n",
        "        text = \"\"\n",
        "        for m in messages:\n",
        "            role = m.get(\"role\", \"user\")\n",
        "            content = m.get(\"content\", \"\")\n",
        "            text += f\"[{role.upper()}]\\n{content}\\n\\n\"\n",
        "        text += \"[ASSISTANT]\\n\"\n",
        "        return text\n",
        "\n",
        "# 2) 후보 생성 + 정답 추출 유틸\n",
        "num_pattern = re.compile(r\"(-?\\d+(?:[.,]\\d+)?)\")\n",
        "def extract_answer(text):\n",
        "    \"\"\"\n",
        "    생성 결과에서 '최종 답'을 추려냅니다.\n",
        "    - '답은 X' 같은 패턴 우선\n",
        "    - 숫자 없으면 마지막 줄을 트림\n",
        "    \"\"\"\n",
        "    # 우선 '답은' 패턴 탐색\n",
        "    m = re.search(r\"답\\s*[:은]\\s*([^\\n\\.]+)\", text)\n",
        "    if m:\n",
        "        cand = m.group(1).strip()\n",
        "        # cand에 숫자가 있으면 그 숫자, 아니면 cand 전체\n",
        "        m2 = num_pattern.search(cand.replace(\",\", \"\"))\n",
        "        return m2.group(1) if m2 else cand\n",
        "\n",
        "    # 숫자 전반 탐색\n",
        "    nums = num_pattern.findall(text.replace(\",\", \"\"))\n",
        "    if nums:\n",
        "        return nums[-1]  # 마지막으로 언급한 수를 채택(경험상 최종 계산치일 가능성↑)\n",
        "\n",
        "    # 숫자가 전혀 없으면 마지막 줄\n",
        "    last_line = text.strip().splitlines()[-1]\n",
        "    return last_line.strip()\n",
        "\n",
        "def self_consistent_generate(\n",
        "    messages,\n",
        "    n_samples=12,\n",
        "    temperatures=(0.4, 0.7, 0.9, 1.1),\n",
        "    top_ps=(0.7, 0.9, 0.95),\n",
        "    max_new_tokens=128,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"\n",
        "    다양한 temperature/top_p 조합으로 n_samples개 생성하여 다수결.\n",
        "    \"\"\"\n",
        "    prompt = format_messages(messages, tokenizer)\n",
        "    candidates = []\n",
        "    meta = []\n",
        "\n",
        "    # 조합 풀 만들기\n",
        "    combos = [(t, p) for t in temperatures for p in top_ps]\n",
        "    # n_samples에 맞게 랜덤 샘플링(중복 허용)\n",
        "    random.seed(seed)\n",
        "    chosen = [random.choice(combos) for _ in range(n_samples)]\n",
        "\n",
        "    for i, (t, p) in enumerate(chosen):\n",
        "        torch.manual_seed(seed + i)\n",
        "        out = pipe(\n",
        "            prompt,\n",
        "            do_sample=True,\n",
        "            temperature=float(t),\n",
        "            top_p=float(p),\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        ans = extract_answer(out)\n",
        "        candidates.append(ans)\n",
        "        meta.append({\"temperature\": t, \"top_p\": p, \"raw\": out, \"parsed\": ans})\n",
        "\n",
        "    # 다수결\n",
        "    tally = Counter(candidates)\n",
        "    best_ans, best_cnt = tally.most_common(1)[0]\n",
        "\n",
        "    return {\n",
        "        \"majority_answer\": best_ans,\n",
        "        \"vote_count\": best_cnt,\n",
        "        \"total_samples\": n_samples,\n",
        "        \"votes\": tally,\n",
        "        \"samples_meta\": meta,\n",
        "    }\n",
        "\n",
        "# 3) 예시: CoT + Self-Consistency 프롬프트\n",
        "#    - 첫 번째 QA로 CoT 풀이를 보여준 뒤, 두 번째 문제를 질의\n",
        "cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"로저는 테니스공 5개를 가지고 있습니다. 그는 테니스공 캔 두 개를 더 삽니다. 각 캔에는 테니스공이 3개씩 들어 있습니다. 이제 그는 테니스공을 몇 개 가지고 있습니까?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"로저는 공 5개로 시작했습니다. 테니스공 3개가 들어있는 캔 2개는 테니스공 6개입니다. 5 + 6 = 11입니다. 답은 11입니다.\"},\n",
        "    {\"role\": \"user\", \"content\": \"식당에는 사과 23개가 있었습니다. 점심을 만드는 데 20개를 사용하고 6개를 더 샀다면 사과는 몇 개입니까?\"},\n",
        "    # 코틀릿(CoT) 유도 힌트: \"생각을 단계적으로 설명한 다음 '답은 X'로 끝내라\"\n",
        "    {\"role\": \"assistant\", \"content\": \"생각을 단계적으로 설명한 다음 마지막 줄에 '답은 (정답)' 형태로 끝내세요.\"}\n",
        "]\n",
        "\n",
        "result = self_consistent_generate(\n",
        "    messages=cot_prompt,\n",
        "    n_samples=15,                # 표본 수(증가 시 안정성↑, 시간↑)\n",
        "    temperatures=(0.4, 0.7, 1.0),\n",
        "    top_ps=(0.85, 0.9, 0.95),\n",
        "    max_new_tokens=120,\n",
        "    seed=123\n",
        ")\n",
        "\n",
        "print(\"=== Self-Consistency 결과 ===\")\n",
        "print(\"다수결 최종 답:\", result[\"majority_answer\"])\n",
        "print(\"득표수/총샘플:\", result[\"vote_count\"], \"/\", result[\"total_samples\"])\n",
        "print(\"득표 분포:\", result[\"votes\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ79zvr4ODvB",
        "outputId": "c2f9b481-9211-4a73-a6d9-a2f27c84ce9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Self-Consistency 결과 ===\n",
            "다수결 최종 답: 6\n",
            "득표수/총샘플: 7 / 15\n",
            "득표 분포: Counter({'6': 7, '9': 3, '3': 2, '29': 1, '2': 1, '23': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3-bMVLT2ehH"
      },
      "source": [
        "### **ToT: 중간 단계 탐색**\n",
        "- 아이디어를 깊게 탐색할 수 있는 ToT(Tree-of-thought)\n",
        "- 동작 방법\n",
        "    - 여러 단계의 추론이 필요한 문제를 만났을 때 이 문제를 여러 단계로 나누고\n",
        "    - 각 단계에서 생성 모델이 당면한 문제를 위한 여러 다른 솔루션을 탐색하여\n",
        "    - 최상의 솔루션을 뽑고 다음 단계로 계속 이어가는 방법\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZJo-4NBI1V-"
      },
      "outputs": [],
      "source": [
        "# 제로-샷 ToT\n",
        "zeroshot_tot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"세 명의 전문가가 이 질문에 답한다고 가정해 보세요. 모든 전문가는 자신의 생각의 한 단계를 적어 그룹원들과 공유합니다. 그런 다음 모든 전문가가 다음 단계로 넘어가는 식으로 진행합니다. 어느 시점에서든 자신이 틀렸다는 것을 깨닫는 전문가가 있으면 그 자리에서 나갑니다. 질문은 \\\"식당에는 사과가 23개 있었습니다. 20개를 점심으로 만들고 6개를 더 샀다면, 그들은 사과를 몇 개 가지고 있을까요?\\\"입니다. 결과에 대해 반드시 토론하세요.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qieQ9waXLbZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40de9ada-5cd9-4d86-be9f-b188a9f55f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 이 문제는 숫자 덧셈 문제입니다. 질문에서 주어진 정보에 따라 답을 찾아야 합니다. \n",
            "\n",
            "\n",
            "- 식당에 있었던 사과의 수: 23개\n",
            "\n",
            "- 점심으로 만들었던 사과의 수: 20개\n",
            "\n",
            "- 더 샀던 사과의 수: 6개\n",
            "\n",
            "\n",
            "따라서, 점심으로 만들었던 사과의 수를 더한 사과의 수는 20개 + 6개 = 26개입니다. 따라서 식당에 있었던 사과의 수는 23개 + 6개 = 29개입니다.\n",
            "\n",
            "\n",
            "따라서 식당에 있었던 사과는 29개, 점심으로 만들었던 사과는 26개입니다.\n"
          ]
        }
      ],
      "source": [
        "# 출력을 생성합니다.\n",
        "outputs = pipe(zeroshot_tot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NhoKfsg3Y3YG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyCw3A5GZ61D"
      },
      "source": [
        "## **출력 검증**\n",
        "- 생성 모델로 만든 시스템과 애플리케이션은 제품에 투입될 수 있다.\n",
        "- 애플리케이션이 고장 나는 것을 막고 안정된 생성AI 애플리케이션을 만들기 위해서는 모델의 출력을 검증하고 제어하는 것이 중요하다.\n",
        "- **출력을 검증하는 이유**\n",
        "    - **구조적인 출력** : 대부분의 생성모델은 자유로운 형식 텍스트를 만들지만 일부 사용 사례에서는 JSON 같은 특정 포맷의 구조를 가진 출력이 필요하다.\n",
        "    - **유효한 출력** : 예를 들어 둘 중 하나를 선택하여 출력하라고 요청했을 때 모델이 다른 것을 선택하면 안된다.\n",
        "    - **윤리** : 욕설, 개인 식별 정보, 편향, 문화적 고정 관념등이 출력에 포함되지 않아야 한다.\n",
        "    - **정확성** : 특정 표준이나 성능을 따라야 한다. 생성된 정보가 사실적으로 정확하고 일관성이 있고, 환곽이 없는지 재확인해야 한다.\n",
        "- **출력을 제어하는 방법 3가지**\n",
        "    - **예시** : 기대하는 출력의 예시를 여러 개 제공한다.\n",
        "    - **문법** : 토큰 선택 과정을 제어한다.\n",
        "    - **미세 튜닝** :  기대 출력이 포함된 데이터에서 모델을 튜닝한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi67V-xNZ8fS"
      },
      "source": [
        "### **예시 제공**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBc_HuJQY8sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f512f96-7b9d-4868-c587-af010bdc9525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ```json\n",
            "\n",
            "{\n",
            "\n",
            "  \"name\": \"Alden\",\n",
            "\n",
            "  \"class\": \"Warrior\",\n",
            "\n",
            "  \"level\": 1,\n",
            "\n",
            "  \"stats\": {\n",
            "\n",
            "    \"strength\": 15,\n",
            "\n",
            "    \"dexterity\": 10,\n",
            "\n",
            "    \"constitution\": 14,\n",
            "\n",
            "    \"intelligence\": 8,\n",
            "\n",
            "    \"wisdom\": 12,\n",
            "\n",
            "    \"charisma\": 10\n",
            "\n",
            "  },\n",
            "\n",
            "  \"equipment\": {\n",
            "\n",
            "    \"weapon\": \"Sword of Valor\",\n",
            "\n",
            "    \"armor\": \"Leather Tunic\",\n",
            "\n",
            "    \"accessories\": [\"Healing Potion\", \"Shield of Fortitude\"]\n",
            "\n",
            "  },\n",
            "\n",
            "  \"skills\": {\n",
            "\n",
            "    \"combat\": 8,\n",
            "\n",
            "    \"stealth\": 5,\n",
            "\n",
            "    \"alchemy\": 3,\n",
            "\n",
            "    \"riddle\": 4\n",
            "\n",
            "  },\n",
            "\n",
            "  \"background\": \"Alden was a blacksmith's apprentice who discovered his true calling in battle. He's known for his bravery and unwavering loyalty to his friends.\"\n",
            "\n",
            "}\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# 제로-샷 학습: 예시 없음\n",
        "zeroshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"JSON 형식으로 RPG 게임의 캐릭터 프로필을 만듭니다.\"}\n",
        "]\n",
        "\n",
        "# 출력을 생성합니다.\n",
        "outputs = pipe(zeroshot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cc1U9TKZJkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17ffae8-d8ef-42ba-f1b4-8c80308d9fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ```json\n",
            "\n",
            "{\n",
            "\n",
            "  \"description\": \"작은 저녁 잔치 속삭이 되어 있는 저녁 놀이에 참여하는 캐릭터\",\n",
            "\n",
            "  \"name\": \"작은 저녁 잔치 속삭이\",\n",
            "\n",
            "  \"armor\": \"작은 저녁 잔치 속삭이 착용한 옷\",\n",
            "\n",
            "  \"weapon\": \"작은 저녁 잔치 속삭이 착용한 치마\"\n",
            "\n",
            "}\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# 원-샷 학습: 출력 구조에 대한 예시를 제공합니다.\n",
        "one_shot_template = \"\"\"RPG 게임의 짧은 캐릭터 프로필을 만드세요. 이 형식만 사용하세요.:\n",
        "\n",
        "{\n",
        "  \"description\": \"간단한 설명\",\n",
        "  \"name\": \"캐릭터의 이름\",\n",
        "  \"armor\": \"한 조각의 갑옷\",\n",
        "  \"weapon\": \"하나 이상의 무기\"\n",
        "}\n",
        "\"\"\"\n",
        "one_shot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": one_shot_template}\n",
        "]\n",
        "\n",
        "# 출력을 생성합니다.\n",
        "outputs = pipe(one_shot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR3QSswQ2kAq"
      },
      "source": [
        "### **문법: 제약 샘플링** (개인 공부)\n",
        "\n",
        "- 퓨샷(few-shot) 학습의 단점\n",
        "    - 특정 출력이 생성되는 것을 명시적으로 막을 수 없다.\n",
        "    - 모델에게 가이드와 지시 사항을 제공하지만 모델이 이를 완전히 따르지 않을 수 있다.\n",
        "- **생성 모델의 출력을 제어하고 검증하기 위한 패키지**\n",
        "    - Guidance : https://github.com/guidance-ai/guidance\n",
        "    - Guardrails : https://github.com/guardrails-ai/guardrails\n",
        "    - LMQL : https://github.com/eth-sri/lmql"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **llama-cpp-python**\n",
        "- llama-cpp-python은 llama.cpp 라이브러리의 파이썬 바인딩이다\n",
        "    - 기본으로 설치되는 CPU 버전은 오랜 시간이 거리기때문에 CUDA같은 하드웨어 가속기를 지원한는 whl 파일을 깃허브에서 다우로드하여 설치한다.\n",
        "    - https://github.com/abetlen/llama-cpp-python/releases\n",
        "- 코랩에서 llama-cpp-python 설치하기\n",
        "- https://github.com/abetlen/llama-cpp-python\n"
      ],
      "metadata": {
        "id": "L8OQn8IF6PaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# 사용하는 파이썬과 CUDA 버전에 맞는 llama-cpp-python 패키지를 설치하세요.\n",
        "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "BVkaObXueXfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "J_MyXQy-ezxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 whl 파일 설치시 오류가 발생하면 아래 1단계 > 2단계 > 3단계 방법을 사용해 본다."
      ],
      "metadata": {
        "id": "OcCqyiiHe1Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **1단계: GPU 런타임 확인 및 설정**"
      ],
      "metadata": {
        "id": "EYJn2P3VcCTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 사용 가능 여부 확인\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"🔍 현재 Python 버전 확인:\")\n",
        "print(f\"Python {sys.version}\\n\")\n",
        "\n",
        "print(\"🔍 GPU 사용 가능 여부 확인:\")\n",
        "try:\n",
        "    gpu_info = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT)\n",
        "    print(gpu_info.decode('utf-8'))\n",
        "    print(\"✅ GPU를 사용할 수 있습니다!\")\n",
        "    USE_GPU = True\n",
        "except:\n",
        "    print(\"❌ GPU를 사용할 수 없습니다. (CPU 모드로 진행)\")\n",
        "    print(\"💡 런타임 > 런타임 유형 변경 > T4 GPU 선택을 권장합니다.\")\n",
        "    USE_GPU = False"
      ],
      "metadata": {
        "id": "YHnb6Kf86A5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **2단계: Colab 최적화 설치** (GPU 지원)"
      ],
      "metadata": {
        "id": "7Zmmbezc6Isf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab에서 GPU 가속 llama-cpp-python 설치\n",
        "# 이 방법이 가장 빠르고 안정적입니다\n",
        "\n",
        "if USE_GPU:\n",
        "    print(\"🚀 GPU 가속 버전 설치 중...\")\n",
        "    # CUDA 지원 버전 설치\n",
        "    !CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python --force-reinstall --no-cache-dir -q\n",
        "    print(\"✅ GPU 가속 버전 설치 완료!\")\n",
        "else:\n",
        "    print(\"🐢 CPU 전용 버전 설치 중...\")\n",
        "    !pip install llama-cpp-python -q\n",
        "    print(\"✅ CPU 버전 설치 완료!\")\n",
        "\n",
        "# 추가 필요 패키지 설치\n",
        "!pip install huggingface-hub -q\n",
        "\n",
        "print(\"\\n📦 모든 패키지 설치 완료!\")"
      ],
      "metadata": {
        "id": "BYSKh5gY6HAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3단계: 설치 검증"
      ],
      "metadata": {
        "id": "yolijL8i6U2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 설치 확인 및 버전 체크\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "    import llama_cpp\n",
        "\n",
        "    print(\"✅ llama-cpp-python 설치 성공!\")\n",
        "    print(f\"📌 버전: {llama_cpp.__version__}\")\n",
        "\n",
        "    # GPU 사용 가능 여부 확인\n",
        "    if USE_GPU:\n",
        "        print(\"🎮 GPU 가속이 활성화되었습니다.\")\n",
        "    else:\n",
        "        print(\"💻 CPU 모드로 실행됩니다.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"❌ 설치 실패: {e}\")\n",
        "    print(\"다시 설치를 시도해주세요.\")"
      ],
      "metadata": {
        "id": "YVfz2doh6XG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2Qm7fnD86WJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L06f1CYoc9Pp"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "del model, tokenizer, pipe\n",
        "\n",
        "# 메모리를 비웁니다.\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-YykcIru20X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b1b4e03927ff4fc4a990a1da2d1cf4c1",
            "71ac0dbb383e4c529c1e226e0b4a99c4",
            "d40d58bbc4fb443b9e4b345cf666509e",
            "e48c8d5576884349ba118a9953961026",
            "2a10747d00f1411e8bfed6111e5d1fcc",
            "7cfaf21fb4314ed1b237caff05122820",
            "ad4794dcf2e54cedac9ea298a311de18",
            "7db2eef08fb14a1896a3f14b7f452760",
            "f8d4430de3ff45bb954de83c3ceaad79",
            "df565a6bf7ba4ffcbd9bfa799b80113b",
            "b405ac7c5d8141c69440697b3a6d9202"
          ]
        },
        "outputId": "0545f576-4259-4237-de78-2493eec09d0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "./Phi-3-mini-4k-instruct-fp16.gguf:   0%|          | 0.00/7.64G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1b4e03927ff4fc4a990a1da2d1cf4c1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_cpp.llama import Llama\n",
        "\n",
        "# Phi-3를 로드합니다.\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    filename=\"*fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=4096,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# fp16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j9TYBVIgti6"
      },
      "outputs": [],
      "source": [
        "# 출력을 생성합니다.\n",
        "output = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"JSON 형식으로 RPG용 전사를 만듭니다.\"},\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0,\n",
        ")['choices'][0]['message'][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp7FBjFdsbHe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# JSON 문자열을 로드합니다.\n",
        "json_output = json.dumps(json.loads(output), indent=4)\n",
        "print(json_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##프롬프트 기법 참고 기사 (AI에 다양한 답변을 요구하는 방법)\n",
        "https://www.aitimes.com/news/articleView.html?idxno=203264\n",
        "- 버벌라이즈드 샘플링"
      ],
      "metadata": {
        "id": "_h0_UplM0dTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 클로드 프롬프트 엔지니어링\n",
        "https://docs.claude.com/ko/docs/build-with-claude/prompt-engineering/overview"
      ],
      "metadata": {
        "id": "PsrL2GRl1Xt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [미션] 프롬프트 엔지니어링 적용하기\n",
        "1. [필수] 적절한 모델 선정하여(2개 이상 선정 후 비교) **고급 프롬프트 엔지니어링 방법** 따라서 적용해보기.\n",
        "  - 복잡한 프롬프트\n",
        "  - 문맥 내 학습: 예시 제공 방법\n",
        "  - 프롬프트 체인: 문서 쪼개기\n",
        "  - CoT 추론\n",
        "  - ToT: 중간 단계 탐색\n",
        "  - 출력검증: 예시 제공\n",
        "  - 파일명: 20251021_고급프롬프트 엔지니어링 방법_장지현.ipynb\n",
        "\n",
        "2. [선택] base model 선정 -> 미세튜닝(+ 추가 데이터 추가학습) -> 성능 지표 검증 -> 프롬프트 엔지니어링 적용 -> 출력 검증\n",
        "  - 목표 정의 ( ~ 하는 OOO LLM 모델 만들기 )\n",
        "  & 성능 지표 선택\n",
        "  - LLM 모델링\n",
        "    - 베이스모델 선정\n",
        "    - 미세 튜닝(+ 추가 데이터 학습)\n",
        "    - 성능 지표 검증\n",
        "    - 프롬프트 엔지니어링 적용\n",
        "    - 출력 검증\n",
        "    - 파일명: 20251021_OOO LLM 모델 만들기_장지현.ipynb"
      ],
      "metadata": {
        "id": "XEKlre1X2zV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 미션 1 - 고급 프롬프트 엔지니어링 방법 적용\n",
        "\n",
        "(2개 이상 모델 선정 후 비교)"
      ],
      "metadata": {
        "id": "QbmUNEIIgwtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  모델 선정\n",
        "1. `Qwen/Qwen2.5-3B-Instruct` (소형)\n",
        "  - 한글 성능 우수 (다국어 모델)\n",
        "  - 빠른 추론 속도\n",
        "  - 작은 파라미터 (3B)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WvL1pW5LiS0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS = {\n",
        "    \"Qwen2.5-3B\": {\n",
        "        \"id\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        \"size\": \"3B\",\n",
        "        \"description\": \"소형, 빠른 추론\"\n",
        "    },\n",
        "    \"Phi-3-mini\": { # 교안에서 씀\n",
        "        \"id\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        \"size\": \"3.8B\",\n",
        "        \"description\": \"중소형, MS 모델, 교안 메인\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "3Ebk1-g_kmrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델별 최적 do_sample, 추가 파라미터 설정 가이드\n",
        "\n",
        "| 모델 | do_sample | 이유 | 추가 파라미터 |\n",
        "|------|-----------|------|---------------|\n",
        "| **Llama 2/3** | ✅ True | • 대화형 모델로 창의성/다양성 중요<br>• 긴 텍스트 생성에 적합<br>• 반복 생성 방지 | `temperature=0.7`<br>`top_p=0.9` |\n",
        "| **GPT-2** | ✅ True | • 창작 글쓰기에 특화<br>• 다양한 스타일 생성 필요<br>• 작은 모델이라 샘플링으로 품질 향상 | `temperature=0.8`<br>`top_k=50` |\n",
        "| **Phi-3** | ❌ False | • Instruction-following 특화<br>• 정확한 답변이 우선<br>• 작은 모델로 일관성 중요 | `max_new_tokens=512` |\n",
        "| **CodeLlama** | ❌ False | • 코드 정확성이 필수<br>• 문법 오류 최소화<br>• 실행 가능한 코드 생성 목표 | `num_beams=5`<br>`early_stopping=True` |\n",
        "| **Mistral 7B** | ✅ True | • 범용 대화 모델<br>• 균형잡힌 창의성과 정확성<br>• 효율적인 7B 모델 | `temperature=0.6`<br>`top_p=0.95` |\n",
        "| **Falcon** | ✅ True | • 대규모 다목적 모델<br>• 창의적 작업에 강점<br>• 다양한 도메인 지원 | `temperature=0.7`<br>`repetition_penalty=1.1` |\n",
        "| **T5/Flan-T5** | ❌ False | • Task-specific fine-tuning<br>• 정확한 변환 작업 (번역, 요약)<br>• Encoder-Decoder 구조 특성 | `num_beams=4`<br>`length_penalty=2.0` |\n",
        "| **BERT-based** | ❌ False | • 분류/이해 작업 중심<br>• 마스크 언어 모델<br>• 생성보다는 이해에 특화 | 생성 작업에 비추천 |\n",
        "| **ChatGLM** | ✅ True | • 중국어 대화 특화<br>• 대화형 상호작용 설계<br>• 문화적 뉘앙스 표현 필요 | `temperature=0.8`<br>`top_p=0.8` |\n",
        "| **Vicuna** | ✅ True | • ChatGPT 스타일 모방<br>• 자연스러운 대화 중요<br>• 다양한 톤과 스타일 지원 | `temperature=0.7`<br>`top_p=0.9` |\n",
        "\n",
        "## 📋 사용 목적별 가이드\n",
        "\n",
        "### 🎨 **창의적 작업** → do_sample=True\n",
        "- **모델**: Llama, GPT-2, Mistral\n",
        "- **용도**: 소설 쓰기, 브레인스토밍, 대화\n",
        "\n",
        "### 🎯 **정확성 중요** → do_sample=False  \n",
        "- **모델**: Phi-3, CodeLlama, T5\n",
        "- **용도**: 번역, 요약, 코딩, QA\n",
        "\n",
        "### ⚖️ **균형 잡힌 접근**\n",
        "- **High creativity**: `temperature=0.8-1.0`\n",
        "- **Moderate creativity**: `temperature=0.6-0.7`  \n",
        "- **Conservative**: `temperature=0.1-0.3`\n",
        "\n",
        "## 💡 실전 팁\n",
        "\n",
        "1. **대화형 챗봇**: 항상 `do_sample=True`\n",
        "2. **코드 생성**: 항상 `do_sample=False`\n",
        "3. **번역**: `do_sample=False` + beam search\n",
        "4. **창작**: `do_sample=True` + 높은 temperature"
      ],
      "metadata": {
        "id": "bUlMCprm1wGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 공통 설정\n",
        "import gc\n",
        "import torch\n",
        "import warnings\n",
        "import logging\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from collections import Counter\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "\n",
        "# 경고 숨기기\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "\n",
        "# 유틸리티\n",
        "def load_model(model_id):\n",
        "    \"\"\"모델 로드\"\"\"\n",
        "    print(f\"\\n⏳ 모델 로딩 중: {model_id}\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"cuda\",\n",
        "        torch_dtype=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # pad_token 설정\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=500,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    print(\"✅ 로딩 완료\")\n",
        "    return model, tokenizer, pipe\n",
        "\n",
        "\n",
        "def cleanup_model(*objs):\n",
        "    # objs: model, tokenizer, pipe 등 전달\n",
        "    for o in objs:\n",
        "        try:\n",
        "            del o\n",
        "        except Exception:\n",
        "            pass\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"👌 메모리 정리 완료\")\n",
        "\n",
        "\n",
        "def measure_time(func):\n",
        "    \"\"\"실행 시간 측정 데코레이터\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        elapsed = time.time() - start\n",
        "        return result, elapsed\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def print_comparison(results):\n",
        "    \"\"\"결과 비교 출력\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"결과 비교\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for model_name in MODELS.keys():\n",
        "        print(f\"\\n[{model_name}]\")\n",
        "        print(\"-\"*70)\n",
        "        print(f\"출력: {results[model_name]['output']}\")\n",
        "        print(f\"실행 시간: {results[model_name]['time']:.2f}초\")"
      ],
      "metadata": {
        "id": "8qros13Tkdbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "m3WyMoWUL7-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 아주 간단한 프롬프트로 테스트\n",
        "simple_prompt = \"안녕하세요. 인공지능에 대해 한 줄로 설명해주세요.\"\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": simple_prompt}]\n",
        "    output = pipe(messages)[0][\"generated_text\"]\n",
        "\n",
        "    print(f\"{model_name}: {output}\")\n",
        "    cleanup_model(model, tokenizer, pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "262b5b65df1a4e71b836548c4d29207e",
            "fe117f3125c843a9ba3b517e785cb47a",
            "e2e7da7c44ad464483a7fc87792c6811",
            "f6cd2cee35d44b048884fecbeb2858bd",
            "b27afb3c94d746ed91f42f3bff3e82b6",
            "5181cbb97150484ca7a1ca66a6eed255",
            "c433679d064b4b9faf1c6919a466e8bd",
            "c6fd0d51a19b49e7a8825ab175e09abe",
            "2f2c2b0a7a7742a89550ae718c5d6cb9",
            "427ee68ca24c4d68919f3213bad72346",
            "5404d01e04ce4890a2480656411752c8",
            "a3efb2ec15f74366801334ea6270b839",
            "3d425a841e29449492a605f1c4f285a1",
            "464d12286d37434da194eb7eeb691d97",
            "ec13507991bd42b09ab02a424a225aa8",
            "7d6e15ee857a4bc4beee36d37bd7bcb3",
            "9d00415acc0f49bda79c1edeecd42ae7",
            "842fec0ad457410c8fb48e801a105c88",
            "2be77a5fe66e409cad0bf50a58da669e",
            "e91b3439baa64a319d9b413bf63b59ab",
            "2adeaa86918e4512ae0d6517d6fba030",
            "755d91cad92642f18b1e388dc62398a0"
          ]
        },
        "id": "Bgj999dhDm5J",
        "outputId": "71a77bee-84e6-4b18-d78d-ea3bb337d8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ 모델 로딩 중: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "262b5b65df1a4e71b836548c4d29207e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "Qwen2.5-3B: 안녕하세요. 인공지능은 컴퓨터 시스템이 지정된 작업을 이해하고 수행하는 능력입니다.\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "⏳ 모델 로딩 중: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3efb2ec15f74366801334ea6270b839"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "Phi-3-mini:  인공지능은 기계학습을 통해 사람들이 개발하는 컴퓨터 시스템의 일종입니다. 이는 다양한 기법을 활용하여 다음과 같은 종류의 기능을 수행합니다:\n",
            "\n",
            "- 문자 추출 (OCR) : 영상에서 판단하여 문자를 읽어내고, 그 문장을 입력 데이터로 만들어 실행할 수 있습니다.\n",
            "- 질문 및 답변 : 사용자가 질문을 제출하면, 인공지능이 정확한 답변을 제공합니다.\n",
            "- 생산 및 운영 업무 : 컴퓨터 시스템이 작업을 수행하거나 사회적 활동을 조율할 수 있습니다.\n",
            "- 데이터 분석 : 많은 데이터를 분석하고 올바른 결론을 도출하여 사회적 활동을 수행합니다.\n",
            "- 웹 서비스 : 사용자들이 웹 페이지에서 인공지능을 통해 서비스를 받을 수 있습니다.\n",
            "👌 메모리 정리 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 복잡한 프롬프트\n"
      ],
      "metadata": {
        "id": "99RuXdpHhNbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 복잡한 프롬프트 구성\n",
        "text = \"\"\"\n",
        "인공지능(AI)은 단순한 계산 자동화 기술을 넘어, 인간의 사고방식을\n",
        "모방하고 스스로 학습하는 시스템으로 진화하고 있다. 특히 딥러닝 기반의\n",
        "언어 모델과 이미지 생성 모델은 인간의 창의성을 위협할 정도의 표현력을\n",
        "보이고 있으며, 의료, 금융, 교육 등 다양한 분야에서 인간의 판단을 보조하거나\n",
        "대체하는 수준에 도달했다.\n",
        "\n",
        "하지만 이러한 기술의 발전은 동시에 여러 윤리적 문제를 동반하고 있다.\n",
        "대표적으로 ‘편향된 학습 데이터’로 인한 차별, 자동화로 인한 일자리 감소,\n",
        "그리고 인공지능의 의사결정 과정이 불투명하다는 점 등이 있다.\n",
        "AI가 내린 결정을 사람조차 설명할 수 없는 ‘블랙박스 문제’는 사회적 신뢰를\n",
        "저하시킬 수 있는 중요한 요인이다.\n",
        "\n",
        "최근에는 AI가 생성한 이미지나 텍스트가 실제 인간의 창작물과 구별하기\n",
        "어려운 수준으로 발전하면서, ‘저작권’과 ‘창작자성’에 대한 논의도 활발하다.\n",
        "특히 생성형 AI가 인터넷상의 데이터를 무단으로 학습하는 과정에서\n",
        "원저작자의 권리가 침해되는 사례가 보고되고 있다.\n",
        "\n",
        "한편, 기술 발전을 멈출 수 없다는 현실 속에서, ‘책임 있는 AI 개발’이라는\n",
        "새로운 방향성이 제시되고 있다. AI 모델의 학습 과정에서 데이터 출처를\n",
        "투명하게 공개하고, 편향을 줄이는 정량적 기준을 마련하는 것이\n",
        "그 핵심 과제로 떠오르고 있다. 또한 AI가 사회적으로 미칠 영향을\n",
        "사전에 예측하고, 인류 전체의 이익을 고려하는 거버넌스 체계 구축이\n",
        "중요하다는 목소리도 커지고 있다.\n",
        "\n",
        "결국 AI의 발전은 기술만의 문제가 아니라, 인간이 기술을 어떻게\n",
        "이해하고 활용할 것인가의 문제로 귀결된다. 기술의 속도보다\n",
        "윤리적 판단의 속도가 늦어질 때, 사회는 그 대가를 치르게 될지도 모른다.\n",
        "\"\"\"\n",
        "\n",
        "persona = \"당신은 AI 전문 기술 작가입니다.\\n\"\n",
        "instruction = \"제공된 텍스트의 핵심 내용을 요약하세요.반드시 한국어로만 답변하세요.\\n\"\n",
        "context = \"이 요약은 비전공자도 이해할 수 있어야 합니다.\\n\"\n",
        "data_format = \"3줄 이내로 요약하고, 핵심 키워드를 포함하세요.\\n\"\n",
        "audience = \"대상 독자는 AI에 관심 있는 일반인입니다.\\n\"\n",
        "tone = \"쉽고 친근한 어조를 사용하세요.\\n\"\n",
        "data = f\"요약할 텍스트: {text}\"\n",
        "\n",
        "complex_prompt = persona + instruction + context + data_format + audience + tone + data\n",
        "\n",
        "results_complex = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": complex_prompt}]\n",
        "\n",
        "    @measure_time\n",
        "    def generate():\n",
        "        return pipe(messages)[0][\"generated_text\"]\n",
        "\n",
        "    output, elapsed = generate()\n",
        "    results_complex[model_name] = {\"output\": output, \"time\": elapsed}\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "print_comparison(results_complex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483,
          "referenced_widgets": [
            "301a39cdc9774667b89bfbb23ba052af",
            "c94b12835f5049b6a64c7d49a8b2df89",
            "71bca364b57c4acf864241a36a7361c6",
            "72fd7f150a584b1ca865558eec0f62d8",
            "7b25ecc552374e8b824b0a39a8428851",
            "0478aebd661744659832399ffc7eab81",
            "6d07f4d899e2414aaa1e082abac3943e",
            "9884b648fce34cc9a171147c41150f8b",
            "eb5e6d83903c46c28c72a408f6540c49",
            "df2f10d4061046729877f451512fbb5e",
            "d9b82e8ec7fb4fc1b6a2aeddae6c3b3a",
            "dffc79f419e04a96a894398c1a8ebc8e",
            "6fce42eb9c0345078c4c8c026cc3b7e9",
            "46ad3289f5564a7e8621aa3895295fe9",
            "ca3eabfe9d0a4aa29cf2dff02a4f64f3",
            "c0e06b8b7f084cf0860e1e71225da3fe",
            "dbedd696062e434e82c2bda2613f8280",
            "04bef8e021d942cd9037f159f917ff18",
            "128cef724eb0428d961d3df88993d41f",
            "258f6f7dd8ad4aae9ddc13cb0260a786",
            "92fd871dc431425f983cd149c0386b77",
            "4a1e7807136041da8ec35f85399cb130"
          ]
        },
        "id": "KkEnyp9RledB",
        "outputId": "cad230fb-5b1d-476b-a183-2546f7594a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ 모델 로딩 중: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "301a39cdc9774667b89bfbb23ba052af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "⏳ 모델 로딩 중: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dffc79f419e04a96a894398c1a8ebc8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "======================================================================\n",
            "결과 비교\n",
            "======================================================================\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "출력: 인공지능은 이제 인간의 창의성까지 위협할 수 있는 강력한 기술이 되었습니다. 하지만 이 발전은 여러 윤리적 문제를 야기합니다. 편향된 학습 데이터로 인한 차별부터 저작권 논란까지, AI의 결정이 명확하지 않아 사회 신뢰를 잃을 수도 있습니다. 따라서 '책임있는 AI 개발'이 필요하며, 데이터 공개와 편향 해소를 위한 기준이 마련되어야 합니다. 결국 AI는 우리 모두가 어떻게 이를 이해하고 활용하느냐에 달려있습니다.\n",
            "실행 시간: 7.00초\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            "출력:  AI 기술은 잘 발전하며, 인간의 창의성을 포유되지만, 이에 함께 있는 윤리적 문제가 있으므로 활동은 점점 잠재적 불안정성으로 이어질 수 있다. '블랙박스 문제', 인간의 창작 권리, 인터넷 데이터 획득 관리, 저작권 문제, 사회적 영향 예측, 인류 이익 추구 계획이 중요한 요소들이 있음. 이러한 문제를 해결하려면 AI 개발 과정에서 투명한 기술 사용, 평가 방법 구축, 인류 이익 측정 방법 개발이 중요하다.\n",
            "실행 시간: 16.52초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문맥 내 학습: 예시 제공"
      ],
      "metadata": {
        "id": "NMVgrXbOhSDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot\n",
        "zeroshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"다음 리뷰의 감정을 분석하세요: '음식이 정말 맛있었어요!'\"}\n",
        "]\n",
        "\n",
        "# One-shot\n",
        "oneshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"리뷰: '서비스가 별로였어요.'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"감정: 부정\"},\n",
        "    {\"role\": \"user\", \"content\": \"리뷰: '음식이 정말 맛있었어요!'\"}\n",
        "]\n",
        "\n",
        "# Few-shot\n",
        "fewshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"리뷰: '서비스가 별로였어요.'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"감정: 부정\"},\n",
        "    {\"role\": \"user\", \"content\": \"리뷰: '분위기가 좋네요!'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"감정: 긍정\"},\n",
        "    {\"role\": \"user\", \"content\": \"리뷰: '그냥 그래요.'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"감정: 중립\"},\n",
        "    {\"role\": \"user\", \"content\": \"리뷰: '음식이 정말 맛있었어요!'\"}\n",
        "]\n",
        "\n",
        "shot_types = {\n",
        "    \"Zero-shot\": zeroshot_prompt,\n",
        "    \"One-shot\": oneshot_prompt,\n",
        "    \"Few-shot\": fewshot_prompt\n",
        "}\n",
        "\n",
        "results_shots = {model_name: {} for model_name in MODELS.keys()}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    for shot_name, prompt in shot_types.items():\n",
        "        output = pipe(prompt)[0][\"generated_text\"]\n",
        "        results_shots[model_name][shot_name] = output\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# 비교 출력\n",
        "print(\"\\nShot 방식별 비교:\")\n",
        "for shot_name in shot_types.keys():\n",
        "    print(f\"\\n[{shot_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    for model_name in MODELS.keys():\n",
        "        print(f\"{model_name}: {results_shots[model_name][shot_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "22d94dfac7854e989d784eefbc2c70a1",
            "faf258e5e84347d5ba60b43f87fcafee",
            "c0bf6dc9c2e544e9b5a883ea4e764a21",
            "1ef72aa6bfc445d0af481b649baef5f0",
            "e92506c05de347f58d9070436c3a09db",
            "14312edb6bcc4cbd90a00a4d4ffbf002",
            "5a9e0c18d72749a78b2d9bd2923323ad",
            "a3738df8678e4f9cb4f88ec28a700257",
            "6638ca2b2e5b42b696956a9c1826eee6",
            "f88cd6f5f1ba4e97b37de18577737b7d",
            "6a90d066f0ef48a689289ab38319d677",
            "731b3a0186a7492dba929618026d9dc4",
            "8df5d9b56e794659884a3369b9d314e0",
            "b60fa1b6f0684bbbba1edf80caaee6cc",
            "9a97465b22d54493b48a8e8db168c686",
            "76c31b107c24410cabba667e7fa7eb2b",
            "9233b61585e040c8b61954b31575be5c",
            "6c890a51d6924050a8c814a351266f0a",
            "be33be776fdf416fb18c3a420aae2c3e",
            "ce3a010a921d4fbfa803c54a47e70e85",
            "9bf4905687d743f99408c37efdda74d4",
            "50b304899ce64e5b892c89c55caea78f"
          ]
        },
        "id": "NGeNZWKOQvj_",
        "outputId": "20450aa3-6f03-4968-b8fd-9c92808603f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ 모델 로딩 중: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22d94dfac7854e989d784eefbc2c70a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "⏳ 모델 로딩 중: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "731b3a0186a7492dba929618026d9dc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "Shot 방식별 비교:\n",
            "\n",
            "[Zero-shot]\n",
            "----------------------------------------------------------------------\n",
            "Qwen2.5-3B: 이 리뷰는 매우 긍정적인 감정을 나타내고 있습니다. 사용자는 음식이 \"맛있었다\"고 표현했으며, 이는 명확한 만족감과 좋은 경험을 의미합니다. 단어 \"실로\"는 강조를 더해주며 더욱 높은 평가를 전달하고 있습니다.\n",
            "Phi-3-mini:  이 리뷰에서 나타나는 감정은 환호적인 상태를 나타내고 있습니다. 주어진 문장 \"음식이 정말 맛있었어요!\"는 음식이 맛있다고 얘기하고 있으며, 이는 음식을 선호하거나 기쁩니다. 이 결과로 나오는 감정은 환호적인 반면에 조용한 정직도가 보일 수 있습니다. 이 감정을 분석하려면 음식이 맛있다는 것을 인지하고 그 결과로 인해 사람의 피부나 몸에 좋은 영향을 미치는 것을 알 수 있습니다. 이 리뷰는 음식 품질의 좋은 성질을 강조하고 있으며, 이는 음식을 즐기는 경험을 표현하며 환호적인 감정을 나타낼 수 있습니다.\n",
            "\n",
            "[One-shot]\n",
            "----------------------------------------------------------------------\n",
            "Qwen2.5-3B: 감정: 긍정\n",
            "Phi-3-mini:  감정: 기쁨\n",
            "\n",
            "분석 내용:\n",
            "\n",
            "- 특정 서비스에 대해 주장하고 있으며, 이는 음식 제품에 대한 긍정적인 평가를 의미합니다.\n",
            "\n",
            "- 광범위한 정보를 제공하지 않으므로 상세한 내용을 분석할 수 없습니다. 그러나 일반적으로 음식이 맛있다는 말은 음식 제품이 원시적인 즐거 umami 등의 특성을 잃지 않고 완전히 맛보여주고 있다는 것을 의미합니다.\n",
            "\n",
            "- 감정: 기쁨 - 음식이 맛있다는 말에 따라, 음식 제품이 소중한 경험을 주는 것으로 감정적으로 감사드립니다.\n",
            "\n",
            "[Few-shot]\n",
            "----------------------------------------------------------------------\n",
            "Qwen2.5-3B: 감정: 긍정\n",
            "Phi-3-mini:  감정: 긍정\n",
            "\n",
            "이 리뷰에서는 음식을 만나고 시청하는 경향이 있음을 보여주고 있다. 사용자가 음식이 맛있다고 느낌표를 사용한 것으로, 음식을 굉장히 좋아하고 흥미롭게 느껴졌음을 나타내고 있다. 따라서 긍정의 감정을 선택하였다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 프롬프트 체인: 문서 쪼개기\n",
        "- 이름에 '민주'가 들어가니 민주주의 설명이 나온다 ,,"
      ],
      "metadata": {
        "id": "yc-bp1ggh2La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_chain = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    # 1단계: 이름 생성\n",
        "    step1_prompt = [\n",
        "        {\"role\": \"user\", \"content\": \"한국 여자 아이 이름 한 개를 지어줘. 이름만 답해줘.\"}\n",
        "    ]\n",
        "    name = pipe(step1_prompt, max_new_tokens=50)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # 2단계: 캐릭터 설정\n",
        "    step2_prompt = [\n",
        "        {\"role\": \"user\", \"content\": f\"'{name}'라는 이름의 판타지 소설 주인공 설정을 2-3줄로 만들어줘.\"}\n",
        "    ]\n",
        "    character = pipe(step2_prompt, max_new_tokens=200)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # 3단계: 홍보 문구\n",
        "    step3_prompt = [\n",
        "        {\"role\": \"user\", \"content\": f\"다음 캐릭터에 대한 흥미로운 홍보 문구를 작성해줘:\\n\\n{character}\"}\n",
        "    ]\n",
        "    promo = pipe(step3_prompt, max_new_tokens=200)[0][\"generated_text\"].strip()\n",
        "\n",
        "    results_chain[model_name] = {\n",
        "        \"step1_name\": name,\n",
        "        \"step2_character\": character,\n",
        "        \"step3_promo\": promo\n",
        "    }\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# 비교 출력\n",
        "print(\"\\n프롬프트 체인 결과:\")\n",
        "for model_name, chain in results_chain.items():\n",
        "    print(f\"\\n[{model_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"1단계 (이름): {chain['step1_name']}\")\n",
        "    print(f\"2단계 (캐릭터): {chain['step2_character']}\")\n",
        "    print(f\"3단계 (홍보): {chain['step3_promo']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "56ee5ab57a354641b4c04dcd1a5bf75e",
            "8b7fcbfbebc348b5894831ecf07d6b12",
            "2b2c6a82b0224f4788d9ff06cf81513c",
            "2c87a00bd33f4dee84549c744ffc8a79",
            "9da147912ef24a4baeb4d3cad28d4920",
            "9729f5ed735b406d9e7255c7a3beda57",
            "cc3cb453600b4cbe950298d1be649a9e",
            "8904393e326a421d91f40446cd15e2ba",
            "4b187dc3118149ba90857c817ee35eee",
            "24a2f4e909954fef92b6eaad9148c9da",
            "527b1347918f4d3cbe709f20ba26ab9d",
            "d1eb8e0963524bf295efa6330c69dfd2",
            "fe4dd619adac4d318d2170d4b65a12ef",
            "663202f3e0644055a2fd40b28702ebfd",
            "2df03c83d26a49bba52f997b0be9beab",
            "73552af3ad734864b83591c33aa3f4b1",
            "4617d513c57744fb986b878f69c0b6e9",
            "dc3251f417814716ae51306310024136",
            "d8a671938fdb4f668a42ada93e3e17e0",
            "2f7afae2f44049808273ba6ffc0fef54",
            "d6f9b22a0d724c819fbc07d41be00ef5",
            "6a9159c5e0f8452f873c36b3878d329f"
          ]
        },
        "id": "sJRbnbZcRyf9",
        "outputId": "cfd80ddb-c8bd-4191-f6af-f2bdcf04901a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ 모델 로딩 중: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ee5ab57a354641b4c04dcd1a5bf75e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "⏳ 모델 로딩 중: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1eb8e0963524bf295efa6330c69dfd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "프롬프트 체인 결과:\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "1단계 (이름): 서연\n",
            "2단계 (캐릭터): '서연', 그림 같은 투명한 피부와 검은 눈동자에 빛나는 파란 머리카락이 돋보이는 소녀. 서연은 세상에서 가장 순수하고 따뜻한 존재로, 언제나 다른 사람들을 위해 행동하는 천사처럼 아름다운 그녀는 판타지 세계를 더욱 밝게 비추는 빛이다.\n",
            "3단계 (홍보): \"서연: 천사의 미소, 순수함의 탄생\"\n",
            "이 광고 문구는 서연이라는 캐릭터가 가지고 있는 특징을 잘 전달합니다. '천사의 미소'는 그녀의 따뜻함과 사랑스러움을 표현하며, '순수함의 탄생'은 그녀가 세상에 가져다주는 영향력을 강조합니다. 또한, \"판타지 세계를 더욱 밝게 비추는 빛\"이라는 표현은 서연이 주는 긍정적인 에너지를 상징적으로 보여줍니다.\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            "1단계 (이름): 민주\n",
            "\n",
            "\n",
            "(Note: The provided solution is a common Korean name, Minju, which means \"democracy\". It's chosen as an example of simplicity and directness in response to the instruction.)\n",
            "2단계 (캐릭터): 민주는 결정적인 명사로서 민주주의를 나타내는 명사입니다. 한국에서 일본 양화 문제에 대한 개방적인 평화와 자유를 가진 민주주의를 바탕으로 이러한 이름을 선택하였습니다. 민주주의의 중요성과 역사적 영향을 담고 있는 이름은 현대 문화에서도 그리스어 영문에서도 민주주의를 의미하는 단어인 'D\n",
            "3단계 (홍보): \"민주주의는 우리 모두에게 헌신과 불안거나 불만을 잔들지 않는 데 중요한 원칙이 있습니다.\"\n",
            "\n",
            "\n",
            "\"민주주의는 국민의 희망과 평화를 위한 기본적인 문화적 아이디어입니다.\"\n",
            "\n",
            "\n",
            "\"민주주의는 개인의 존경과 공동체의 균형을 유지하기 위해 중요한 목적입니다.\"\n",
            "\n",
            "\n",
            "\"민주주의는\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoT 추론\n",
        "\n",
        "- 결과: zeroshot 보다 one shot이 더 효과적\n",
        "\n"
      ],
      "metadata": {
        "id": "iJ4aplOWh5U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot CoT\n",
        "zeroshot_cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "식당에는 사과 23개가 있었습니다.\n",
        "20개를 점심 만드는 데 사용하고 6개를 더 샀다면,\n",
        "남은 사과는 몇 개일까요?\n",
        "\n",
        "계산 과정 단계별로 생각해 봅시다.\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "# Few-shot CoT\n",
        "fewshot_cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"로저는 테니스공 5개를 가지고 있습니다. 캔 2개를 더 샀고, 각 캔에는 공이 3개씩 들어있습니다. 이제 몇 개일까요?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"로저는 공 5개로 시작했습니다. 캔 2개 × 공 3개 = 6개입니다. 5 + 6 = 11개입니다. 답은 11개입니다.\"},\n",
        "    {\"role\": \"user\", \"content\": \"식당에는 사과 23개가 있었습니다. 20개를 점심 만드는 데 사용하고 6개를 더 샀다면, 남은 사과는 몇 개일까요?\"}\n",
        "]\n",
        "\n",
        "cot_types = {\n",
        "    \"Zero-shot CoT\": zeroshot_cot_prompt,\n",
        "    \"Few-shot CoT\": fewshot_cot_prompt\n",
        "}\n",
        "\n",
        "results_cot = {model_name: {} for model_name in MODELS.keys()}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    for cot_name, prompt in cot_types.items():\n",
        "        output = pipe(prompt)[0][\"generated_text\"]\n",
        "        results_cot[model_name][cot_name] = output\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# 비교 출력\n",
        "print(\"\\nCoT 방식별 비교:\")\n",
        "for cot_name in cot_types.keys():\n",
        "    print(f\"\\n[{cot_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    for model_name in MODELS.keys():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(results_cot[model_name][cot_name][:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969,
          "referenced_widgets": [
            "23f39583e4a543bab895f9c898c543cc",
            "ca09f57e6cb24659a392b3dfb6c29740",
            "68139d154bd6456281340ebf800b3390",
            "a6a30eb4f5fa4845830d88b312b905b3",
            "210ca520ee6641b29f8c53642ebf9eda",
            "d16d036e031747ebab8591737a577eff",
            "fcb631bc71ec4cb5b44b7ec3545ff219",
            "b33868980f6a4d7d97aca6c20fb4a88a",
            "b273b8bf2a9845ca99a7b77cc7ec4868",
            "00f2835b66864eb9959e5e62538f1b95",
            "a1c1a1867e874fc695ce1b754c52608d",
            "e667d651b80d4ce3b38950d0157f494a",
            "92e11363029a4c78bdea19d7c3464b9d",
            "2dd795abcbfe4690a2d80722b8e565d0",
            "dbba9d3d33eb4a879c281105af6c2e13",
            "ac0d3a0cf1064855b259b332d88feec5",
            "f26bdd4ec2e9463f9de99908bcbdbcb4",
            "3028a7ee8a8748aa9bec65cc15baef16",
            "b52e9815155f471dbded150d9a31bfdd",
            "2a31aeaf557f43f1b180706d0f2fc51f",
            "58c2e66f8a7146f29caa95bb4ae813a2",
            "63f159d65d0d4ad1802031232802eb92"
          ]
        },
        "id": "mIzykiFrSmfG",
        "outputId": "5fcb173f-bc34-4315-8702-c60351608bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ 모델 로딩 중: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23f39583e4a543bab895f9c898c543cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "⏳ 모델 로딩 중: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e667d651b80d4ce3b38950d0157f494a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "CoT 방식별 비교:\n",
            "\n",
            "[Zero-shot CoT]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Qwen2.5-3B:\n",
            "물론입니다! 문제를 단계별로 해결해 보겠습니다.\n",
            "\n",
            "1) **초기 상태**: 식당에 있는 사과의 수는 23개입니다.\n",
            "   \n",
            "2) **사과 사용**: 20개를 점심을 만들 때 사용합니다.\n",
            "   - 남아있는 사과의 수 = 초기 상태의 사과 수 - 사용한 사과 수\n",
            "   - 남아있는 사과의 수 = 23 - 20 = 3개\n",
            "   \n",
            "3) **추가 구입**: 남아있는 3개...\n",
            "\n",
            "Phi-3-mini:\n",
            " 1. 식당에서 처음 들어왔을 때 사과의 수: 23개\n",
            "\n",
            "2. 점심 만들기 위해 사용했던 사과의 수: 20개\n",
            "\n",
            "3. 새로운 사과 추가: 6개\n",
            "\n",
            "4. 총 사과의 수 계산: 23개 (첫 인덱스) + 6개 (두 번째 인덱스) = 29개\n",
            "\n",
            "5. 사과가 사용된 사과 수: 20개\n",
            "\n",
            "6. 남은 사과의 수 계산: 29개 (총 사과 수) - 20개 (사용된 사과 수) = 9...\n",
            "\n",
            "[Few-shot CoT]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Qwen2.5-3B:\n",
            "처음으로 식당에 있는 사과의 수는 23개였습니다. 점심을 위해 20개를 사용했으므로 남은 사과는 23 - 20 = 3개가 됩니다.\n",
            "\n",
            "그 다음에 6개를 추가로 샀기 때문에, 현재 남아있는 사과의 수는 3 + 6 = 9개가 됩니다.\n",
            "\n",
            "따라서, 지금 남아있는 사과의 수는 9개입니다....\n",
            "\n",
            "Phi-3-mini:\n",
            " 식당에서 촉구한 사과의 수는 23개였습니다. 점심 만들기에 사용할 사과는 20개입니다. 남은 사과는 23 - 20 = 3개입니다. 그런데 6개를 더 샀으므로, 3 + 6 = 9개입니다. 답은 9개입니다....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 자기 일관성 (Self-Consistency)"
      ],
      "metadata": {
        "id": "HqYmxLHgbIku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 답변 추출 함수\n",
        "def extract_answer(text):\n",
        "    \"\"\"응답에서 숫자 추출\"\"\"\n",
        "    numbers = re.findall(r'\\d+', text)\n",
        "    # 문제의 숫자 제외\n",
        "    filtered = [int(n) for n in numbers if int(n) not in [23, 20, 6]]\n",
        "    return filtered[-1] if filtered else None\n",
        "\n",
        "question = \"\"\"\n",
        "식당에는 사과 23개가 있었습니다.\n",
        "20개를 점심 만드는 데 사용하고 6개를 더 샀다면,\n",
        "남은 사과는 몇 개일까요?\n",
        "\"\"\"\n",
        "\n",
        "self_consistency_prompt = [\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "results_consistency = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    answers = []\n",
        "    n_samples = 7\n",
        "\n",
        "    print(f\"\\n[{model_name}] {n_samples}번 샘플링 중...\")\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        output = pipe(\n",
        "            self_consistency_prompt,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            max_new_tokens=300\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        answer = extract_answer(output)\n",
        "        if answer:\n",
        "            answers.append(answer)\n",
        "            print(f\"  시도 {i+1}: {answer}개\")\n",
        "\n",
        "    # 다수결\n",
        "    if answers:\n",
        "        vote_counts = Counter(answers)\n",
        "        final_answer, count = vote_counts.most_common(1)[0]\n",
        "        confidence = count / n_samples\n",
        "\n",
        "        results_consistency[model_name] = {\n",
        "            \"answers\": answers,\n",
        "            \"final\": final_answer,\n",
        "            \"confidence\": confidence,\n",
        "            \"votes\": dict(vote_counts)\n",
        "        }\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# 비교 출력\n",
        "print(\"\\n자기 일관성 결과:\")\n",
        "for model_name, result in results_consistency.items():\n",
        "    print(f\"\\n[{model_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"수집된 답변: {result['answers']}\")\n",
        "    print(f\"최종 답변: {result['final']}개\")\n",
        "    print(f\"신뢰도: {result['confidence']:.1%}\")\n",
        "    print(f\"득표 분포: {result['votes']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810,
          "referenced_widgets": [
            "c32a1cbee91c494dbbedf6bf81e5d338",
            "d11e9781cbcc401ea49b0b34dbdc936f",
            "366c7bfd6bc842f0b0847273315c5f95",
            "1e7a2e965cb94784b4f9087e732a36cd",
            "9e99dc6d4c914f4a9fab92b51275e897",
            "a13a236e289c4111b5fad062455ae121",
            "f3c36200f95240c095ab85c853243a60",
            "de503b5c20e547a7bac12659d9c6c066",
            "4c863f7ab5c24641a164a77f5c93bbb3",
            "2b6f3a7d24c34259814010d2077cfcad",
            "6914f35b01354a6a988457d3ba2a8f9b",
            "ec71cdb6d8464bc1bc0c5e53d3113f03",
            "40f4dcb4b56d4865910c8060d9dd8c27",
            "ed512287f63c4e52b0ca277dfadb5170",
            "aa114300a2f64d7ea7434590dd643538",
            "9803b13f95f5470c803b93df334405b2",
            "4dd20ab2e8cf437687bbe67a6a2bf199",
            "3db310c49d4f4fffa8cbcf9afa73b844",
            "14206984b9f74e51b5cac8fe7f7ee1d3",
            "f3d67459ff404231af3074d110fa95be",
            "c47f2e9a031545ec9624c8c8873e9998",
            "c408f8bf11d14fb4aa28be5ae187b447"
          ]
        },
        "id": "1kQREUAxbMDp",
        "outputId": "08c38dcd-cd92-46b9-8b55-5653aae42d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ 모델 로딩 중: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c32a1cbee91c494dbbedf6bf81e5d338"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "\n",
            "[Qwen2.5-3B] 7번 샘플링 중...\n",
            "  시도 1: 9개\n",
            "  시도 2: 9개\n",
            "  시도 3: 9개\n",
            "  시도 4: 9개\n",
            "  시도 5: 9개\n",
            "  시도 6: 9개\n",
            "  시도 7: 3개\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "⏳ 모델 로딩 중: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec71cdb6d8464bc1bc0c5e53d3113f03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "\n",
            "[Phi-3-mini] 7번 샘플링 중...\n",
            "  시도 1: 9개\n",
            "  시도 2: 16개\n",
            "  시도 3: 26개\n",
            "  시도 4: 29개\n",
            "  시도 5: 26개\n",
            "  시도 6: 9개\n",
            "  시도 7: 3개\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "자기 일관성 결과:\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "수집된 답변: [9, 9, 9, 9, 9, 9, 3]\n",
            "최종 답변: 9개\n",
            "신뢰도: 85.7%\n",
            "득표 분포: {9: 6, 3: 1}\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            "수집된 답변: [9, 16, 26, 29, 26, 9, 3]\n",
            "최종 답변: 9개\n",
            "신뢰도: 28.6%\n",
            "득표 분포: {9: 2, 16: 1, 26: 2, 29: 1, 3: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ToT: 중간 단계 탐색"
      ],
      "metadata": {
        "id": "6ywCmJfkh9YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "세 명의 전문가가 이 질문에 답한다고 가정해 보세요.\n",
        "모든 전문가는 자신의 생각의 한 단계를 적어 그룹원들과 공유합니다.\n",
        "그런 다음 모든 전문가가 다음 단계로 넘어가는 식으로 진행합니다.\n",
        "어느 시점에서든 자신이 틀렸다는 것을 깨닫는 전문가가 있으면 그 자리에서 나갑니다.\n",
        "\n",
        "질문: \"식당에는 사과 23개가 있었습니다. 20개를 점심으로 만들고 6개를 더 샀다면,\n",
        "사과는 몇 개일까요?\"\n",
        "\n",
        "각 전문가의 추론 과정을 보여주고, 최종 답을 도출하세요.\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "results_tot = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    output = pipe(tot_prompt, max_new_tokens=800)[0][\"generated_text\"]\n",
        "    results_tot[model_name] = output\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# 비교 출력\n",
        "print(\"\\nToT 결과:\")\n",
        "for model_name, output in results_tot.items():\n",
        "    print(f\"\\n[{model_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    print(output[:400] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830,
          "referenced_widgets": [
            "4e97cd6aa0f44a50974d52d028c4046c",
            "952ba02866fd408d85471f1d147dbb8a",
            "2cd44d7a303f45ff96e038db98152fce",
            "1882151c4bf84311b51f9ee7c440f0e4",
            "fd63743ce83a428caaa07753a267a87c",
            "5bbb9fe3ea3447ff8814c8fce00bad24",
            "7cb26e8adb0b4612b20c86bbceb7b354",
            "b5cf03b18ee44ef099e4de8e7670c44a",
            "fb994784efeb4cbfb38e6a6ff4d98eb0",
            "d9a80d7c8cdc4d14b7fa5b96f08d505e",
            "808f84685e5e4f2daef6caae18d179b9",
            "2a7c98fe49f14170ae80126a755192b0",
            "a06fb77d893e4bdcba00348ad17fee32",
            "41da21dff34641318068c9727266fa56",
            "6c1bd9b82a684896821724ebbc208131",
            "d99ffa12bc574d3eb26febfccc872ab7",
            "18c05f799a6f45258a40bfcd09fee076",
            "fc6d1561bda9434d9ea5067c7b69d5e0",
            "78f17161bf724a2995a776175c316813",
            "8e7f41bd5ff04168818442854eca6d18",
            "8d3dc042fb5f400fb4bc7af4bc2ae9a4",
            "eebeb97cadd64a2da5dd01b392e08141"
          ]
        },
        "id": "f8T0JiCYUoBM",
        "outputId": "43b17634-7eb0-4fcd-aaa1-7701ed6b6c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ 모델 로딩 중: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e97cd6aa0f44a50974d52d028c4046c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "⏳ 모델 로딩 중: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a7c98fe49f14170ae80126a755192b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "ToT 결과:\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "네, 각 전문가가 문제를 해결하는 과정을 따라보도록 하겠습니다.\n",
            "\n",
            "### 전문가 A\n",
            "\n",
            "**단계 1:** 초기 상태에서 사과는 23개입니다.\n",
            "**단계 2:** 20개를 점심으로 사용했으므로 남은 수량은 23 - 20 = 3개입니다.\n",
            "**단계 3:** 그리고 6개를 추가적으로 샀으므로, 현재의 사과의 총 수는 3 + 6 = 9개입니다.\n",
            "\n",
            "**결론:** 사과는 9개입니다.\n",
            "\n",
            "---\n",
            "\n",
            "### 전문가 B\n",
            "\n",
            "**단계 1:** 초기 상태에서 사과는 23개입니다.\n",
            "**단계 2:** 20개를 점심으로 사용했으므로 남은 수량은 23 - 20 = 3개입니다.\n",
            "**단계 3:** 그리고 6개를 추가적으로 샀으므로, 현재의 사과의 총 수는 3 + 6 = 9개입니다.\n",
            "\n",
            "**결论（注意：原文中“結論”应为“结论”）:** 사과는 9개입니다....\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            " 전문가1: 먼저 20개를 점심으로 만들었으므로, 남은 사과는 23 - 20 = 3개가 남았습니다. 그런 다음 6개를 더 준 것이므로, 3 + 6 = 9개가 됩니다. 따라서 당시 식당에 있던 사과의 수는 9개였습니다.\n",
            "\n",
            "전문가2: 점심 먹기 위해 20개를 사용했으므로, 남은 사과는 23 - 20 = 3개가 남았습니다. 그런 다음 6개를 더 준 것이므로, 3 + 6 = 9개가 됩니다. 따라서 당시 식당에 있던 사과의 수는 9개였습니다.\n",
            "\n",
            "전문가3: 점심 먹기 전에 23개의 사과가 있었습니다. 20개를 사용했으므로, 남은 사과는 23 - 20 = 3개가 남았습니다. 그런 다음 6개를 더 준 것이므로, 3 + 6 = 9개가 됩니다. 따라서 당시 식당에 있던 사과의 수는 9개였습니다.\n",
            "\n",
            "따라서 모든 전문가는 답은 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 출력 검증: 예시 제공"
      ],
      "metadata": {
        "id": "M_m84Z3DiKKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "json_template = \"\"\"\n",
        "RPG 게임의 짧은 캐릭터 프로필을 만드세요.\n",
        "반드시 이 JSON 형식만 사용하세요:\n",
        "\n",
        "{\n",
        "  \"name\": \"캐릭터 이름\",\n",
        "  \"class\": \"직업\",\n",
        "  \"weapon\": \"무기\",\n",
        "  \"skill\": \"특수 능력\"\n",
        "}\n",
        "\n",
        "예시:\n",
        "{\n",
        "  \"name\": \"아리아\",\n",
        "  \"class\": \"마법사\",\n",
        "  \"weapon\": \"지팡이\",\n",
        "  \"skill\": \"화염구\"\n",
        "}\n",
        "\n",
        "이제 전사 캐릭터를 만들어주세요.\n",
        "\"\"\"\n",
        "\n",
        "validation_prompt = [\n",
        "    {\"role\": \"user\", \"content\": json_template}\n",
        "]\n",
        "\n",
        "def extract_and_parse_json(output):\n",
        "    \"\"\"개선된 JSON 추출 및 파싱\"\"\"\n",
        "\n",
        "    # 1. 마크다운 코드 블록에서 추출\n",
        "    patterns = [\n",
        "        r'```(?:json)?\\s*(\\{.*?\\})\\s*```',  # ```json { ... } ```\n",
        "        r'```\\s*(\\{.*?\\})\\s*```',           # ``` { ... } ```\n",
        "        r'\\{.*\\}',                          # 직접 JSON 객체\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, output, re.DOTALL)\n",
        "        if match:\n",
        "            json_candidate = match.group(1) if match.groups() else match.group()\n",
        "\n",
        "            try:\n",
        "                parsed = json.loads(json_candidate.strip())\n",
        "                return parsed, \"✅ JSON 파싱 성공\"\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "    return None, \"❌ 유효한 JSON을 찾을 수 없음\"\n",
        "\n",
        "# 검증 코드 수정\n",
        "results_validation = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    output = pipe(validation_prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "\n",
        "    # 개선된 JSON 파싱\n",
        "    parsed, validation_msg = extract_and_parse_json(output)\n",
        "    is_valid = parsed is not None\n",
        "\n",
        "    if is_valid:\n",
        "        # 필수 필드 검증\n",
        "        required = [\"name\", \"class\", \"weapon\", \"skill\"]\n",
        "        missing = [f for f in required if f not in parsed]\n",
        "        if missing:\n",
        "            validation_msg += f\" (누락 필드: {missing})\"\n",
        "\n",
        "    results_validation[model_name] = {\n",
        "        \"output\": output,\n",
        "        \"valid\": is_valid,\n",
        "        \"parsed\": parsed,\n",
        "        \"message\": validation_msg\n",
        "    }\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\n개선된 출력 검증 결과:\")\n",
        "for model_name, result in results_validation.items():\n",
        "    print(f\"\\n[{model_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"원본 출력:\\n{result['output']}\")\n",
        "    print(f\"\\n검증: {result['message']}\")\n",
        "    if result['parsed']:\n",
        "        print(f\"파싱 데이터:\\n{json.dumps(result['parsed'], indent=2, ensure_ascii=False)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a659e7f08ec14b538c4d4e36a8105b72",
            "73d7b6b100c74064b04b118bcc24ec9c",
            "f551cfa22fca45109b7f8f83850d7850",
            "6fe342cdb083400daf41789246dd0619",
            "7cfd02dba44b4b8793a880d730fc931f",
            "7b26a1ba95e04eeaade59c1336bed1f4",
            "acd00bc9350146378b11fee6ee8cfa60",
            "74ac541cecb2424e8a5537308037d6ef",
            "79b41b215dcf4015a369ce522788339e",
            "f8e6895efe0b44a9897cd20644f889a5",
            "0b734aacb88b40e096825bb9a3709fb1",
            "e9b67d899c344f6888550ce02c5cf045",
            "5bc51e547f0841a0949a65c57aabf213",
            "5978a1f9cbcb42989710a513a6be4c10",
            "aa67328e95a542658a33fdfcd4dd6865",
            "d5c3e15992c74ad6bf1101e67223312f",
            "ea70375dd1f64bdfaa48cccddb1741d8",
            "39c5d883e68f4d1da3c9999a105003bf",
            "7da94fbe61134809b520599bc1803274",
            "f99aa74291c14444a439f09ee2acd555",
            "5a4d7b9cf49b4aa298878d5a5ab99208",
            "4b9dfd933da1453298654b8b94766614"
          ]
        },
        "id": "H87k60khWc_L",
        "outputId": "af604989-3dbd-402f-8fb7-f7561ffa8fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ 모델 로딩 중: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a659e7f08ec14b538c4d4e36a8105b72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "⏳ 모델 로딩 중: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9b67d899c344f6888550ce02c5cf045"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 로딩 완료\n",
            "👌 메모리 정리 완료\n",
            "\n",
            "개선된 출력 검증 결과:\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "원본 출력:\n",
            "{\n",
            "  \"name\": \"브루스\",\n",
            "  \"class\": \"전사\",\n",
            "  \"weapon\": \"검\",\n",
            "  \"skill\": \"대형 칼날 던질기\"\n",
            "}\n",
            "\n",
            "검증: ✅ JSON 파싱 성공\n",
            "파싱 데이터:\n",
            "{\n",
            "  \"name\": \"브루스\",\n",
            "  \"class\": \"전사\",\n",
            "  \"weapon\": \"검\",\n",
            "  \"skill\": \"대형 칼날 던질기\"\n",
            "}\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            "원본 출력:\n",
            " ```json\n",
            "\n",
            "{\n",
            "\n",
            "  \"name\": \"전사 민수\",\n",
            "  \"class\": \"전사\",\n",
            "  \"weapon\": \"짱구\",\n",
            "  \"skill\": \"방어융합\"\n",
            "\n",
            "}\n",
            "\n",
            "```\n",
            "\n",
            "검증: ✅ JSON 파싱 성공\n",
            "파싱 데이터:\n",
            "{\n",
            "  \"name\": \"전사 민수\",\n",
            "  \"class\": \"전사\",\n",
            "  \"weapon\": \"짱구\",\n",
            "  \"skill\": \"방어융합\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ji3sGC5NhihD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"최종 종합 비교\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_table = {\n",
        "    \"모델\": list(MODELS.keys()),\n",
        "    \"크기\": [MODELS[m][\"size\"] for m in MODELS.keys()],\n",
        "    \"복잡한 프롬프트 속도\": [f\"{results_complex[m]['time']:.2f}초\" for m in MODELS.keys()],\n",
        "    \"자기일관성 신뢰도\": [f\"{results_consistency[m]['confidence']:.1%}\" for m in MODELS.keys()],\n",
        "    \"JSON 검증\": [results_validation[m]['message'] for m in MODELS.keys()],\n",
        "}\n",
        "\n",
        "# 표 형식 출력\n",
        "print(\"\\n\")\n",
        "for key, values in comparison_table.items():\n",
        "    print(f\"{key:20s}\", end=\"\")\n",
        "    for v in values:\n",
        "        print(f\"{str(v):25s}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"결론\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 속도 비교\n",
        "faster_model = min(MODELS.keys(), key=lambda m: results_complex[m]['time'])\n",
        "print(f\"속도: {faster_model} 우세\")\n",
        "\n",
        "# 신뢰도 비교\n",
        "reliable_model = max(MODELS.keys(), key=lambda m: results_consistency[m]['confidence'])\n",
        "print(f\"일관성: {reliable_model} 우세\")\n",
        "\n",
        "print(\"\\n각 모델의 특징:\")\n",
        "for model_name, info in MODELS.items():\n",
        "    print(f\"  • {model_name}: {info['description']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiAUNvDe4Jc9",
        "outputId": "bfc882f0-e58e-41fd-ed1b-f5ceb8f1e04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 종합 비교\n",
            "======================================================================\n",
            "\n",
            "\n",
            "모델                  Qwen2.5-3B               Phi-3-mini               \n",
            "크기                  3B                       3.8B                     \n",
            "복잡한 프롬프트 속도         7.00초                    16.52초                   \n",
            "자기일관성 신뢰도           85.7%                    28.6%                    \n",
            "JSON 검증             ✅ JSON 파싱 성공             ✅ JSON 파싱 성공             \n",
            "\n",
            "======================================================================\n",
            "결론\n",
            "======================================================================\n",
            "속도: Qwen2.5-3B 우세\n",
            "일관성: Qwen2.5-3B 우세\n",
            "\n",
            "각 모델의 특징:\n",
            "  • Qwen2.5-3B: 소형, 빠른 추론\n",
            "  • Phi-3-mini: 중소형, MS 모델, 교안 메인\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **모델 비교 분석 결과**\n",
        "\n",
        "## 성능 비교\n",
        "\n",
        "| 항목 | Qwen2.5-3B | Phi-3-mini |\n",
        "|------|------------|------------|\n",
        "| 크기 | 3B | 3.8B |\n",
        "| 속도 | 7.00초 | 16.52초 |\n",
        "| 일관성 | 85.7% | 28.6% |\n",
        "| JSON 검증 | 성공 | 성공 |\n",
        "\n",
        "---\n",
        "\n",
        "## 주요 발견\n",
        "\n",
        "### 속도\n",
        "- Qwen이 2.36배 빠름\n",
        "- 작은 모델이 더 빠른 역설\n",
        "\n",
        "### 일관성\n",
        "- Qwen: 7회 시도 중 6회 일치\n",
        "- Phi-3: 7회 중 2회만 일치\n",
        "- 신뢰도 차이 3배\n",
        "\n",
        "### 구조화 출력\n",
        "- 둘 다 JSON 생성 성공\n",
        "\n",
        "---\n",
        "\n",
        "## 원인 분석\n",
        "\n",
        "### Qwen 우세 이유\n",
        "- 다국어 토크나이저 (한국어 최적화)\n",
        "- 효율적 아키텍처\n",
        "- 안정적 학습 데이터\n",
        "\n",
        "### Phi-3 부진 이유\n",
        "- 영어 중심 설계\n",
        "- 한국어 토큰 처리 비효율\n",
        "- 출력 변동성 높음\n",
        "\n",
        "---\n",
        "\n",
        "## 테스트 한계\n",
        "\n",
        "### 한국어 중심 평가\n",
        "- Phi-3는 본래 영어에 강함\n",
        "- 언어 차이로 인한 불리\n",
        "- 영어 테스트 시 다른 결과 가능\n",
        "\n",
        "---\n",
        "\n",
        "## 결론\n",
        "\n",
        "### Qwen2.5-3B 적합 상황\n",
        "- 영어 외 다국어 서비스(한국어 포함)\n",
        "- 빠른 응답 속도 필요\n",
        "- 일관된 출력 필수\n",
        "- 비용 효율 중시\n",
        "\n",
        "### Phi-3-mini 적합 상황\n",
        "- 영어 서비스\n",
        "- MS 생태계\n",
        "- 교육/연구\n",
        "\n",
        "---\n",
        "\n",
        "## 요약\n",
        "\n",
        "한국어 태스크에서 Qwen2.5-3B가 속도와 일관성 모두 우세.\n",
        "단, 영어 서비스에서는 Phi-3도 경쟁력 있을 수 있음."
      ],
      "metadata": {
        "id": "jYJsIZ9YcpC-"
      }
    }
  ]
}