{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuRpYQ3c15Ke+oXxB5X9pD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wkdwlgus/ktcloud_genai/blob/main/251021_%EA%B3%A0%EA%B8%89_%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8_%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81_%EC%9E%A5%EC%A7%80%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**"
      ],
      "metadata": {
        "id": "yp-AQkTjxp64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°œìš”**\n",
        "\n",
        "- **í…ìŠ¤íŠ¸ í‘œí˜„**ì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸\n",
        "    - BERTë‚˜ BERT íŒŒìƒ ëª¨ë¸\n",
        "- **í…ìŠ¤íŠ¸ ìƒì„±**ì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸\n",
        "    - **GPT ê³„ì—´** ëª¨ë¸\n",
        "    - ì‚¬ìš©ìì˜ **í”„ë¡œí”„íŠ¸(prompt)ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±**\n",
        "- **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**(Prompt Engineering)\n",
        "    - <mark>**ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•˜ëŠ” ë°©ë²•**\n",
        "\n",
        "- **ëª©í‘œ**:\n",
        "    - <mark>í”„ë¡¬í”„íŠ¸ë¥¼ ì„¸ì‹¬í•˜ê²Œ ì„¤ê³„í•˜ì—¬ LLMì´ ì›í•˜ëŠ” ì‘ë‹µì„ í•˜ë„ë¡ ìœ ë„í•  ìˆ˜ ìˆë‹¤.\n",
        "- **ê¸°ì–µí•  ì **\n",
        "    - í”„ë¡¬í”„íŠ¸ ìµœì í™”ëŠ” ë°˜ë³µì ì¸ ê³¼ì •ì´ë©° ì‹¤í—˜ì´ í•„ìš”í•˜ë‹¤.\n",
        "    - ì™„ë²½í•œ í”„ë¡¬í”„íŠ¸ëŠ” ì„¤ê³„ëŠ” ì—†ìœ¼ë©° ì•ìœ¼ë¡œë„ ê°€ëŠ¥í•˜ì§€ ì•Šë‹¤.\n"
      ],
      "metadata": {
        "id": "B4ULIQMFRYF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **í”„ë¡¬í”„íŠ¸ì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ**\n",
        "\n",
        "1. **ì§€ì‹œì‚¬í•­/ëª…ë ¹ì–´** (Instruction)\n",
        "    - í”„ë¡¬í”„íŠ¸ì˜ í•µì‹¬ìœ¼ë¡œ, AIì—ê²Œ ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•í•˜ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤\n",
        "    - ëª¨í˜¸í•œ í‘œí˜„ë³´ë‹¤ëŠ” êµ¬ì²´ì ì´ê³  ì§ì ‘ì ì¸ ë™ì‚¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤\n",
        "    - \"í•´ì¤˜\", \"ë§Œë“¤ì–´ì¤˜\", \"ë¶„ì„í•´ì¤˜\", \"ìš”ì•½í•´ì¤˜\" ë“±ì˜ ëª…í™•í•œ ë™ì‚¬ ì‚¬ìš©\n",
        "2. **ì…ë ¥ ë°ì´í„°** (Input Data)\n",
        "    - ì‹¤ì œë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•  ì›ë³¸ ë°ì´í„°ì…ë‹ˆë‹¤\n",
        "    - êµ¬ì¡°í™”ëœ ë°ì´í„°(JSON, CSV)ë‚˜ ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„°(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€) ëª¨ë‘ ê°€ëŠ¥í•©ë‹ˆë‹¤\n",
        "3. **ì¶œë ¥ ì§€ì‹œì–´** (Output Format)\n",
        "    - ì‘ë‹µì˜ êµ¬ì¡°ë¥¼ ë¯¸ë¦¬ ì§€ì •í•˜ë©´ ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "    - íŠ¹íˆ í”„ë¡œê·¸ë˜ë°ì—ì„œ íŒŒì‹±ì´ í•„ìš”í•œ ê²½ìš° ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤"
      ],
      "metadata": {
        "id": "WL_3HbQRHyxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì§€ì‹œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸**\n",
        "\n",
        "- ì§€ì‹œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ë€?\n",
        "    - íŠ¹ì • ì§ˆë¬¸ì— ë‹µë³€í•˜ê±°ë‚˜ íŠ¹ì • ì‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸\n",
        "    - ì‘ì—…ë§ˆë‹¤ ë‹¤ë¥¸ ì§€ì‹œê°€ í•„ìš”í•˜ì§€ë§Œ ì¶œë ¥ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê¸°ìˆ ì—ëŠ” ê³µí†µì ì´ ìˆë‹¤.\n",
        "\n",
        "- **ì¶œë ¥ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ê¸°ìˆ **\n",
        "    - **êµ¬ì²´ì„±** : ì›í•˜ëŠ” ë°”ë¥¼ ì •í™•íˆ/êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”.\n",
        "    - **í™˜ê° ëŒ€ì‘ì±…** : LLMì—ê²Œ ë‹µë³€ì„ ì•Œ ë•Œë§Œ ìƒì„±í•˜ë¼ê³  ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹µì„ ì•Œì§€ ëª»í•˜ë©´ `ë‹µì„ ëª¨ë¥¸ë‹¤`ë¼ê³  ì¶œë ¥í• í•˜ë¼ê³  êµ¬ì²´ì ìœ¼ë¡œ ì§€ì‹œí•œë‹¤.\n",
        "    - **ìˆœì„œ** : í”„ë¡¬í”„íŠ¸ ì‹œì‘ì´ë‚˜ ëì— ì§€ì‹œ ì‚¬í•­ì„ ì „ë‹¬í•œë‹¤. íŠ¹íˆ, ê¸´ í”„ë¡¬í”„íŠ¸ì˜ ê²½ìš° ì¤‘ê°„ì— ìˆëŠ” ì •ë³´ê°€ ìŠí ìˆ˜ ìˆë‹¤."
      ],
      "metadata": {
        "id": "6Cs4_8STKFUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ìƒì„± ëª¨ë¸ ì‚¬ìš©í•˜ê¸°**"
      ],
      "metadata": {
        "id": "UldzT7hWLhUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "# 1. ì¼ë°˜ì ì¸ Python ê²½ê³ (DeprecationWarning ë“±) ìˆ¨ê¸°ê¸°\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# 2. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ìˆ˜ì¤€ ì¡°ì ˆ (Warning ì´í•˜ëŠ” ìˆ¨ê¸°ê¸°)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "UwHRcv9Zde3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**"
      ],
      "metadata": {
        "id": "EpV2uvnPlgkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# íŒŒì´ì¬ ë²„ì „ í™•ì¸(3.12)\n",
        "!python --version"
      ],
      "metadata": {
        "id": "6jBSq5FKdprY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07cfe37-13ad-4f7a-bb24-01e511ebcb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA ë²„ì „ í™•ì¸ (12.5)\n",
        "!nvcc --version | grep cuda_"
      ],
      "metadata": {
        "id": "5ZcvURivdpyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2737fb2d-f6c0-4c15-8d63-9732a15f6e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- llama-cpp-python\n",
        "    - https://github.com/abetlen/llama-cpp-python/releases\n",
        "    - llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl\n",
        "- transformers\n",
        "- ì‚¬ìš© ëª¨ë¸\n",
        "    - microsoft/Phi-3-mini-4k-instruct\n"
      ],
      "metadata": {
        "id": "uSpY0d9RlaGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRhw6oeA-yro"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# ì‚¬ìš©í•˜ëŠ” íŒŒì´ì¬ê³¼ CUDA ë²„ì „ì— ë§ëŠ” llama-cpp-python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\n",
        "# !pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl\n",
        "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUKDSu2MLEKe",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee32200-26bb-4620-8868-e614ec8ab360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.3)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.10.5)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.48.3\n"
          ]
        }
      ],
      "source": [
        "# Phi-3 ëª¨ë¸ê³¼ í˜¸í™˜ì„± ë•Œë¬¸ì— transformers 4.48.3 ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "!pip install transformers==4.48.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate tokenizers datasets safetensors"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-BUWHelAhvWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f41c418-27f1-46ec-9737-eade4d54f3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.21.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "import os\n",
        "import tqdm\n",
        "from transformers.utils import logging\n",
        "\n",
        "# tqdm ë¹„í™œì„±í™”\n",
        "tqdm.tqdm = lambda *args, **kwargs: iter([])\n",
        "tqdm.auto.tqdm = lambda *args, **kwargs: iter([])\n",
        "tqdm.notebook.tqdm = lambda *args, **kwargs: iter([])\n",
        "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "CsRRyH_f8jTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ì„ íƒí•˜ê¸°**"
      ],
      "metadata": {
        "id": "Nw5Msk5JXcAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **AI ëª¨ë¸ ë¶„ë¥˜**\n",
        "- **AI ëª¨ë¸ ë¶„ë¥˜ ë°œì „ì‚¬***\n",
        "|ì‹œê¸°|ì£¼ìš” ì‚¬ê±´|ì‚¬ìš©ëœ ìš©ì–´|ë°°ê²½|\n",
        "|---|---|---|---|\n",
        "| 2012-2017| ë”¥ëŸ¬ë‹ ì´ˆê¸°| \"Proprietary Model\"| ê¸°ì—…ë“¤ì´ ìì²´ ëª¨ë¸ ê°œë°œ|\n",
        "| 2018-2019| BERT, GPT-2 ë“±ì¥| \"Open vs Closed\"| OpenAIê°€ GPT-2 ë‹¨ê³„ì  ê³µê°œ|\n",
        "| 2020-2022| **GPT-3, Claude**| \"Closed-source Model\"| **API ì ‘ê·¼ë§Œ ì œê³µí•˜ëŠ” ëª¨ë¸** ì¦ê°€|\n",
        "| 2023-í˜„ì¬| **LLaMA, Mistral ë“±**| \"Open-weight Model\"| **ê°€ì¤‘ì¹˜ëŠ” ê³µê°œ, ë°ì´í„°ëŠ” ë¹„ê³µê°œ**|\n",
        "- ì°¸ê³ \n",
        "    - OpenAI GPT-2 Release Strategy (2019): https://openai.com/research/gpt-2-1-5b-release\n",
        "    - Meta LLaMA Release (2023): https://ai.meta.com/llama/\n",
        "\n"
      ],
      "metadata": {
        "id": "S8bLRDZOSl_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ëª¨ë¸ ë¶„ë¥˜ ìœ í˜•**\n",
        "|ìš©ì–´|í•œêµ­ì–´|ì˜ë¯¸|ê³µê°œ ë²”ìœ„|ì˜ˆì‹œ|\n",
        "|---|---|---|---|---|\n",
        "|Closed-source Model|íì‡„í˜• ëª¨ë¸|ëª¨ë¸ ê°€ì¤‘ì¹˜, ì•„í‚¤í…ì²˜, í•™ìŠµ ë°ì´í„° ëª¨ë‘ ë¹„ê³µê°œ|APIë§Œ ì œê³µ|GPT-4, Claude, Gemini|\n",
        "|Proprietary Model|ë…ì  ëª¨ë¸|ìƒì—…ì  ì†Œìœ ê¶Œì´ ìˆëŠ” ëª¨ë¸ (ì¼ë¶€ëŠ” ê°€ì¤‘ì¹˜ ê³µê°œ ê°€ëŠ¥)|ë‹¤ì–‘í•¨|GPT-4, Claude (ì™„ì „ ë¹„ê³µê°œ)|\n",
        "|Open-source Model|ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸|ì½”ë“œ, ê°€ì¤‘ì¹˜, í•™ìŠµ ë°©ë²• ëª¨ë‘ ê³µê°œ|ì™„ì „ ê³µê°œ|Pythia, OLMo, BLOOM|\n",
        "|Open-weight Model|ì˜¤í”ˆ ê°€ì¤‘ì¹˜ ëª¨ë¸|ê°€ì¤‘ì¹˜ëŠ” ê³µê°œ, í•™ìŠµ ë°ì´í„°/ë°©ë²•ì€ ë¹„ê³µê°œ|ê°€ì¤‘ì¹˜ë§Œ ê³µê°œ|LLaMA, Mistral, Qwen|\n",
        "|Open API Model|ì˜¤í”ˆ API ëª¨ë¸|APIë¡œë§Œ ì ‘ê·¼ ê°€ëŠ¥í•˜ì§€ë§Œ ë¬´ë£Œ ë˜ëŠ” ì €ë ´|APIë§Œ|GPT-3.5 (ë¬´ë£Œ ë²„ì „)"
      ],
      "metadata": {
        "id": "mDP4HQswWhZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ê° **ëª¨ë¸ ìœ í˜•ì˜ íŠ¹ì§•**\n",
        "|íŠ¹ì§•|Open-source|Open-weight|Closed-source|\n",
        "|---|---|---|---|\n",
        "| ëª¨ë¸ ê°€ì¤‘ì¹˜| âœ… ê³µê°œ| âœ… ê³µê°œ| âŒ ë¹„ê³µê°œ|\n",
        "| í•™ìŠµ ì½”ë“œ| âœ… ê³µê°œ| âš ï¸ ë¶€ë¶„ ê³µê°œ| âŒ ë¹„ê³µê°œ|\n",
        "| í•™ìŠµ ë°ì´í„°| âœ… ê³µê°œ| âŒ ë¹„ê³µê°œ| âŒ ë¹„ê³µê°œ|\n",
        "| ì•„í‚¤í…ì²˜| âœ… ê³µê°œ| âœ… ê³µê°œ| âš ï¸ ë¶€ë¶„ ê³µê°œ|\n",
        "| ë¡œì»¬ ì‹¤í–‰| âœ… ê°€ëŠ¥| âœ… ê°€ëŠ¥| âŒ ë¶ˆê°€ëŠ¥|\n",
        "| ìƒì—…ì  ì‚¬ìš©| âœ… ê°€ëŠ¥ (ë¼ì´ì„ ìŠ¤ì— ë”°ë¼)| âš ï¸ ì œí•œì | âŒ API ìš”ê¸ˆ|\n",
        "| ëª¨ë¸ ìˆ˜ì •| âœ… ê°€ëŠ¥| âœ… ê°€ëŠ¥| âŒ ë¶ˆê°€ëŠ¥|\n",
        "| ì¬í˜„ ê°€ëŠ¥ì„±| âœ… ë†’ìŒ| âš ï¸ ì¤‘ê°„| âŒ ë¶ˆê°€ëŠ¥|\n"
      ],
      "metadata": {
        "id": "Xuv88yOtVQj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **(ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸)íŒŒìš´ë°ì´ì…˜ ëª¨ë¸**(Foundation model)\n",
        "    - ì´ëŸ° LLMì€ ëŒ€ê·œëª¨ ë±ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì‚¬ì „ í›ˆë ¨ë˜ë©° íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•´ ë¯¸ì„¸ íŠœë‹ëœë‹¤.\n",
        "    - ë¯¸ì„¸ íŠœë‹ëœ ëª¨ë¸ì´ ìˆ˜ë°±ê°œ ì´ìƒ ë¨\n",
        "    - íŠ¹ì • ì‘ì—…ì— ì˜ ë§\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ibK55HC56skczaF9saU9FKlIR6BiZdqT\" width=\"80%\">"
      ],
      "metadata": {
        "id": "h3j7Yr1gTMk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ì˜ˆì œ : ë¼ì´ì„ ìŠ¤ë³„ ì‚¬ìš© ê°€ëŠ¥ ë²”ìœ„ ë¹„êµ**"
      ],
      "metadata": {
        "id": "n12c7GTOft9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# ì˜ˆì œ: ë¼ì´ì„ ìŠ¤ ë¹„êµ ë° ì í•©ì„± íŒë‹¨\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âš–ï¸ AI ëª¨ë¸ ë¼ì´ì„ ìŠ¤ ë¹„êµ\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class LicenseChecker:\n",
        "    \"\"\"ë¼ì´ì„ ìŠ¤ ì í•©ì„± ê²€ì‚¬ê¸°\"\"\"\n",
        "\n",
        "    licenses = {\n",
        "        \"Apache 2.0\": {\n",
        "            \"commercial\": True,\n",
        "            \"modify\": True,\n",
        "            \"redistribute\": True,\n",
        "            \"restrictions\": [],\n",
        "            \"models\": [\"Mistral 7B\", \"Qwen 2.5\", \"SOLAR\"]\n",
        "        },\n",
        "        \"LLaMA 2 License\": {\n",
        "            \"commercial\": True,\n",
        "            \"modify\": True,\n",
        "            \"redistribute\": True,\n",
        "            \"restrictions\": [\"MAU > 700M requires special license\"],\n",
        "            \"models\": [\"LLaMA 2\", \"LLaMA 3\"]\n",
        "        },\n",
        "        \"Gemma License\": {\n",
        "            \"commercial\": True,\n",
        "            \"modify\": True,\n",
        "            \"redistribute\": True,\n",
        "            \"restrictions\": [\"Cannot compete with Google services\"],\n",
        "            \"models\": [\"Gemma\", \"Gemma 2\"]\n",
        "        },\n",
        "        \"GPT-4 API\": {\n",
        "            \"commercial\": True,\n",
        "            \"modify\": False,\n",
        "            \"redistribute\": False,\n",
        "            \"restrictions\": [\"API usage only\", \"Rate limits\", \"Cost per token\"],\n",
        "            \"models\": [\"GPT-4\", \"GPT-4 Turbo\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def check_use_case(license_name, use_case):\n",
        "        \"\"\"ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•œ ë¼ì´ì„ ìŠ¤ ì í•©ì„± ê²€ì‚¬\"\"\"\n",
        "\n",
        "        license_info = LicenseChecker.licenses.get(license_name)\n",
        "        if not license_info:\n",
        "            return \"Unknown license\"\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # ìƒì—…ì  ì‚¬ìš©\n",
        "        if \"commercial\" in use_case.lower():\n",
        "            if license_info[\"commercial\"]:\n",
        "                results.append(\"âœ… ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥\")\n",
        "            else:\n",
        "                results.append(\"âŒ ìƒì—…ì  ì‚¬ìš© ë¶ˆê°€\")\n",
        "\n",
        "        # ëª¨ë¸ ìˆ˜ì •\n",
        "        if \"modify\" in use_case.lower() or \"fine-tun\" in use_case.lower():\n",
        "            if license_info[\"modify\"]:\n",
        "                results.append(\"âœ… ëª¨ë¸ ìˆ˜ì •/íŒŒì¸íŠœë‹ ê°€ëŠ¥\")\n",
        "            else:\n",
        "                results.append(\"âŒ ëª¨ë¸ ìˆ˜ì • ë¶ˆê°€\")\n",
        "\n",
        "        # ì¬ë°°í¬\n",
        "        if \"redistribute\" in use_case.lower():\n",
        "            if license_info[\"redistribute\"]:\n",
        "                results.append(\"âœ… ëª¨ë¸ ì¬ë°°í¬ ê°€ëŠ¥\")\n",
        "            else:\n",
        "                results.append(\"âŒ ëª¨ë¸ ì¬ë°°í¬ ë¶ˆê°€\")\n",
        "\n",
        "        # ì œì•½ì‚¬í•­\n",
        "        if license_info[\"restrictions\"]:\n",
        "            results.append(f\"âš ï¸  ì œì•½ì‚¬í•­:\")\n",
        "            for restriction in license_info[\"restrictions\"]:\n",
        "                results.append(f\"   - {restriction}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "# ì‚¬ìš© ì‚¬ë¡€ í…ŒìŠ¤íŠ¸\n",
        "use_cases = [\n",
        "    (\"Apache 2.0\", \"ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ ìƒì—…ì  ì„œë¹„ìŠ¤ ê°œë°œ ë° ëª¨ë¸ íŒŒì¸íŠœë‹\"),\n",
        "    (\"LLaMA 2 License\", \"ëŒ€ê¸°ì—…(MAU 10ì–µ)ì—ì„œ ìƒì—…ì  ì„œë¹„ìŠ¤ ê°œë°œ\"),\n",
        "    (\"GPT-4 API\", \"ìƒì—…ì  ì±—ë´‡ ì„œë¹„ìŠ¤ì— API í†µí•©\"),\n",
        "    (\"Gemma License\", \"êµ¬ê¸€ ê²€ìƒ‰ ê²½ìŸ ì„œë¹„ìŠ¤ ê°œë°œ\"),\n",
        "]\n",
        "\n",
        "print(\"\\në‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•œ ë¼ì´ì„ ìŠ¤ ì í•©ì„±:\\n\")\n",
        "\n",
        "for license_name, use_case in use_cases:\n",
        "    print(f\"ã€ {license_name} ã€‘\")\n",
        "    print(f\"ì‚¬ìš© ì‚¬ë¡€: {use_case}\")\n",
        "    print(f\"ëŒ€í‘œ ëª¨ë¸: {', '.join(LicenseChecker.licenses[license_name]['models'])}\")\n",
        "    print()\n",
        "\n",
        "    results = LicenseChecker.check_use_case(license_name, use_case)\n",
        "\n",
        "    if isinstance(results, list):\n",
        "        for result in results:\n",
        "            print(f\"  {result}\")\n",
        "    else:\n",
        "        print(f\"  {results}\")\n",
        "\n",
        "    print(\"-\"*70)\n",
        "\n",
        "print(\"\\nğŸ’¡ ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ì— ì í•©í•œ ë¼ì´ì„ ìŠ¤ë¥¼ ì°¾ì•„ë³´ì„¸ìš”!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6F574Pe7fwB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d645a58f-e8ed-4b66-92ab-06bbb083e9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "âš–ï¸ AI ëª¨ë¸ ë¼ì´ì„ ìŠ¤ ë¹„êµ\n",
            "======================================================================\n",
            "\n",
            "ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•œ ë¼ì´ì„ ìŠ¤ ì í•©ì„±:\n",
            "\n",
            "ã€ Apache 2.0 ã€‘\n",
            "ì‚¬ìš© ì‚¬ë¡€: ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ ìƒì—…ì  ì„œë¹„ìŠ¤ ê°œë°œ ë° ëª¨ë¸ íŒŒì¸íŠœë‹\n",
            "ëŒ€í‘œ ëª¨ë¸: Mistral 7B, Qwen 2.5, SOLAR\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "ã€ LLaMA 2 License ã€‘\n",
            "ì‚¬ìš© ì‚¬ë¡€: ëŒ€ê¸°ì—…(MAU 10ì–µ)ì—ì„œ ìƒì—…ì  ì„œë¹„ìŠ¤ ê°œë°œ\n",
            "ëŒ€í‘œ ëª¨ë¸: LLaMA 2, LLaMA 3\n",
            "\n",
            "  âš ï¸  ì œì•½ì‚¬í•­:\n",
            "     - MAU > 700M requires special license\n",
            "----------------------------------------------------------------------\n",
            "ã€ GPT-4 API ã€‘\n",
            "ì‚¬ìš© ì‚¬ë¡€: ìƒì—…ì  ì±—ë´‡ ì„œë¹„ìŠ¤ì— API í†µí•©\n",
            "ëŒ€í‘œ ëª¨ë¸: GPT-4, GPT-4 Turbo\n",
            "\n",
            "  âš ï¸  ì œì•½ì‚¬í•­:\n",
            "     - API usage only\n",
            "     - Rate limits\n",
            "     - Cost per token\n",
            "----------------------------------------------------------------------\n",
            "ã€ Gemma License ã€‘\n",
            "ì‚¬ìš© ì‚¬ë¡€: êµ¬ê¸€ ê²€ìƒ‰ ê²½ìŸ ì„œë¹„ìŠ¤ ê°œë°œ\n",
            "ëŒ€í‘œ ëª¨ë¸: Gemma, Gemma 2\n",
            "\n",
            "  âš ï¸  ì œì•½ì‚¬í•­:\n",
            "     - Cannot compete with Google services\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸ’¡ ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ì— ì í•©í•œ ë¼ì´ì„ ìŠ¤ë¥¼ ì°¾ì•„ë³´ì„¸ìš”!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ì˜ˆì œ: ëª¨ë¸ ë¹„ìš© ê³„ì‚°ê¸°**\n",
        "- íŒ€ í”„ë¡œì íŠ¸ì‹œ ë°˜ë“œì‹œ í•´ì•¼í•  ê³¼ì œì„!"
      ],
      "metadata": {
        "id": "5zvIhHaoffla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# ì˜ˆì œ : Open vs Closed ëª¨ë¸ ë¹„ìš© ë¹„êµ\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ’° Open-weight vs Closed-source ë¹„ìš© ë¹„êµ\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class ModelCostCalculator:\n",
        "    \"\"\"ëª¨ë¸ ì‚¬ìš© ë¹„ìš© ê³„ì‚°ê¸°\"\"\"\n",
        "\n",
        "    # API ëª¨ë¸ ê°€ê²© (2024-2025 ê¸°ì¤€, USD)\n",
        "    api_pricing = {\n",
        "        \"GPT-4\": {\n",
        "            \"input\": 0.03,  # per 1K tokens\n",
        "            \"output\": 0.06,\n",
        "            \"type\": \"closed-source\"\n",
        "        },\n",
        "        \"GPT-3.5 Turbo\": {\n",
        "            \"input\": 0.0005,\n",
        "            \"output\": 0.0015,\n",
        "            \"type\": \"closed-source\"\n",
        "        },\n",
        "        \"Claude Opus\": {\n",
        "            \"input\": 0.015,\n",
        "            \"output\": 0.075,\n",
        "            \"type\": \"closed-source\"\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # ì˜¤í”ˆ ëª¨ë¸ ì¸í”„ë¼ ë¹„ìš© (ì›” ê¸°ì¤€)\n",
        "    open_model_costs = {\n",
        "        \"LLaMA 2 7B\": {\n",
        "            \"gpu\": \"T4\",\n",
        "            \"gpu_cost_per_hour\": 0.35,  # Google Cloud\n",
        "            \"monthly_cost\": 0.35 * 24 * 30,\n",
        "            \"type\": \"open-weight\"\n",
        "        },\n",
        "        \"Mistral 7B\": {\n",
        "            \"gpu\": \"T4\",\n",
        "            \"gpu_cost_per_hour\": 0.35,\n",
        "            \"monthly_cost\": 0.35 * 24 * 30,\n",
        "            \"type\": \"open-weight\"\n",
        "        },\n",
        "        \"LLaMA 2 70B\": {\n",
        "            \"gpu\": \"A100\",\n",
        "            \"gpu_cost_per_hour\": 2.93,\n",
        "            \"monthly_cost\": 2.93 * 24 * 30,\n",
        "            \"type\": \"open-weight\"\n",
        "        },\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_api_cost(model_name, input_tokens, output_tokens):\n",
        "        \"\"\"API ëª¨ë¸ ë¹„ìš© ê³„ì‚°\"\"\"\n",
        "\n",
        "        pricing = ModelCostCalculator.api_pricing.get(model_name)\n",
        "        if not pricing:\n",
        "            return None\n",
        "\n",
        "        input_cost = (input_tokens / 1000) * pricing[\"input\"]\n",
        "        output_cost = (output_tokens / 1000) * pricing[\"output\"]\n",
        "        total_cost = input_cost + output_cost\n",
        "\n",
        "        return {\n",
        "            \"input_cost\": input_cost,\n",
        "            \"output_cost\": output_cost,\n",
        "            \"total_cost\": total_cost\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def compare_costs(monthly_requests, avg_input_tokens, avg_output_tokens):\n",
        "        \"\"\"ì›”ê°„ ë¹„ìš© ë¹„êµ\"\"\"\n",
        "\n",
        "        print(f\"\\nì‚¬ìš©ëŸ‰ ê°€ì •:\")\n",
        "        print(f\"  ì›”ê°„ ìš”ì²­ ìˆ˜: {monthly_requests:,}íšŒ\")\n",
        "        print(f\"  í‰ê·  ì…ë ¥ í† í°: {avg_input_tokens:,} tokens\")\n",
        "        print(f\"  í‰ê·  ì¶œë ¥ í† í°: {avg_output_tokens:,} tokens\")\n",
        "        print(f\"  ì´ í† í°/ì›”: {(monthly_requests * (avg_input_tokens + avg_output_tokens)):,} tokens\")\n",
        "        print()\n",
        "\n",
        "        # API ëª¨ë¸ ë¹„ìš©\n",
        "        print(\"ã€ Closed-source (API) ëª¨ë¸ ë¹„ìš© ã€‘\")\n",
        "        for model_name, pricing in ModelCostCalculator.api_pricing.items():\n",
        "            cost_per_request = ModelCostCalculator.calculate_api_cost(\n",
        "                model_name, avg_input_tokens, avg_output_tokens\n",
        "            )\n",
        "\n",
        "            if cost_per_request:\n",
        "                monthly_cost = cost_per_request[\"total_cost\"] * monthly_requests\n",
        "                print(f\"  {model_name:20s}: ${monthly_cost:,.2f} / ì›”\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # ì˜¤í”ˆ ëª¨ë¸ ë¹„ìš©\n",
        "        print(\"ã€ Open-weight ëª¨ë¸ ë¹„ìš© (ì¸í”„ë¼) ã€‘\")\n",
        "        for model_name, cost_info in ModelCostCalculator.open_model_costs.items():\n",
        "            monthly_cost = cost_info[\"monthly_cost\"]\n",
        "            gpu_type = cost_info[\"gpu\"]\n",
        "            print(f\"  {model_name:20s}: ${monthly_cost:,.2f} / ì›” ({gpu_type} 24/7 ê°€ë™)\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # ì†ìµë¶„ê¸°ì  ê³„ì‚°\n",
        "        print(\"ã€ ì†ìµë¶„ê¸°ì  ë¶„ì„ ã€‘\")\n",
        "\n",
        "        # GPT-4 vs LLaMA 2 7B\n",
        "        gpt4_per_request = ModelCostCalculator.calculate_api_cost(\n",
        "            \"GPT-4\", avg_input_tokens, avg_output_tokens\n",
        "        )[\"total_cost\"]\n",
        "\n",
        "        llama_monthly = ModelCostCalculator.open_model_costs[\"LLaMA 2 7B\"][\"monthly_cost\"]\n",
        "\n",
        "        breakeven_requests = llama_monthly / gpt4_per_request\n",
        "\n",
        "        print(f\"  GPT-4 vs LLaMA 2 7B:\")\n",
        "        print(f\"    ì›” {breakeven_requests:,.0f}íšŒ ì´ìƒ ì‚¬ìš© ì‹œ LLaMA 2ê°€ ê²½ì œì \")\n",
        "        print(f\"    í˜„ì¬ ì‚¬ìš©ëŸ‰ ({monthly_requests:,}íšŒ): \", end=\"\")\n",
        "\n",
        "        if monthly_requests > breakeven_requests:\n",
        "            print(\"âœ… Open-weight ëª¨ë¸ì´ ìœ ë¦¬\")\n",
        "        else:\n",
        "            print(\"ğŸ’° API ëª¨ë¸ì´ ìœ ë¦¬\")\n",
        "\n",
        "# ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\n",
        "scenarios = [\n",
        "    {\n",
        "        \"name\": \"ì†Œê·œëª¨ ìŠ¤íƒ€íŠ¸ì—…\",\n",
        "        \"monthly_requests\": 10000,\n",
        "        \"avg_input\": 500,\n",
        "        \"avg_output\": 300\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ì¤‘ê·œëª¨ ì„œë¹„ìŠ¤\",\n",
        "        \"monthly_requests\": 1000000,\n",
        "        \"avg_input\": 500,\n",
        "        \"avg_output\": 300\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤\",\n",
        "        \"monthly_requests\": 10000000,\n",
        "        \"avg_input\": 500,\n",
        "        \"avg_output\": 300\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, scenario in enumerate(scenarios, 1):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"ì‹œë‚˜ë¦¬ì˜¤ {i}: {scenario['name']}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    ModelCostCalculator.compare_costs(\n",
        "        scenario[\"monthly_requests\"],\n",
        "        scenario[\"avg_input\"],\n",
        "        scenario[\"avg_output\"]\n",
        "    )\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ’¡ ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ ê·œëª¨ì— ë§ëŠ” ëª¨ë¸ì„ ì„ íƒí•´ë³´ì„¸ìš”!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wwqz9t0VfREF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fbf2a8-6225-4caa-9476-fbafeb0aec82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ’° Open-weight vs Closed-source ë¹„ìš© ë¹„êµ\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ì‹œë‚˜ë¦¬ì˜¤ 1: ì†Œê·œëª¨ ìŠ¤íƒ€íŠ¸ì—…\n",
            "======================================================================\n",
            "\n",
            "ì‚¬ìš©ëŸ‰ ê°€ì •:\n",
            "  ì›”ê°„ ìš”ì²­ ìˆ˜: 10,000íšŒ\n",
            "  í‰ê·  ì…ë ¥ í† í°: 500 tokens\n",
            "  í‰ê·  ì¶œë ¥ í† í°: 300 tokens\n",
            "  ì´ í† í°/ì›”: 8,000,000 tokens\n",
            "\n",
            "ã€ Closed-source (API) ëª¨ë¸ ë¹„ìš© ã€‘\n",
            "  GPT-4               : $330.00 / ì›”\n",
            "  GPT-3.5 Turbo       : $7.00 / ì›”\n",
            "  Claude Opus         : $300.00 / ì›”\n",
            "\n",
            "ã€ Open-weight ëª¨ë¸ ë¹„ìš© (ì¸í”„ë¼) ã€‘\n",
            "  LLaMA 2 7B          : $252.00 / ì›” (T4 24/7 ê°€ë™)\n",
            "  Mistral 7B          : $252.00 / ì›” (T4 24/7 ê°€ë™)\n",
            "  LLaMA 2 70B         : $2,109.60 / ì›” (A100 24/7 ê°€ë™)\n",
            "\n",
            "ã€ ì†ìµë¶„ê¸°ì  ë¶„ì„ ã€‘\n",
            "  GPT-4 vs LLaMA 2 7B:\n",
            "    ì›” 7,636íšŒ ì´ìƒ ì‚¬ìš© ì‹œ LLaMA 2ê°€ ê²½ì œì \n",
            "    í˜„ì¬ ì‚¬ìš©ëŸ‰ (10,000íšŒ): âœ… Open-weight ëª¨ë¸ì´ ìœ ë¦¬\n",
            "\n",
            "======================================================================\n",
            "ì‹œë‚˜ë¦¬ì˜¤ 2: ì¤‘ê·œëª¨ ì„œë¹„ìŠ¤\n",
            "======================================================================\n",
            "\n",
            "ì‚¬ìš©ëŸ‰ ê°€ì •:\n",
            "  ì›”ê°„ ìš”ì²­ ìˆ˜: 1,000,000íšŒ\n",
            "  í‰ê·  ì…ë ¥ í† í°: 500 tokens\n",
            "  í‰ê·  ì¶œë ¥ í† í°: 300 tokens\n",
            "  ì´ í† í°/ì›”: 800,000,000 tokens\n",
            "\n",
            "ã€ Closed-source (API) ëª¨ë¸ ë¹„ìš© ã€‘\n",
            "  GPT-4               : $33,000.00 / ì›”\n",
            "  GPT-3.5 Turbo       : $700.00 / ì›”\n",
            "  Claude Opus         : $30,000.00 / ì›”\n",
            "\n",
            "ã€ Open-weight ëª¨ë¸ ë¹„ìš© (ì¸í”„ë¼) ã€‘\n",
            "  LLaMA 2 7B          : $252.00 / ì›” (T4 24/7 ê°€ë™)\n",
            "  Mistral 7B          : $252.00 / ì›” (T4 24/7 ê°€ë™)\n",
            "  LLaMA 2 70B         : $2,109.60 / ì›” (A100 24/7 ê°€ë™)\n",
            "\n",
            "ã€ ì†ìµë¶„ê¸°ì  ë¶„ì„ ã€‘\n",
            "  GPT-4 vs LLaMA 2 7B:\n",
            "    ì›” 7,636íšŒ ì´ìƒ ì‚¬ìš© ì‹œ LLaMA 2ê°€ ê²½ì œì \n",
            "    í˜„ì¬ ì‚¬ìš©ëŸ‰ (1,000,000íšŒ): âœ… Open-weight ëª¨ë¸ì´ ìœ ë¦¬\n",
            "\n",
            "======================================================================\n",
            "ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤\n",
            "======================================================================\n",
            "\n",
            "ì‚¬ìš©ëŸ‰ ê°€ì •:\n",
            "  ì›”ê°„ ìš”ì²­ ìˆ˜: 10,000,000íšŒ\n",
            "  í‰ê·  ì…ë ¥ í† í°: 500 tokens\n",
            "  í‰ê·  ì¶œë ¥ í† í°: 300 tokens\n",
            "  ì´ í† í°/ì›”: 8,000,000,000 tokens\n",
            "\n",
            "ã€ Closed-source (API) ëª¨ë¸ ë¹„ìš© ã€‘\n",
            "  GPT-4               : $330,000.00 / ì›”\n",
            "  GPT-3.5 Turbo       : $7,000.00 / ì›”\n",
            "  Claude Opus         : $300,000.00 / ì›”\n",
            "\n",
            "ã€ Open-weight ëª¨ë¸ ë¹„ìš© (ì¸í”„ë¼) ã€‘\n",
            "  LLaMA 2 7B          : $252.00 / ì›” (T4 24/7 ê°€ë™)\n",
            "  Mistral 7B          : $252.00 / ì›” (T4 24/7 ê°€ë™)\n",
            "  LLaMA 2 70B         : $2,109.60 / ì›” (A100 24/7 ê°€ë™)\n",
            "\n",
            "ã€ ì†ìµë¶„ê¸°ì  ë¶„ì„ ã€‘\n",
            "  GPT-4 vs LLaMA 2 7B:\n",
            "    ì›” 7,636íšŒ ì´ìƒ ì‚¬ìš© ì‹œ LLaMA 2ê°€ ê²½ì œì \n",
            "    í˜„ì¬ ì‚¬ìš©ëŸ‰ (10,000,000íšŒ): âœ… Open-weight ëª¨ë¸ì´ ìœ ë¦¬\n",
            "\n",
            "======================================================================\n",
            "ğŸ’¡ ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ ê·œëª¨ì— ë§ëŠ” ëª¨ë¸ì„ ì„ íƒí•´ë³´ì„¸ìš”!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaB0B1LdMcPM"
      },
      "source": [
        "### **í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ë¡œë“œí•˜ê¸°**\n",
        "\n",
        "- ì‚¬ìš© ëª¨ë¸ :\n",
        "    - microsoft/Phi-3-mini-4k-instruct\n",
        "        - 8GN VRAM  ì¥ì¹˜ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥\n",
        "    - \"skt/KoGPT2-base-v2\"\n",
        "        - SKTê°€ ê°œë°œí•œ, í…ìŠ¤íŠ¸ ìƒì„±(Generative)ì´ ê°€ëŠ¥í•œ í•œêµ­ì–´ GPT-2 ëª¨ë¸ì˜ ê¸°ë³¸(base) ë²„ì „\n",
        "        - ì•½ 1ì–µ 2500ë§Œ ê°œ(125M)ì˜ íŒŒë¼ë¯¸í„°\n",
        "\n",
        "ì½”ë“œ ì§œëŠ”ê±° ì™¸ìš°ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNkbw28oMeAM",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a11ac9bb1626480a9bcd258bcf4c8e3e",
            "df9ba77f3b3e41f8a23f5ee8e7a7bdb5",
            "564c4b5d1b214fc48f0b24b455fa70b8",
            "2999811fdbfa4daa924f52faf7c2e9a2",
            "56b3a8676df641fe9ba7dc17bc330f8e",
            "e9f31c7c329148a0b5b0f0e9b2448ee5",
            "b0d71dff8db44063a4df053c2d09ed79",
            "9972a75896b74919b4c470d21fbd5ffb",
            "48e41165a1584d808f0ae08b8dd060d6",
            "6b2194fd84084768928ee3f5378b0e66",
            "f086d3e3c1864b06b363231e82f66ca7"
          ]
        },
        "outputId": "3d952b9e-27b7-4106-ec3d-8b032d88dd30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a11ac9bb1626480a9bcd258bcf4c8e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# AutoModelForCausalLM: ìƒì„±ëª¨ë¸\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì ˆë¥´ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "# model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"  # ì–´í…ì…˜ êµ¬í˜„ ë°©ì‹ì„ 'eager' (ê¸°ë³¸)ë¡œ ëª…ì‹œ\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False, # í”„ë¡¬í”„íŠ¸ë¥¼ ì œì™¸í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ ë°˜í™˜\n",
        "    max_new_tokens=500,     # ë‹¨, KoGPT2ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ 4k í† í°ì„ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ 500ì€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. (150 ì •ë„ë¡œ ì¡°ì ˆ)\n",
        "    do_sample=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **í˜¸ë‘ì´ì™€ ê´€ë ¨ëœ ë†ë‹´ì„ ë§Œë“¤ì–´ ë‹¬ë¼ê³  ìš”ì²­í•˜ê¸°**"
      ],
      "metadata": {
        "id": "tCRbsr2-2qkG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZIMVL0Q3g8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28818b4e-60d9-4c71-b5fc-d7fbc61e9cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " í˜¸ë‘ì´ì™€ ê´€ë ¨ëœ ì¬ë¯¸ë‚œ ì´ì•¼ê¸°:\n",
            "\n",
            "\n",
            "ì €ëŠ” í˜¸ë‘ì´ì™€ ê´€ë ¨ëœ ì´ì•¼ê¸°ë¥¼ ìƒê°í•´ ë³´ì•˜ìŠµë‹ˆë‹¤. í˜¸ë‘ì´ëŠ” ì˜ˆìˆ ì ìœ¼ë¡œ ë§¤ìš° ìœ ëª…í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ê·¸ë“¤ì˜ ë©‹ì§„ í˜•íƒœì™€ ëŠë‚Œì…ë‹ˆë‹¤. í˜¸ë‘ì´ëŠ” ëŒ€í˜• ëª¸ë¬´ê²Œ, ë°œê¸¸ì´ê°€ ë„“ì€ ë°œ, ë°˜ì§€ ë“± íŠ¹ì´í•œ í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë“¤ì€ ë§¤ìš° ê°•ë ¥í•˜ê³  ì ì‹¤ì´ ë˜ì–´ ìˆì–´ ì˜ˆìˆ ì ìœ¼ë¡œ ë†’ì€ ê°ì •ì„ ì£¼ë©° ì‘í’ˆì„ ì‘ì„±í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "í•œí¸, í˜¸ë‘ì´ëŠ” ì˜ˆìˆ ì ìœ¼ë¡œë„ ì˜ ì‘ìš©í•©ë‹ˆë‹¤. ê·¸ë“¤ì˜ ëª¸ê³¼ ëª¸ì§‘ì„ í†µí•´ ë‹¤ì–‘í•œ ì˜ˆìˆ  ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê·¸ë“¤ì€ ë¬¼ê±´ì„ ì‹¸ê³  ìˆê±°ë‚˜ ë¬¼ê±´ì„ ì“°ëŠ” ê²ƒìœ¼ë¡œ ì‘í’ˆì„ ë§Œë“¤ê³  ìˆìŠµë‹ˆë‹¤. ì´ï¿½ï¿½\n"
          ]
        }
      ],
      "source": [
        "# í”„ë¡¬í”„íŠ¸\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"í˜¸ë‘ì´ì™€ ê´€ë ¨ëœ ì¬ë¯¸ë‚œ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì¤˜.\"}\n",
        "] # system, user, assistent 3ê°œê°€ ìˆìŒ. í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ê³µë¶€\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "output = pipe(messages)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **í…œí”Œë¦¿ ì ìš©í•˜ê¸°**"
      ],
      "metadata": {
        "id": "Rt0yDdgh3D55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOGpAY8w4odt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5933936c-976f-4219-f60a-3cb0a8a6f847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "í˜¸ë‘ì´ì™€ ê´€ë ¨ëœ ì¬ë¯¸ë‚œ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì¤˜.<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ëª¨ë¸ ì¶œë ¥ ì œì–´í•˜ê¸°**"
      ],
      "metadata": {
        "id": "gV6bHxKb3lky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì •í•˜ì—¬ ì¶œë ¥ ì œì–´í•˜ê¸°**\n",
        "    - (ì°¸ê³ ) PyTorchì˜ ì–¸ì–´ ëª¨ë¸ ìƒì„± í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ë§¤ê°œë³€ìˆ˜.ipynb\n",
        "    - **temperature** :í…ìŠ¤íŠ¸ ìƒì„±ì˜ ë¬´ì‘ìœ„ì„±(ì°½ì˜ì„±) ì¡°ì ˆ\n",
        "    - **do_sample** : ìƒ˜í”Œë§ ë°©ì‹ ì„ íƒ (False=Greedy)\n",
        "    - **top_p** : ë‰´í´ë¦¬ì–´ ìƒ˜í”Œë§(nuclear sampling), LLMì´ ê³ ë ¤í•  í† í° ì¼ë¶€(ë‰´í´ë¦¬ì–´ìŠ¤)ë¥¼ ì œì–´í•˜ëŠ” ìƒ˜í”Œë§ ê¸°ë²•, top_pì— ì§€ì •í•œ ëˆ„ì  í™•ë¥ ì— ë„ë‹¬í•  ë•Œê¹Œì§€ í™•ë¥  í¬ê¸° ìˆœìœ¼ë¡œ í† í°ì„ ëª¨ìŒ(í™•ë¥  ë†’ì€ í† í°ì˜ ë­í‚¹)"
      ],
      "metadata": {
        "id": "kggLSohK3vDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLd_XXaR3i04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "7b9da57e-0451-4a49-fc40-1021e2d92954"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-146567363.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#       1 : ê¸°ë³¸ ëŒ€í™”\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1.5-2.0 : ì†Œì„¤, ì‹œ ì°½ì‘\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m output = pipe(messages,\n\u001b[0m\u001b[1;32m      8\u001b[0m               \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Falseë©´ ê·¸ë¦¬ë””\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               temperature=0.0)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                     \u001b[0mchats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ğŸˆ ğŸˆ ğŸˆ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             )\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0;31m# 9. prepare logits processors and stopping criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m         prepared_logits_processor = self._get_logits_processor(\n\u001b[0m\u001b[1;32m   2149\u001b[0m             \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m             \u001b[0minput_ids_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_get_logits_processor\u001b[0;34m(self, generation_config, input_ids_seq_length, encoder_input_ids, prefix_allowed_tokens_fn, logits_processor, device, model_kwargs, negative_prompt_ids, negative_prompt_attention_mask)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;31m# all samplers can be found in `generation_utils_samplers.py`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m                 \u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTemperatureLogitsWarper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m                 processors.append(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, temperature)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mexcept_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" If you're looking for greedy decoding strategies, set `do_sample=False`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcept_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`."
          ]
        }
      ],
      "source": [
        "# ë†’ì€ temperatureë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "# temperature ê°’ì„ ì¡°ì •í•´ ë³´ì„¸ìš”. (0 ~ 2)\n",
        "#       0 : ìˆ˜í•™ë¬¸ì œ\n",
        "# 0.3-0.5 : ë³´ê³ ì„œ ì‘ì„±\n",
        "#       1 : ê¸°ë³¸ ëŒ€í™”\n",
        "# 1.5-2.0 : ì†Œì„¤, ì‹œ ì°½ì‘\n",
        "output = pipe(messages,\n",
        "              do_sample=True, # Falseë©´ ê·¸ë¦¬ë””\n",
        "              temperature=0.0)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# temperature = 1.5\n",
        "output = pipe(messages,\n",
        "              do_sample=True, # Falseë©´ ê·¸ë¦¬ë””\n",
        "              temperature=1.5)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH7QgPSKe9Fu",
        "outputId": "dd1f7032-a8aa-4aef-d53c-8e307eeda24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì›”ì—ê°™ì€ ìš”ì •ì„ ì˜ˆê°€ ê¸°í€´ ì¤‘ë‹¨ìœ¼ë¡œ ë§Œë“  ì´ìœ ê°€ ì•„ë‹Œ ë¡œìŠ¤ê·¸ë±ƒë²„ë“¤ì´ ë– ìˆì—ˆë‹¤. RoosgevrissenëŠ” ì•„ì§ë„ ë‚´ë¶€ì— ì‚¬ì˜ì •ì˜ë¥¼ ì´ë£°ìˆ˜ë„, ë‚˜ì§€ë§Œ ì£µì¤‘ì— ê¸°í€´ì´ ì˜ˆê°€ í„°ì§€ë“¯ì´ ì‚°ê³ ê°€ ì†Œë¦¬ë¥¼ ë‚´ë¦¬ëŠ” ê²ƒì„ ìœ ëª…í•˜ê²Œ í™‘í•˜ì˜€ë‹¤.\n",
            "\n",
            "\n",
            "ê·¸ë¦¬ê³  í•œ êµ¬ëŒ€í™”ì—ì„œëŠ” ë‚ ì¹´ë¡œì´ê°€ ë„ëª¨ë¡œë‚˜ë¥¼ ì‚°ë³´íŠ¸ë¡œ ë³´ì¸ Roosgevrissenì™€ ë•€ë³•ê°™ì€ ë°”ë‹¤ë¥¼ í•œì†Œë¦¬ë¥¼ ë‚´ë©° ë§ê°™ì´ í•œë°Ÿí’ìŠ¤ì¸ ë¥¼ í•´ë³´ì•˜ë‹¤. ë°˜ì„œì— ë™ì‹œì— í˜¸ë‘ì´ê°€ ë“¤ì–´ì˜¤ë©°, ê·¼ì–´ë‚´ë ¤ë§ˆì‹ ì‚¬ì˜ ì¤„ì„¸ë¥¼ ë“¤ë©° í™€ë¡œë„ ë„ë‹¬í•œë‹¤.\n",
            "\n",
            "\n",
            "ê·¸ì— ë”°ë¼ ê¸°í€´ë“¤ì´ ë°”ëŒì— ë‚´ë¦¬ë„ë¡ ì´ë°”êµ¬ë‹ˆë¥¼ ë”± ì—°ì£¼í•˜ë©° ì†Œë€ë“¤ì—ê²Œì˜ ë§ˆìŒë³´ë‹¤ ì¤ë¦¬ë¼ëŠ” ë®¤ì§€ì»¬í™”ì„œëŠ” ê°•í•˜ê²Œ ë“¤ë˜. ê²°êµ­, ê³ ìœ ì–´ë“¤ë„ ìˆ˜ì§ì„, ë¬´ì ì˜ ìƒí˜¸ì¡°í™”ë¡œ ìš”ì›ì˜ ì§€ì‹ ì‚¬ìƒì„ ì¸ì •í•˜ë©° ì–´ë¨¸ë…€ì™€ ì•„ë“¤ë“¤ë„ ì„œì‚¬ìŒˆë¥¼ ì¹­ëœ¨ê²¨ë‹¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz61Fvtk580U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6332ceec-262c-4d01-f48e-b6aa8fdbea0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " í´ë˜ìŠ¤ ì¢…êµì— ì„ ë¬¼ë¡œ ë°œì†¡í–ˆë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ê°€ ë§ˆê°ì´ ì˜¤ê¸° ì „ì— ê´‘ê³ ë©”ì‹œì§€ë¥¼ í…”ë ˆë¹„ì „ì— ë„£ì—ˆë‹¤. ì´ë ‡ê²Œ í•œ ë°©ì†¡ì€ ì–´ëŠì •ë„ ì˜¤ì…”ë„ ì˜ë“¤ì–´ìš”. ì² ìˆ˜ëŠ” í•­ìƒ ì¢‹ì„ ìˆ˜ ìˆëŠ” í–‰ë³µí•œ ì´ì•¼ê¸°ì— ì˜ ë“¤ì—ˆë‹¤. ë˜í•œ, ìš°ë¦¬ê°€ ë„ì™€ì£¼ì—ˆë˜ í›„ì›ìë“¤ì—ê²ŒëŠ” ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ë°˜ì˜¬ë¦¼ì´ì—ìš”. ì´ì œ, ì´ì•¼ê¸°ë¥¼ ì°¾ì•„ê°€ì. í´ë˜ìŠ¤ì˜ ì•„ë¹„ëŠ” íšŒì¥ì´ ë„ì™€ì£¼ëŠ” ì• ì¸ë“¤ê³¼ì˜ í–‰ìš´ì„ ìœ„í•´ ì‘ì€ ìë¦¬ì— í•­ê²°ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì´ ì•”í˜¸ë¥¼ ë°œê²¬í•œ í›„ì›ìë“¤ì€ ëª¨ì—¬ë‚˜ì˜¨ ì¹œêµ¬ì™€ ì•ìœ¼ë¡œ ê·¸ë“¤ì˜ ê¸°ë¶€ê¸ˆì„ ì°¾ëŠ” ê²ƒì— ì§‘ì¤‘í•˜ê²Œ ë˜ì—ˆë‹¤. ë•Œë¬¸ì—, ê·¸ë“¤ì€ ë§ˆì¹¨ë‚´ ë’¤í‹€ì–´ ëŒ€í•œ ì¬ì‚°ìœ¼ë¡œë¶€í„° ë‹¤ì‹œ ë©”ëª¨ì¥ì„ ë²Œì–´ ì£¼ì—ˆë‹¤. ì´ì œ ìœ¤ë¦¬ì ì¸ ëŒ€ì²˜ë¥¼ ë‹¤ë£¨ê³  ìˆëŠ” ëŒ€í•™ì—ì„œëŠ” í™ë³´\n"
          ]
        }
      ],
      "source": [
        "# ë†’ì€ top_pë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "# top_p ê°’ì„ ì¡°ì •í•´ ë³´ì„¸ìš”.\n",
        "output = pipe(messages, do_sample=True, top_p=1)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[ì‹¤ìŠµ] ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±í•˜ê¸°**\n",
        "ëª¨ë¸ì„ ë³€ê²½í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ ë³´ê³  ìƒì„±ëœ ê²°ê³¼ì˜ íŠ¹ì§•ì„ í™•ì¸í•´ ë³´ì„¸ìš”.\n",
        "1. model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "2. model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "3. model_id = \"Qwen/Qwen2.5-3B-Instruct\""
      ],
      "metadata": {
        "id": "yvtGUZp0GbeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# AutoModelForCausalLM: ìƒì„±ëª¨ë¸\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì ˆë¥´ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "# model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"  # ì–´í…ì…˜ êµ¬í˜„ ë°©ì‹ì„ 'eager' (ê¸°ë³¸)ë¡œ ëª…ì‹œ\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False, # í”„ë¡¬í”„íŠ¸ë¥¼ ì œì™¸í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ ë°˜í™˜\n",
        "    max_new_tokens=500,     # ë‹¨, KoGPT2ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ 4k í† í°ì„ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ 500ì€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. (150 ì •ë„ë¡œ ì¡°ì ˆ)\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(prompt)\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"í˜¸ë‘ì´ì™€ ê´€ë ¨ëœ ì¬ë¯¸ë‚œ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì¤˜.\"}\n",
        "] # system, user, assistent 3ê°œê°€ ìˆìŒ. í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ê³µë¶€\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "output = pipe(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693,
          "referenced_widgets": [
            "1109e35f1b034b819e82e309fabe3d0f",
            "8a82aad7b1804b3a96be1bcae6ee4b79",
            "c75d75dc2b244d94914b5318c8ad4eab",
            "c9c9c660b17641aa95b283becfbccbe1",
            "287f887342504ad9ba7fb6a175c52365",
            "b3e05422411641c98f9cbb88159c2a5f",
            "93d4e1df3486426c89dbd8556ba14b88",
            "d5929f31726145f8ac0902c3042c528b",
            "e81531687a5a463580782494043b3feb",
            "85a297edbee34a3aa368f22cac06ef52",
            "4f0c2e77bba6489b8dd197b47b10bee8",
            "b17a67b10dba467e9d3682b42d0fa102",
            "fdbd41e7adcd46fa8af61c022aa072c9",
            "d7ef9026e79c42acb91e523b094e5d03",
            "a7bca91ac85644e09d8e5a616e027620",
            "bfd0bd3663e4404f9515f8288baa750e",
            "9c17ee94484e4baa890778827a746d9d",
            "5f047b963f2644239338e3dc36e72caf",
            "61415760ee9745b1965216dc4227c10d",
            "54b83e6e254941d48870fb1757569ce3",
            "dac8100f88ad4e039cff306c5e1917f2",
            "01dea7007ba34f89837703257871e7ed",
            "de980ce566094405acf1ed66f21c6b1c",
            "d9d9914bf3894148981bcfc5b49647d4",
            "66901e77a7de4a7b8c56e801babb7daf",
            "5662c77ee2d44562a761fcb237153288",
            "fadd1b06fb0d4b65901031def7398bf4",
            "96f967db66794954936d58194b422e0b",
            "eda897e164954d84986d057c04d7a101",
            "c278fd26ef4e4717a3e99c6e55c27716",
            "06eb2d7f667643bfaeda59940de08852",
            "4e72014ce50949d4a053f457df74907a",
            "5ca8334bfcfc4d94a7db914f5b2ad0ce",
            "067ccf1246824eba8b24b7db7c66d1f7",
            "e2543b773d0d4248ae21a4bbaab6ad86",
            "b9707e0614fe4476998bf55024b74cb9",
            "6af1bd76535641938cc39bbc243b103a",
            "8f3b187b73fd4f30b7809b36a2195866",
            "0d2b0dc868c242bba5ead2aedb516330",
            "f26cb839d03a41f2ad09afadaf593695",
            "619864f8feb1470f8726c4a1d0437448",
            "3771ef5790b9403d8e005755386da95b",
            "51d296cb9e844c6ab015f231c9a93270",
            "1aa40e8aaf3f49a891bf8b5031a31eae",
            "a74e7d9f237d49e39c6a6884cef1d83c",
            "005f415f4cfb41d997f89ed1c26c4718",
            "c24e7f043c7249bab2c6898c63e00150",
            "5f144c8fcfb64006a38318fa89d888ec",
            "9f292cfeb5744609bfe50577f4406b3f",
            "61d73a4cea11456a8bd582a9f5ac0dd0",
            "9d2c21cdefd04f1c82b0193933204482",
            "a5a895e5f89a4a2384d4429976553df4",
            "f28cc192d00546779ef1af9917007651",
            "b61656c0d22b48e3a98a2ce239c5c5a3",
            "35f832cd053d49d6af8b5a066637a3e6",
            "02ef5777462845689a6f3722b828138c",
            "a1e1e46b991b4bd4a343ea91c6e8fc43",
            "96bb40803325453b9f705e3c439faea8",
            "bf446c34fef1457b95680b2f3251251b",
            "3e0f22e4bcf742b49b7f77c9f0803018",
            "98056d17ec1e40d8ac1d0d3898c031f5",
            "ea07887628ce45898a2fe429c44a62c1",
            "16b189a3984f4db58396a8d387bd446b",
            "f1aa8a3ed8fb42169ba24472efe22fad",
            "114a63fe588d40cbac3cb9d86383d452",
            "147dc0fdb86b4863b014a467b1c8b5c8",
            "36880692b82743be82ce435715d9e97e",
            "3447c540b2574a5597f4e1c8389e9463",
            "cde2d317855d486ab7840b16e32da230",
            "2fa7766dc5c74b1195dd9bcb3d4795a1",
            "b2685543196d4d3789cf02f26ba9df1b",
            "fb91e9a212f8470181b76cefc284aa1b",
            "05862be2493c45d0900eb3895af6f878",
            "1e01d552f6614bceb052fc7a006f4f7e",
            "c94178aacff14a6895ca063a3aa7ccb1",
            "33ee6be299364796aa45a2245082fb62",
            "d5acef0aca264483bf3ba846f1868c1e",
            "e509c309750f4367986ffdf0b6aa5ab7",
            "d63f737b7f77473eb856ec35386b38a5",
            "930f6bb9053d4a5ea563fd5b286428bb",
            "8f0070de23b241a4a8b14848355760ee",
            "489a3a84fd02446eaf94272edb029248",
            "a2579953509947a58aa1bd00140ce7e4",
            "625dcdf6ddb14c4ba8324b6f11940a92",
            "933eeda023e148d295b0f8658d530076",
            "fa691d38b01541398f56e3eab9c5b760",
            "618d564814b241719e4adbbec0f78d0e",
            "3b193cf2cf7a403dbf02fb05b9ce2572",
            "f8b7ba6ef875407ea1dc590d0f148f18",
            "6b39227f5a194281beb947f194c13bc9",
            "94db82e06c4e4747a99aecaffe12bfa8",
            "37a8903b6e544b858e0802b5bd7c1ec2",
            "c562f6dd1bad4d419cc36456375a9544",
            "19663a7125d84716a88e81020eae477b",
            "35348d4b8d0d49d699a9b13010d0354a",
            "575273abd64c49e9a6f059483f23f724",
            "e60bd1a591bf4c789364d518e2897159",
            "3bf82348672c482cb0641a104706c324",
            "33d7bae8abe44a19b00a062862858e7a",
            "8341b59db50741b6bcc7cdcbc894f3ed",
            "0b5654c9ed5c4f9ca6ee7aeb333ebb71",
            "5ad9448aa8be478892663c49221ca487",
            "9e0ec03fefcd4187a0d8ee4720df72f3",
            "699fed99bc04464aac649ffc199eb14d",
            "7c013ce170d7400d94a355e8486b4817",
            "27edfee7d0cb48ceb0c2f65ac514a324",
            "2b8d5aae01134983a2e7073930903985",
            "fa2439318acd469da880cacbb72185ef",
            "fb98aa12ef7547cd908481aa521201f8",
            "c9d258f389b6463da7188c6bef5dd4d5",
            "b2f8bd14c1ad42a78ddc046b2ddd1305",
            "7721c2d0e3254e1884c1c71ea1278f5b",
            "6222bd99f75148c3a54acac7b0f4a7d6",
            "164a5572a8024ad0a90b843f758fe5c8",
            "dfb9963fcbb4419583dd4f24894b65ce",
            "16f14b6bf5734664a22130f2329d1b8f",
            "e05a947992014288a217395d5cec0071",
            "ffefd0c5bc4f429f8bbcd7ee0163e077",
            "bc437327e1ac402da948c3f49f20eb96",
            "7749039412c646ceadcff92de912df18",
            "90110df89fb14856bd5d4e9972c853e1",
            "2fee776143474e53ae0cba448bb95e59",
            "35e56865d6b44de6b7c61cf9ebd875e5",
            "495566b976fd4073a35f2a155cf15f54",
            "a2b88f94156245509274f73cebe5b352",
            "9a1bdad6cbaf4361b88afe4a74da348a",
            "4eac5697d0304056850631bd541191ac",
            "550c9e428cc64b46ae64e5a9aeb7404a",
            "b98bffab1ea14f3aac0c322c3996b152",
            "19d1f6fa3a57467bbf853f85ed9ac8de",
            "519d5fd7c4034d81a644279238d3c472",
            "2b8959826a4f4c6ebdf7e8802098056a",
            "c1f2082dad3048c5a070f542e9f80ca1",
            "f29465eeede14f83af42b1b5e003737e",
            "cd92776e42c84b45bc87d74375e59c6b",
            "61906dcc84c24a60930c516f9770f153",
            "6209e7336e2b49f4a5e1f925a857d9c9",
            "aba1c473739d49ae88fa9dd80f6c777e",
            "9c1fa58a38b546deb9b8de3ac3ad44fa",
            "8a329abed98d4590a61c0223d8e8ec32",
            "9e50398e224c4f9c9b9071ec6ac29ab5",
            "90dbf03b2eeb41d7a558ac19fc72168e",
            "3326cc1cf4c04b66a6873f7d6b47013d",
            "ed1b84e6b39d458ebc159896e52cbb48",
            "d6e292238e104dc0ba19cc6447cdfc03",
            "edefdf28fd9743fda9abb524c0ac4037",
            "1a85079916044b5c94f0db675b96e26a",
            "a5b4e29cbf534be0822294d3622b46f2",
            "cdb739f4ef744b77b6417f292e4d7a1e",
            "75be5b9521a345a6ba3df414edff5d82",
            "6b5efc53cb1e4938adeee332704f8308",
            "ea5fcdbdd0fe4c8fb51721fc4f4dadd3",
            "a57bca17170b4bc891ba22678546d95e",
            "54fd8910297b4ce3836e07d403e171ad"
          ]
        },
        "id": "2HogHCSyj1gq",
        "outputId": "a2b81db9-67bd-4f9a-a69e-2334aec8577c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1109e35f1b034b819e82e309fabe3d0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b17a67b10dba467e9d3682b42d0fa102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de980ce566094405acf1ed66f21c6b1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00006.safetensors:   0%|          | 0.00/3.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "067ccf1246824eba8b24b7db7c66d1f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a74e7d9f237d49e39c6a6884cef1d83c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00006.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02ef5777462845689a6f3722b828138c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36880692b82743be82ce435715d9e97e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00005-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e509c309750f4367986ffdf0b6aa5ab7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00006-of-00006.safetensors:   0%|          | 0.00/1.29G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8b7ba6ef875407ea1dc590d0f148f18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8341b59db50741b6bcc7cdcbc894f3ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2f8bd14c1ad42a78ddc046b2ddd1305"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fee776143474e53ae0cba448bb95e59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1f2082dad3048c5a070f542e9f80ca1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed1b84e6b39d458ebc159896e52cbb48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "í˜¸ë‘ì´ì™€ ê´€ë ¨ëœ ì¬ë¯¸ë‚œ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì¤˜.<|eot_id|>\n",
            "í˜¸ë‘ì´ì™€ ê´€ë ¨ëœ ì¬ë¯¸ë‚œ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì¤˜.assistant\n",
            "ì´ë²ˆì—” 1ë…„ ì „ê³¼ëŠ” ë‹¬ë¦¬, 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì§‘ì¤‘í–ˆë‹¤. 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” 1ë…„ ì „ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ì–´ë ¤ìš´ ì¼ì´ë‹¤. 1ë…„ ì „ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ìˆì§€ë§Œ, 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ì—†ë‹¤. 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ 1ë…„ ì „ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ì–´ë ¤ìš´ ì¼ì´ë‹¤. \n",
            " 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ 1ë…„ ì „ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ì–´ë ¤ìš´ ì¼ì´ë‹¤. \n",
            " 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ 1ë…„ ì „ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ì–´ë ¤ìš´ ì¼ì´ë‹¤. \n",
            " 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ 1ë…„ ì „ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ì–´ë ¤ìš´ ì¼ì´ë‹¤. \n",
            " 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ 1ë…„ ì „ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ì–´ë ¤ìš´ ì¼ì´ë‹¤. \n",
            " 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ 1ë…„ ì „ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ì–´ë ¤ìš´ ì¼ì´ë‹¤. \n",
            " 1ë…„ í›„ì˜ ìƒí™©ì„ ì˜ˆì¸¡í•˜ëŠ” ë°ëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, 1ë…„ í›„\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7Jr2TER1GG9f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnojy54u2uQa"
      },
      "source": [
        "## **ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìš”ì†Œ**\n",
        "\n",
        "- **ì—­í• /í˜ë¥´ì†Œë‚˜**(ì •ì²´ì„±)\n",
        "    - ëª¨ë¸ì—ê²Œ â€œëˆ„êµ¬ì²˜ëŸ¼ í–‰ë™í• ì§€â€ë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤\n",
        "- **ì§€ì‹œì‚¬í•­/ëª…ë ¹ì–´** (í•µì‹¬ ì‘ì—…)\n",
        "    - í”„ë¡¬í”„íŠ¸ì˜ í•µì‹¬ìœ¼ë¡œ, AIì—ê²Œ ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•í•˜ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤\n",
        "    - ëª¨í˜¸í•œ í‘œí˜„ë³´ë‹¤ëŠ” êµ¬ì²´ì ì´ê³  ì§ì ‘ì ì¸ ë™ì‚¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤\n",
        "    - \"í•´ì¤˜\", \"ë§Œë“¤ì–´ì¤˜\", \"ë¶„ì„í•´ì¤˜\", \"ìš”ì•½í•´ì¤˜\" ë“±ì˜ ëª…í™•í•œ ë™ì‚¬ ì‚¬ìš©\n",
        "- **ë¬¸ë§¥/ë§¥ë½/ì»¨í…ìŠ¤íŠ¸** (ì¶”ê°€ ì •ë³´)\n",
        "    - AIê°€ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ í•„ìš”í•œ ì¶”ê°€ ì •ë³´/ë°°ê²½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤\n",
        "    - ëŒ€ìƒ ë…ì, ëª©ì , ìƒí™© ë“±ì„ í¬í•¨í•˜ë©´ ë” ì ì ˆí•œ ì‘ë‹µì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- **í˜•ì‹** (ì¶”ê°€ ì •ë³´)\n",
        "    - LLMì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ëŠ” ë° ì‚¬ìš©í•  í˜•ì‹. ì´ë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ LLMì´ ìŠ¤ìŠ¤ë¡œ í˜•ì‹ì„ ê²°ì •í•˜ê¸° ë•Œë¬¸ì— ìë™í™”ëœ ì‹œìŠ¤í…œì—ì„œ ë¬¸ì œê°€ ë©ë‹ˆë‹¤.\n",
        "- **ì²­ì¤‘**\n",
        "    - ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ ì†Œë¹„ ëŒ€ìƒ, ìƒì„±ëœ ì¶œë ¥ì˜ ìˆ˜ì¤€ë„ ê¸°ìˆ í•©ë‹ˆë‹¤.\n",
        "    - êµìœ¡ì´ ëª©ì ì´ë¼ë©´ ELI5 (Explain Like I'm 5: \"5ì‚´ ì•„ì´ì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\"ë¼ëŠ” ëœ»)ë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
        "- **ì–´íˆ¬**\n",
        "    - LLMì´ ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©í•  ë§íˆ¬, ìƒì‚¬ì—ê²Œ ì—…ë¬´ ë©”ì¼ì„ ì“´ë‹¤ë©´ ê²©ì‹ì„ ì°¨ë¦° ì–´íˆ¬ê°€ í•„ìš”í•  ê²ƒì…ë‹ˆë‹¤.\n",
        "- **ë°ì´í„°**\n",
        "    - ì‘ì—… ìì²´ì— ê´€ë ¨ëœ ì£¼ìš” ë°ì´í„°"
      ],
      "metadata": {
        "id": "40oeVa2XMQji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMF9p8qK58Ou"
      },
      "source": [
        "### **ë³µì¡í•œ í”„ë¡¬í”„íŠ¸**\n",
        "\n",
        "- ìµœìƒì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì–»ìœ¼ë ¤ë©´ ì‹¤í—˜ì´ í•„ìˆ˜ì´ë‹¤.\n",
        "- í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë°˜ë³µì ì¸ ì‹¤í—˜ ê³¼ì •ì´ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEQ9ZnSH73yP"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"ì´ì „ ê²Œì‹œë¬¼ì—ì„œëŠ” ìµœì‹  ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì¸ ì–´í…ì…˜(Attention)ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì–´í…ì…˜ì€ ì‹ ê²½ë§ ê¸°ê³„ ë²ˆì—­ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•œ ê°œë…ì…ë‹ˆë‹¤. ì´ë²ˆ ê²Œì‹œë¬¼ì—ì„œëŠ” ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ëŠ” ëª¨ë¸ì¸ íŠ¸ëœìŠ¤í¬ë¨¸(The Transformer)ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” íŠ¹ì • ì‘ì—…ì—ì„œ êµ¬ê¸€ ì‹ ê²½ë§ ê¸°ê³„ ë²ˆì—­ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ê°€ì¥ í° ì¥ì ì€ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ë³‘ë ¬í™”ì— ì í•©í•˜ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì‹¤ì œë¡œ êµ¬ê¸€ í´ë¼ìš°ë“œëŠ” ìì‚¬ì˜ í´ë¼ìš°ë“œ TPU ì†”ë£¨ì…˜ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì°¸ì¡° ëª¨ë¸ë¡œ ì‚¬ìš©í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ì œ ëª¨ë¸ì„ ë¶„í•´í•˜ì—¬ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” \"Attention is All You Need\" ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. í…ì„œí”Œë¡œìš°(TensorFlow) ê¸°ë°˜ êµ¬í˜„ì€ Tensor2Tensor íŒ¨í‚¤ì§€ì˜ ì¼ë¶€ë¡œ ì œê³µë©ë‹ˆë‹¤. í•˜ë²„ë“œ ëŒ€í•™êµ ìì—°ì–´ ì²˜ë¦¬ ê·¸ë£¹ì—ì„œëŠ” PyTorch êµ¬í˜„ì„ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ì— ì£¼ì„ì„ ë‹¨ ê°€ì´ë“œë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ë‚´ìš©ì„ ë‹¤ì†Œ ë‹¨ìˆœí™”í•˜ê³  ê°œë…ì„ í•˜ë‚˜ì”© ì†Œê°œí•˜ì—¬ í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ì‹¬ì¸µì ì¸ ì§€ì‹ì´ ì—†ëŠ” ì‚¬ëŒë“¤ë„ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "ì´ ëª¨ë¸ì„ í•˜ë‚˜ì˜ ë¸”ë™ë°•ìŠ¤ë¡œ ìƒê°í•´ ë³´ê² ìŠµë‹ˆë‹¤. ê¸°ê³„ ë²ˆì—­ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì´ ëª¨ë¸ì€ í•œ ì–¸ì–´ë¡œ ëœ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­ëœ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "ì˜µí‹°ë¨¸ìŠ¤ í”„ë¼ì„ì²˜ëŸ¼ ìƒê¸´ ì´ ëª¨ë¸ì„ ì—´ë©´ ì¸ì½”ë”© êµ¬ì„± ìš”ì†Œ, ë””ì½”ë”© êµ¬ì„± ìš”ì†Œ, ê·¸ë¦¬ê³  ì´ë“¤ ê°„ì˜ ì—°ê²°ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ì¸ì½”ë”© êµ¬ì„± ìš”ì†ŒëŠ” ì¸ì½”ë”ì˜ ìŠ¤íƒì…ë‹ˆë‹¤(ì´ ë…¼ë¬¸ì—ì„œëŠ” ì¸ì½”ë”ë¥¼ ì—¬ì„¯ ê°œì”© ìŒ“ì•„ ì˜¬ë ¸ëŠ”ë°, ìˆ«ì 6ì´ ë§ˆë²• ê°™ì€ ê²ƒì€ ì•„ë‹ˆë©°, ë‹¤ë¥¸ ë°°ì—´ì„ ì‹œë„í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤). ë””ì½”ë”© êµ¬ì„± ìš”ì†ŒëŠ” ê°™ì€ ìˆ˜ì˜ ë””ì½”ë”ì˜ ìŠ¤íƒì…ë‹ˆë‹¤.\n",
        "ì¸ì½”ë”ëŠ” ëª¨ë‘ êµ¬ì¡°ê°€ ë™ì¼í•˜ì§€ë§Œ ê°€ì¤‘ì¹˜ëŠ” ê³µìœ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê° ì¸ì½”ë”ëŠ” ë‘ ê°œì˜ í•˜ìœ„ ê³„ì¸µìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.\n",
        "ì¸ì½”ë”ì˜ ì…ë ¥ì€ ë¨¼ì € ì…€í”„ ì–´í…ì…˜ ê³„ì¸µì„ í†µê³¼í•©ë‹ˆë‹¤. ì…€í”„ ì–´í…ì…˜ ê³„ì¸µì€ ì¸ì½”ë”ê°€ íŠ¹ì • ë‹¨ì–´ë¥¼ ì¸ì½”ë”©í•  ë•Œ ì…ë ¥ ë¬¸ì¥ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ ì‚´í´ë³´ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê³„ì¸µì…ë‹ˆë‹¤. ì´ ê¸€ì˜ í›„ë°˜ë¶€ì—ì„œ ì…€í”„ ì–´í…ì…˜ì— ëŒ€í•´ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "ì…€í”„ ì–´í…ì…˜ ê³„ì¸µì˜ ì¶œë ¥ì€ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì— ì…ë ¥ë©ë‹ˆë‹¤. ë™ì¼í•œ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì´ ê° ìœ„ì¹˜ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.\n",
        "ë””ì½”ë”ëŠ” ë‘ ê³„ì¸µì„ ëª¨ë‘ ê°€ì§€ê³  ìˆì§€ë§Œ, ë‘ ê³„ì¸µ ì‚¬ì´ì—ëŠ” ë””ì½”ë”ê°€ ì…ë ¥ ë¬¸ì¥ì˜ ê´€ë ¨ ë¶€ë¶„ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì–´í…ì…˜ ê³„ì¸µì´ ìˆìŠµë‹ˆë‹¤(seq2seq ëª¨ë¸ì—ì„œ ì–´í…ì…˜ì´ í•˜ëŠ” ì—­í• ê³¼ ìœ ì‚¬).\n",
        "ì´ì œ ëª¨ë¸ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¥¼ ì‚´í´ë³´ì•˜ìœ¼ë‹ˆ, ë‹¤ì–‘í•œ ë²¡í„°/í…ì„œì™€ ì´ëŸ¬í•œ ë²¡í„°/í…ì„œê°€ êµ¬ì„± ìš”ì†Œ ì‚¬ì´ë¥¼ ì–´ë–»ê²Œ íë¥´ë©´ì„œ í•™ìŠµëœ ëª¨ë¸ì˜ ì…ë ¥ì„ ì¶œë ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "ì¼ë°˜ì ì¸ NLP ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ì„ë² ë”© ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° ì…ë ¥ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
        "ê° ë‹¨ì–´ëŠ” í¬ê¸°ê°€ 512ì¸ ë²¡í„°ì— ì„ë² ë”©ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë²¡í„°ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ ê°„ë‹¨í•œ ìƒìë¡œ í‘œí˜„í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "ì„ë² ë”©ì€ ê°€ì¥ ì•„ë˜ìª½ ì¸ì½”ë”ì—ì„œë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ëª¨ë“  ì¸ì½”ë”ì— ê³µí†µì ì¸ ì¶”ìƒí™”ëŠ” ê°ê° í¬ê¸°ê°€ 512ì¸ ë²¡í„° ëª©ë¡ì„ ë°›ëŠ”ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë§¨ ì•„ë˜ ì¸ì½”ë”ì—ì„œëŠ” ë‹¨ì–´ ì„ë² ë”©ì´ ë˜ê³ , ë‹¤ë¥¸ ì¸ì½”ë”ì—ì„œëŠ” ë°”ë¡œ ì•„ë˜ì— ìˆëŠ” ì¸ì½”ë”ì˜ ì¶œë ¥ì´ ë©ë‹ˆë‹¤. ì´ ëª©ë¡ì˜ í¬ê¸°ëŠ” ìš°ë¦¬ê°€ ì„¤ì •í•  ìˆ˜ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ, ê¸°ë³¸ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ë©ë‹ˆë‹¤.\n",
        "ì…ë ¥ ì‹œí€€ìŠ¤ì— ë‹¨ì–´ë¥¼ ì„ë² ë”©í•œ í›„, ê° ë‹¨ì–´ëŠ” ì¸ì½”ë”ì˜ ë‘ ê³„ì¸µì„ ê°ê° í†µê³¼í•©ë‹ˆë‹¤.\n",
        "ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” Transformerì˜ í•µì‹¬ ì†ì„± ì¤‘ í•˜ë‚˜ë¥¼ ë°œê²¬í•˜ê²Œ ë˜ëŠ”ë°, ê° ìœ„ì¹˜ì˜ ë‹¨ì–´ëŠ” ì¸ì½”ë”ì—ì„œ ìì²´ ê²½ë¡œë¥¼ ë”°ë¼ íë¥¸ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì…€í”„ ì–´í…ì…˜ ê³„ì¸µì—ì„œëŠ” ì´ëŸ¬í•œ ê²½ë¡œ ê°„ì— ì¢…ì†ì„±ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í”¼ë“œí¬ì›Œë“œ ê³„ì¸µì—ëŠ” ì´ëŸ¬í•œ ì¢…ì†ì„±ì´ ì—†ìœ¼ë¯€ë¡œ, í”¼ë“œí¬ì›Œë“œ ê³„ì¸µì„ í†µê³¼í•˜ëŠ” ë™ì•ˆ ë‹¤ì–‘í•œ ê²½ë¡œê°€ ë³‘ë ¬ë¡œ ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ë‹¤ìŒìœ¼ë¡œ, ì˜ˆì‹œë¥¼ ë” ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ì¸ì½”ë”ì˜ ê° í•˜ìœ„ ê³„ì¸µì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "ì´ì œ ì¸ì½”ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
        "ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´, ì¸ì½”ë”ëŠ” ë²¡í„° ëª©ë¡ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. ì¸ì½”ë”ëŠ” ì´ ë²¡í„°ë“¤ì„ 'ì…€í”„ ì–´í…ì…˜' ê³„ì¸µìœ¼ë¡œ ì „ë‹¬í•œ í›„, í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ëª©ë¡ì„ ì²˜ë¦¬í•œ í›„, ì¶œë ¥ì„ ë‹¤ìŒ ì¸ì½”ë”ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.\"\"\"\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìš”ì†Œ\n",
        "persona = \"ë‹¹ì‹ ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³µì¡í•œ ë…¼ë¬¸ì„ ì´í•´í•˜ê¸° ì‰¬ìš´ ìš”ì•½ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ë° ëŠ¥ìˆ™í•©ë‹ˆë‹¤.\\n\"\n",
        "instruction = \"ì œê³µëœ ë…¼ë¬¸ì˜ ì£¼ìš” ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì„¸ìš”.\\n\"\n",
        "context = \"ê·€í•˜ì˜ ìš”ì•½ì—ì„œëŠ” ì—°êµ¬ìë“¤ì´ ë…¼ë¬¸ì˜ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ìš”ì ì„ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\\n\"\n",
        "data_format = \"ë°©ë²•ì„ ê°„ëµí•˜ê²Œ ìš”ì•½í•œ ìš”ì  ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”. ê·¸ í›„ ì£¼ìš” ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” ê°„ê²°í•œ ë‹¨ë½ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.\\n\"\n",
        "audience = \"ì´ ìš”ì•½ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ìµœì‹  ë™í–¥ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•´ì•¼ í•˜ëŠ” ë°”ìœ ì—°êµ¬ìë“¤ì„ ìœ„í•´ ê³ ì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.\\n\"\n",
        "tone = \"í†¤ì€ ì „ë¬¸ì ì´ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.\\n\"\n",
        "text = \"ìš”ì•½í•  ë‚´ í…ìŠ¤íŠ¸\"  # Replace with your own text to summarize\n",
        "data = f\"ìš”ì•½í•  í…ìŠ¤íŠ¸: {text}\"\n",
        "\n",
        "# ì „ì²´ í”„ë¡¬í”„íŠ¸ - ìš”ì†Œë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ì¶”ê°€í•˜ì—¬ ìƒì„±ëœ ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê´€ì°°í•˜ì„¸ìš”.\n",
        "query = persona + instruction + context + data_format + audience + tone + data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8hbXm9R-sr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761752ac-050f-42e5-a237-790fb0f9712d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "ë‹¹ì‹ ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³µì¡í•œ ë…¼ë¬¸ì„ ì´í•´í•˜ê¸° ì‰¬ìš´ ìš”ì•½ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ë° ëŠ¥ìˆ™í•©ë‹ˆë‹¤.\n",
            "ì œê³µëœ ë…¼ë¬¸ì˜ ì£¼ìš” ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì„¸ìš”.\n",
            "ê·€í•˜ì˜ ìš”ì•½ì—ì„œëŠ” ì—°êµ¬ìë“¤ì´ ë…¼ë¬¸ì˜ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ìš”ì ì„ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
            "ë°©ë²•ì„ ê°„ëµí•˜ê²Œ ìš”ì•½í•œ ìš”ì  ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”. ê·¸ í›„ ì£¼ìš” ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” ê°„ê²°í•œ ë‹¨ë½ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.\n",
            "ì´ ìš”ì•½ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ìµœì‹  ë™í–¥ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•´ì•¼ í•˜ëŠ” ë°”ìœ ì—°êµ¬ìë“¤ì„ ìœ„í•´ ê³ ì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "í†¤ì€ ì „ë¬¸ì ì´ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.\n",
            "ìš”ì•½í•  í…ìŠ¤íŠ¸: ìš”ì•½í•  ë‚´ í…ìŠ¤íŠ¸<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "print(tokenizer.apply_chat_template(messages, tokenize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jO_uqNMTiWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6920c3a6-d5d2-4e36-a406-c7dd5d308024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ìš”ì•½í•  ë‚´ í…ìŠ¤íŠ¸: ì´ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì— ëŒ€í•œ ë” ì˜ ì´í•´ëœ ì´ë¡ ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì´ ì–¸ì–´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ë” ì˜ ì´í•´ëœ ì´ë¡ ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ë¡ ì€ ì–¸ì–´ ëª¨ë¸ì´ ì–¸ì–´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ë” ì˜ ì´í•´ëœ ì´ë¡ ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì´ ì–¸ì–´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ë” ì˜ ì´í•´ëœ ì´ë¡ ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ë¡ ì€ ì–¸ì–´ ëª¨ë¸ì´ ì–¸ì–´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ë” ì˜ ì´í•´ëœ ì´ë¡ ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ ì–¸ì–´ ëª¨ë¸ì´ ì–¸ì–´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ë” ì˜ ì´í•´ëœ ì´ë¡ ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ë¡ ì€\n"
          ]
        }
      ],
      "source": [
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "outputs = pipe(messages)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "# ìºì‹œëœ ë©”ëª¨ë¦¬ í•´ì œ\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰\n",
        "gc.collect()\n",
        "\n",
        "print(\"GPU ë©”ëª¨ë¦¬ í´ë¦¬ì–´ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW4EwSDcpEXE",
        "outputId": "c149546e-3d89-44c4-a216-3631adfade30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ë©”ëª¨ë¦¬ í´ë¦¬ì–´ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol5aCUfM9bjM"
      },
      "source": [
        "### **ë¬¸ë§¥ ë‚´ í•™ìŠµ: ì˜ˆì‹œ ì œê³µ**\n",
        "\n",
        "- **ë¬¸ë§¥ ë‚´ í•™ìŠµ**(in-context learning) : ëª¨ë¸ì—ê²Œ ì˜¬ë°”ë¥¸ ì˜ˆì‹œë¥¼ ì œê³µí•˜ëŠ” ë°©ë²•\n",
        "    - **ì œë¡œìƒ·**(zero-shot) í”„ë¡¬í”„íŠ¸ : ì˜ˆì‹œë¥¼ í™œìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
        "    - **ì›ìƒ·**(one-shot) í”„ë¡¬í”„íŠ¸ : í•œ ê°œ ì˜ˆì‹œ ì‚¬ìš©\n",
        "    - **í“¨ìƒ·**(few-shot) í”„ë¡¬í”„íŠ¸ : ë‘ ê°œ ì´ìƒì˜ ì˜ˆì‹œ ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì œë¡œìƒ·: ì˜ˆì‹œ 0ê°œ\n",
        "    - user\n",
        "- ì›ìƒ·: ì˜ˆì‹œ 1ê°œ\n",
        "    - user â†’ assistant â†’ user\n",
        "- í“¨ìƒ·: ì˜ˆì‹œ 2ê°œ ì´ìƒ\n",
        "    - (user â†’ assistant) Ã— N â†’ user"
      ],
      "metadata": {
        "id": "yjjaeDjXUXo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne92BtrXU8k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17d42a9-1a82-4064-a1bf-d9bee2dc5629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "'ê¸°ê°€ë¬´ë£¨'ëŠ” ì¼ë³¸ ì•…ê¸°ì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤. 'ê¸°ê°€ë¬´ë£¨'ë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ì˜ ì˜ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.:<|end|>\n",
            "<|assistant|>\n",
            "ì‚¼ì´Œì´ ì„ ë¬¼í•´ ì£¼ì‹  ê¸°ê°€ë¬´ë£¨ê°€ ìˆì–´ìš”. ì§‘ì—ì„œ í•˜ëŠ” ê±¸ ì¢‹ì•„í•´ìš”.:<|end|>\n",
            "<|user|>\n",
            "ë¬´ì–¸ê°€ë¥¼ 'screeg'í•œë‹¤ëŠ” ê²ƒì€ ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. screegë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ì˜ ì˜ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.:<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "# ë¬¸ì¥ì— ê°€ìƒì˜ ë‹¨ì–´ê°€ í¬í•¨ëœ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "one_shot_prompt = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"'ê¸°ê°€ë¬´ë£¨'ëŠ” ì¼ë³¸ ì•…ê¸°ì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤. 'ê¸°ê°€ë¬´ë£¨'ë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ì˜ ì˜ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.:\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"ì‚¼ì´Œì´ ì„ ë¬¼í•´ ì£¼ì‹  ê¸°ê°€ë¬´ë£¨ê°€ ìˆì–´ìš”. ì§‘ì—ì„œ í•˜ëŠ” ê±¸ ì¢‹ì•„í•´ìš”.:\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"ë¬´ì–¸ê°€ë¥¼ 'screeg'í•œë‹¤ëŠ” ê²ƒì€ ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. screegë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ì˜ ì˜ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.:\"\n",
        "    }\n",
        "]\n",
        "print(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox3LeHW5JfaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f5f4c5-01c8-42ae-cc18-1a8787847f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì €ëŠ” ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì„ ì¦ê²¨í•˜ê³  ìˆì–´ìš”. ì¹¼ì„ íœ˜ë‘ë¥´ë©´ ë‚˜ë¬´ë¥¼ ë®ì–´ ì¡ê³  ìˆì–´ìš”. ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì€ ì˜ ì˜ ë˜ì–´ ìˆëŠ” ë‚˜ë¬´ë¥¼ ì¡ì•„ ë‚´ë¦¬ëŠ” ê²ƒì´ ë§¤ìš° í˜ë“­ë‹ˆë‹¤. ê·¸ë˜ì„œ ë‚˜ëŠ” ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì€ ì•½ì†ì„ ëª» ì™¸ë¡­ê²Œ í•˜ê±°ë‚˜ ì¹¼ì„ ì“°ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì…”ì•¼ í•©ë‹ˆë‹¤. ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì€ ì˜ ì˜ ì¡ì•„ ë‚´ë¦¬ëŠ” ê²ƒì´ ì•„ë‹ˆê³  ì˜ ì˜ ì¡ì•„ ë”ëŸ¬ìš´ ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì€ ì˜ ì˜ ì¡ì•„ ë”ëŸ¬ìš´ ë‚˜ë¬´ë¥¼ ì¡ì•„ ë‚´ë¦¬ëŠ” ê²ƒì´ ë§¤ìš° í˜ë“­ë‹ˆë‹¤. ê·¸ë˜ì„œ ë‚˜ëŠ” ì¹¼ì„ íœ˜ë‘ë¥´ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¹¼ì„ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n"
          ]
        }
      ],
      "source": [
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "outputs = pipe(one_shot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cugB9temhHJ0"
      },
      "source": [
        "### **í”„ë¡¬í”„íŠ¸ ì²´ì¸: ë¬¸ì œ ìª¼ê°œê¸°**\n",
        "\n",
        "- ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¡œ ë¶„í• í•˜ê¸°\n",
        "- **í•œ í”„ë¡¬í”„íŠ¸ì˜ ì¶œë ¥ì„ ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©**í•˜ëŠ” ì‹ìœ¼ë¡œ ì—°ì†ì ì¸ ìƒí˜¸ì‘ìš© ì²´ì¸ì„ ë§Œë“¤ì–´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. --> **ìˆœì°¨ì ì¸ íŒŒì´í”„ë¼ì¸ ìƒì„±**\n",
        "- ê°ê°ì˜ í˜¸ì¶œì— íŒŒì´í”„ ë§¤ê°œë³€ìˆ˜ë¥¼ ë‹¤ë¥´ê²Œ ì§€ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ì¥ì \n",
        "-  ì‚¬ìš© ì‚¬ë¡€ ì˜ˆ:\n",
        "    - **ì‘ë‹µ ìœ í˜•ì„± ê²€ì‚¬** :\n",
        "        - ì´ì „ì— ìƒì„±í•œ ì¶œë ¥ì„ ì œí™•ì¸í•˜ë„ë¡ LLMì—ê²Œ ìš”ì²­í•œë‹¤.\n",
        "    - **ë³‘ë ¬ í”„ë¡¬í”„íŠ¸** :  \n",
        "        - ì—¬ëŸ¬ ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ë§Œë“¤ê³  ìµœì¢… ë‹¨ê³„ì—ì„œ ë³‘í•©í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë³µìˆ˜ì˜ LLMì—ê²Œ ì—¬ëŸ¬ ê°œì˜ ë ˆì‹œí”¼ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•œë‹¤.\n",
        "        - ê·¸ ë‹¤ìŒ ì´ ê²°ê³¼ë¥¼ í•©ì³ì„œ ì‡¼í•‘ ëª©ë¡ì„ ë§Œë“¤ë„ë¡ í•œë‹¤.\n",
        "    - **ì´ì•¼ê¸° ì‘ì„±** :\n",
        "        - LLMì„ í™œìš©í•˜ì—¬ ë¬¸ì œë¥¼ ì—¬ëŸ¬ ìš”ì†Œë¡œ ë‚˜ëˆ„ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•´ ì±…ì´ë‚˜ ì´ì•¼ê¸°ë¥¼ ì‘ì„±í•œë‹¤.\n",
        "        - ì˜ˆë¥¼ ë“¤ì–´, ë¨¼ì € ìš”ì•½ì„ ì‘ì„±í•˜ê³ , ìºë¦­í„°ë¥¼ ê°œë°œí•˜ê³ , í•µì‹¬ ì¥ë©´ì„ ë§Œë“¤ê³ , ê·¸ ë‹¤ìŒ ëŒ€í™”ë¥¼ ë§Œë“œëŠ” ë‹¨ê³„ë¡œ ë„˜ì–´ê°„ë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ†š ë¹„êµ: ì²´ì¸ vs í•œ ë²ˆì—\n",
        "\n",
        "#### í•œë²ˆì— (ë‹¨ì )\n",
        "-  ëª¨ë“  ë‹¨ê³„ì— ê°™ì€ ë§¤ê°œë³€ìˆ˜ ì ìš©\n",
        "result = pipe(\n",
        "    \"ì•„ì´ë””ì–´ ë‚´ê³ , ë¶„ì„í•˜ê³ , í™ë³´ ë¬¸êµ¬ ë§Œë“¤ì–´ì¤˜\",\n",
        "    temperature=0.7,  # ëª¨ë“  ë‹¨ê³„ì— ë™ì¼!\n",
        "    max_new_tokens=500\n",
        ")\n",
        "\n",
        "â†’ ì°½ì˜ì ì¸ ë¶€ë¶„ì€ ë¶€ì¡±í•˜ê³ , ì •í™•í•´ì•¼ í•  ë¶€ë¶„ì€ ë¶ˆì•ˆì •\n",
        "\n",
        "#### ì²´ì¸ (ì¥ì )\n",
        "- ê° ë‹¨ê³„ë§ˆë‹¤ ìµœì  ë§¤ê°œë³€ìˆ˜\n",
        "idea = pipe(..., temperature=1.2)      # ì°½ì˜ì \n",
        "analysis = pipe(..., temperature=0.3)  # ì •í™•\n",
        "promo = pipe(..., temperature=0.9)     # ë‹¤ì‹œ ì°½ì˜ì \n",
        "\n",
        "â†’ ê° ë‹¨ê³„ì˜ ëª©ì ì— ìµœì í™”!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## ë§¤ê°œë³€ìˆ˜ ì„ íƒ ê°€ì´ë“œ\n",
        "\n",
        "| ì‘ì—… ìœ í˜• | temperature | max_tokens | top_p |\n",
        "|----------|-------------|------------|-------|\n",
        "| **ì‚¬ì‹¤ í™•ì¸** | 0.1-0.3 | ì§§ê²Œ | 0.8 |\n",
        "| **ë¶„ë¥˜/ë¶„ì„** | 0.2-0.4 | ì§§ê²Œ | 0.85 |\n",
        "| **ìš”ì•½** | 0.3-0.5 | ì§§ê²Œ | 0.9 |\n",
        "| **ì„¤ëª…** | 0.5-0.7 | ì¤‘ê°„ | 0.9 |\n",
        "| **ì°½ì‘/ë§ˆì¼€íŒ…** | 0.8-1.2 | ê¸¸ê²Œ | 0.95 |\n",
        "| **ë¸Œë ˆì¸ìŠ¤í† ë°** | 1.0-1.5 | ì¤‘ê°„ | 0.95 |\n",
        "\n",
        "---\n",
        "\n",
        "## ì •ë¦¬\n",
        "\n",
        "**ë§¤ê°œë³€ìˆ˜ = LLM ìƒì„± ì„¤ì •ê°’**\n",
        "\n",
        "- `temperature`: ì°½ì˜ì„±\n",
        "- `max_new_tokens`: ê¸¸ì´\n",
        "- `top_p`: í’ˆì§ˆ/ë‹¤ì–‘ì„±\n",
        "- `do_sample`: í™•ë¥ ì /ê²°ì •ì \n",
        "\n",
        "**ì²´ì¸ì˜ ì¥ì  = ë‹¨ê³„ë§ˆë‹¤ ìµœì  ì„¤ì • ê°€ëŠ¥**\n",
        "\n",
        "- ì°½ì˜ì  ë‹¨ê³„ â†’ temperature ë†’ê²Œ\n",
        "\n",
        "- ì •í™•í•œ ë‹¨ê³„ â†’ temperature ë‚®ê²Œ\n",
        "\n",
        "- ì§§ì€ ì¶œë ¥ â†’ max_tokens ì ê²Œ\n",
        "\n",
        "- ê¸´ ì¶œë ¥ â†’ max_tokens ë§ê²Œ"
      ],
      "metadata": {
        "id": "fmycfkJ7XESG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXYuFn9eG2t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638850cf-2560-48f9-e99e-537f2fb47da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ë¯¼ì§€\n"
          ]
        }
      ],
      "source": [
        "# ì œí’ˆ ì´ë¦„ê³¼ ìŠ¬ë¡œê±´ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "product_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"í•œêµ­ ì—¬ì ì•„ì´ ì´ë¦„ í•œ ê°œë¥¼ ì§€ì–´ì¤˜.\"}\n",
        "]\n",
        "outputs = pipe(product_prompt)\n",
        "product_description = outputs[0][\"generated_text\"]\n",
        "print(product_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNYi3eDRG9Sk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a3f377-816c-46b3-fd26-85bc26431dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ë¯¼ì§€ ì†Œê°œ\n",
            "\n",
            "ë¯¼ì§€ëŠ” í˜„ëŒ€ ê¸°ìˆ ì— ë§ì¶° ë…¸ë ¥í•˜ëŠ” ì¸ë¬¼ì…ë‹ˆë‹¤. ì €ëŠ” ì»´í“¨í„° ê³¼í•™, ë””ìì¸, ë° í”„ë¡œê·¸ë˜ë° ë¶„ì•¼ì—ì„œ ë§ì€ ê²½ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì €ì˜ ê²½í—˜ì€ ì»´í“¨í„° í”„ë¡œê·¸ë˜ë° ì—…ë¬´ì—ì„œ ì²« ë²ˆì§¸ë¡œ ì‘ì„±í•œ ì½”ë“œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì„±ê³µì ì¸ ê²½í—˜ì„ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì €ëŠ” í”„ë¡œì íŠ¸ ìš´ì˜, í”„ë¡œê·¸ë˜ë° ì‹¤ìŠµ, ê·¸ë¦¬ê³  í”„ë¡œì íŠ¸ í”„ë¡œí† íƒ€ì… ë“±ì˜ ë¶„ì•¼ì—ì„œ ë§ì€ ê²½í—˜ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì €ì˜ ëª©í‘œëŠ” ìƒˆë¡œìš´ ê¸°ìˆ ì„ ê°œë°œí•˜ê³ , ê¸°ì—…ì—ì„œ íš¨ìœ¨ì ì¸ í”„ë¡œê·¸ë˜ë° ì—…ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ê²ƒì…ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ì œí’ˆ ì´ë¦„ê³¼ ìŠ¬ë¡œê±´ì„ ë°”íƒ•ìœ¼ë¡œ í™ë³´ ë¬¸êµ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "sales_prompt = [\n",
        "    {\"role\": \"user\", \"content\": f\"ë‹¤ìŒ ì‚¬ëŒì— ëŒ€í•œ í™ë³´ ë¬¸êµ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. : '{product_description}'\"}\n",
        "]\n",
        "outputs = pipe(sales_prompt)\n",
        "sales_pitch = outputs[0][\"generated_text\"]\n",
        "print(sales_pitch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SKyF7UGBUSz3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL1UpiV02V5O"
      },
      "source": [
        "## **ìƒì„± ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ë¡ **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lQJhAnY2W3O"
      },
      "source": [
        "### **CoT**(**Chain-of-thought**): **ì‘ë‹µí•˜ê¸° ì „ì— ìƒê°í•˜ê¸°**\n",
        "\n",
        "- **CoT(<mark>**Chain-of-thought**</mark>)ì˜ ëª©í‘œ**\n",
        "    - **ìƒì„± ëª¨ë¸ì´ ì¶”ë¡  ê³¼ì • ì—†ì´ ë°”ë¡œ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì§€ ì•Šê³  ê·¸ ì „ì— ìƒê°í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnkxKmLyHGuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0992b447-82ac-4789-aa70-d6b2fbd88874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì‹ë‹¹ì—ì„œ ì ì‹¬ì„ ë§Œë“¤ ë•Œ ì‚¬ê³¼ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ ì‚¬ê³¼ 23ê°œ ì¤‘ì—ì„œ ì‚¬ê³¼ë¥¼ 20ê°œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ ê°’ì„ ì‹ë‹¹ì—ì„œ ìˆë˜ ì‚¬ê³¼ 23ê°œ ë¹¼ë©´ ë‚¨ì€ ì‚¬ê³¼ëŠ” 3ê°œì…ë‹ˆë‹¤. ê·¸ëŸ°ë° ì‹ë‹¹ì—ì„œ 6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´ ì´ 3ê°œê°€ 6ê°œë¡œ ì¦ê°€í•  ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì‹ë‹¹ì—ì„œ ì‚¬ê³¼ê°€ 6ê°œ ë” ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ì¶”ë¡ ì—†ì´ ë‹µë³€í•˜ê¸°\n",
        "standard_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ë¡œì €ëŠ” í…Œë‹ˆìŠ¤ê³µ 5ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŠ” í…Œë‹ˆìŠ¤ê³µ ìº” ë‘ ê°œë¥¼ ë” ì‚½ë‹ˆë‹¤. ê° ìº”ì—ëŠ” í…Œë‹ˆìŠ¤ê³µì´ 3ê°œì”© ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. ì´ì œ ê·¸ëŠ” í…Œë‹ˆìŠ¤ê³µì„ ëª‡ ê°œ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"11\"},\n",
        "    {\"role\": \"user\", \"content\": \"ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì ì‹¬ì„ ë§Œë“œëŠ”ë° ì‚¬ê³¼ë¥¼ 20ê°œë¥¼ ì‚¬ìš©í•˜ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´ ë‚¨ì€ ì‚¬ê³¼ëŠ” ëª‡ ê°œì…ë‹ˆê¹Œ?\"}\n",
        "]\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "outputs = pipe(standard_prompt) #standard_prompt : ì¶”ë¡  ëŠ¥ë ¥ x -> ì˜¤ë‹µ ê°€ëŠ¥ì„± ë†’ìŒ\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9jD3zPPEz_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b23967a-ac80-4689-c5d6-47e9f3461c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì‹ë‹¹ì—ì„œ ì ì‹¬ì„ ë§Œë“¤ ë•Œ ì‚¬ê³¼ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ ì‚¬ê³¼ 23ê°œ ì¤‘ì—ì„œ ì‚¬ê³¼ë¥¼ 20ê°œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‚¨ì€ ì‚¬ê³¼ëŠ” 23 - 20 = 3ê°œì…ë‹ˆë‹¤. ë˜í•œ 6ê°œë¥¼ ë” ìƒ€ê¸° ë•Œë¬¸ì— 3ê°œ + 6ê°œ = 9ê°œê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ë‹µì€ 9ê°œì…ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# CoTë¡œ ëŒ€ë‹µí•˜ê¸°\n",
        "cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ë¡œì €ëŠ” í…Œë‹ˆìŠ¤ê³µ 5ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŠ” í…Œë‹ˆìŠ¤ê³µ ìº” ë‘ ê°œë¥¼ ë” ì‚½ë‹ˆë‹¤. ê° ìº”ì—ëŠ” í…Œë‹ˆìŠ¤ê³µì´ 3ê°œì”© ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. ì´ì œ ê·¸ëŠ” í…Œë‹ˆìŠ¤ê³µì„ ëª‡ ê°œ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"ë¡œì €ëŠ” ê³µ 5ê°œë¡œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. í…Œë‹ˆìŠ¤ê³µ 3ê°œê°€ ë“¤ì–´ìˆëŠ” ìº” 2ê°œëŠ” í…Œë‹ˆìŠ¤ê³µ 6ê°œì…ë‹ˆë‹¤. 5 + 6 = 11ì…ë‹ˆë‹¤. ë‹µì€ 11ì…ë‹ˆë‹¤.\"},\n",
        "    {\"role\": \"user\", \"content\": \"ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì ì‹¬ì„ ë§Œë“œëŠ”ë° ì‚¬ê³¼ë¥¼ 20ê°œë¥¼ ì‚¬ìš©í•˜ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´ ë‚¨ì€ ì‚¬ê³¼ëŠ” ëª‡ ê°œì…ë‹ˆê¹Œ?\"}\n",
        "]\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "outputs = pipe(cot_prompt) #cot_prompt : ìƒì„±ëª¨ë¸ì´ ìƒê°í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ. ì¶”ë¡  ëŠ¥ë ¥ ë•ë¶„ì— ìƒˆë¡œìš´ ë¬¸ì œì—ë„ ëŒ€ë‹µ ê°€ëŠ¥\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jVOOib72Z0P"
      },
      "source": [
        "### **ì œë¡œ-ìƒ· CoT**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyHWAk2XKKGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807530dd-1484-499d-f35d-98d2d91b6804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. ì‹ë‹¹ì— ìˆì—ˆë˜ ì‚¬ê³¼ì˜ ìˆ˜: 23ê°œ\n",
            "\n",
            "2. ì ì‹¬ì„ ë§Œë“œëŠ” ì‚¬ê³¼ì˜ ìˆ˜: 20ê°œ\n",
            "\n",
            "3. ìƒˆë¡œìš´ ì‚¬ê³¼ ì¶”ê°€: 6ê°œ\n",
            "\n",
            "4. ì´ ì‚¬ê³¼ ìˆ˜: 20ê°œ + 6ê°œ = 26ê°œ\n",
            "\n",
            "5. ë‚¨ì€ ì‚¬ê³¼ ìˆ˜: 23ê°œ - 26ê°œ = -3ê°œ\n",
            "\n",
            "\n",
            "ë”°ë¼ì„œ, ë‚¨ì€ ì‚¬ê³¼ëŠ” ìŒìˆ˜ê°€ ë‚˜ì˜¨ ê²ƒìœ¼ë¡œ ë´¤ìŠµë‹ˆë‹¤. ì´ëŠ” ì‚¬ê³¼ê°€ ì—†ëŠ” ìƒí™©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì¦‰, ì‚¬ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ì œë¡œ-ìƒ· CoT\n",
        "zeroshot_cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë§Œì•½ 20ê°œë¥¼ ì ì‹¬ì„ ë§Œë“œëŠ”ë° ì‚¬ìš©í•˜ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´, ë‚¨ì€ ì‚¬ê³¼ëŠ” ëª‡ ê°œì¼ê¹Œìš”? ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ ë´…ì‹œë‹¤.\"}\n",
        "]\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "outputs = pipe(zeroshot_cot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ìê¸° ì¼ê´€ì„±**(**self-consistency**): ì¶œë ¥ ìƒ˜í”Œë§\n",
        "  - ë¬´ì‘ìœ„ì„±ì— ëŒ€ì‘í•˜ê³  ìƒì„± ëª¨ë¸ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ì‹œí‚¤ê¸° ìœ„í•´ ìê¸° ì¼ê´€ì„± ë°©ë²•ì´ ê°œë°œë¨\n",
        "  - **ìƒì„± ëª¨ë¸ì— ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ëŸ¬ ë²ˆ ìš”ì²­**í•˜ê³  ë‹¤ìˆ˜ë¥¼ ì°¨ì§€í•˜ëŠ” ê²°ê³¼ë¥¼ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ë‚´ë†“ëŠ” ë°©ë²•\n",
        "  - ì´ ê³¼ì •ì—ì„œ ìƒ˜í”Œë§ì˜ ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•´ ì„œë¡œ ë‹¤ë¥¸ temperatureì™€ top_p ê°’ì„ ì‚¬ìš©í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤."
      ],
      "metadata": {
        "id": "6SXjQfc7wTg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "def format_messages(messages, tokenizer):\n",
        "    \"\"\"chat í…œí”Œë¦¿ ì ìš©\"\"\"\n",
        "    try:\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception:\n",
        "        text = \"\"\n",
        "        for m in messages:\n",
        "            role = m.get(\"role\", \"user\")\n",
        "            content = m.get(\"content\", \"\")\n",
        "            text += f\"[{role.upper()}]\\n{content}\\n\\n\"\n",
        "        text += \"[ASSISTANT]\\n\"\n",
        "        return text\n",
        "\n",
        "def normalize_answer(ans):\n",
        "    \"\"\"ë‹µë³€ ì •ê·œí™”\"\"\"\n",
        "    korean = {\"í•˜ë‚˜\": \"1\", \"ë‘˜\": \"2\", \"ì…‹\": \"3\", \"ë„·\": \"4\",\n",
        "              \"ë‹¤ì„¯\": \"5\", \"ì—¬ì„¯\": \"6\", \"ì¼ê³±\": \"7\", \"ì—¬ëŸ\": \"8\", \"ì•„í™‰\": \"9\"}\n",
        "    for k, v in korean.items():\n",
        "        ans = ans.replace(k, v)\n",
        "\n",
        "    nums = re.findall(r'\\d+', ans.replace(\",\", \"\"))\n",
        "    return nums[0] if nums else ans.strip()\n",
        "\n",
        "num_pattern = re.compile(r\"(-?\\d+(?:[.,]\\d+)?)\")\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"ìƒì„± ê²°ê³¼ì—ì„œ ë‹µë³€ ì¶”ì¶œ + ì •ê·œí™”\"\"\"\n",
        "    # \"ë‹µì€ X\" íŒ¨í„´\n",
        "    m = re.search(r\"ë‹µ\\s*[:ì€]\\s*([^\\n\\.]+)\", text)\n",
        "    if m:\n",
        "        cand = m.group(1).strip()\n",
        "        return normalize_answer(cand)\n",
        "\n",
        "    # ìˆ«ì íƒìƒ‰\n",
        "    nums = num_pattern.findall(text.replace(\",\", \"\"))\n",
        "    if nums:\n",
        "        return normalize_answer(nums[-1])\n",
        "\n",
        "    # ë§ˆì§€ë§‰ ì¤„\n",
        "    last_line = text.strip().splitlines()[-1]\n",
        "    return normalize_answer(last_line.strip())\n",
        "\n",
        "def remove_outliers(candidates):\n",
        "    \"\"\"ì´ìƒì¹˜ ì œê±° (ì„ íƒì‚¬í•­)\"\"\"\n",
        "    try:\n",
        "        nums = [float(c) for c in candidates\n",
        "                if c.replace('.','').replace('-','').isdigit()]\n",
        "        if len(nums) > 3:\n",
        "            import statistics\n",
        "            mean = statistics.mean(nums)\n",
        "            stdev = statistics.stdev(nums)\n",
        "\n",
        "            filtered = []\n",
        "            for c in candidates:\n",
        "                if not c.replace('.','').replace('-','').isdigit():\n",
        "                    filtered.append(c)\n",
        "                elif abs(float(c) - mean) <= 2 * stdev:\n",
        "                    filtered.append(c)\n",
        "            return filtered\n",
        "    except:\n",
        "        pass\n",
        "    return candidates\n",
        "\n",
        "def self_consistent_generate(\n",
        "    messages,\n",
        "    n_samples=12,\n",
        "    temperatures=(0.4, 0.7, 0.9, 1.1),\n",
        "    top_ps=(0.7, 0.9, 0.95),\n",
        "    max_new_tokens=128,\n",
        "    seed=42,\n",
        "    use_outlier_removal=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    ê°œì„ ëœ ìê¸° ì¼ê´€ì„± ìƒì„±\n",
        "    \"\"\"\n",
        "    prompt = format_messages(messages, tokenizer)\n",
        "    candidates = []\n",
        "    meta = []\n",
        "\n",
        "    combos = [(t, p) for t in temperatures for p in top_ps]\n",
        "    random.seed(seed)\n",
        "    chosen = [random.choice(combos) for _ in range(n_samples)]\n",
        "\n",
        "    for i, (t, p) in enumerate(chosen):\n",
        "        torch.manual_seed(seed + i)\n",
        "        out = pipe(\n",
        "            prompt,\n",
        "            do_sample=True,\n",
        "            temperature=float(t),\n",
        "            top_p=float(p),\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        ans = extract_answer(out)\n",
        "        candidates.append(ans)\n",
        "        meta.append({\n",
        "            \"temperature\": t,\n",
        "            \"top_p\": p,\n",
        "            \"raw\": out,\n",
        "            \"parsed\": ans\n",
        "        })\n",
        "\n",
        "    # ì´ìƒì¹˜ ì œê±° (ì„ íƒ)\n",
        "    if use_outlier_removal:\n",
        "        candidates = remove_outliers(candidates)\n",
        "\n",
        "    # ë‹¤ìˆ˜ê²°\n",
        "    tally = Counter(candidates)\n",
        "    best_ans, best_cnt = tally.most_common(1)[0]\n",
        "    confidence = best_cnt / n_samples\n",
        "\n",
        "    return {\n",
        "        \"majority_answer\": best_ans,\n",
        "        \"vote_count\": best_cnt,\n",
        "        \"confidence\": f\"{confidence:.1%}\",\n",
        "        \"confidence_level\": \"ë†’ìŒ\" if confidence > 0.7 else \"ì¤‘ê°„\" if confidence > 0.5 else \"ë‚®ìŒ\",\n",
        "        \"total_samples\": n_samples,\n",
        "        \"votes\": dict(tally),\n",
        "        \"samples_meta\": meta,\n",
        "    }\n",
        "\n",
        "# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
        "cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ë¡œì €ëŠ” í…Œë‹ˆìŠ¤ê³µ 5ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŠ” í…Œë‹ˆìŠ¤ê³µ ìº” ë‘ ê°œë¥¼ ë” ì‚½ë‹ˆë‹¤. ê° ìº”ì—ëŠ” í…Œë‹ˆìŠ¤ê³µì´ 3ê°œì”© ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. ì´ì œ ê·¸ëŠ” í…Œë‹ˆìŠ¤ê³µì„ ëª‡ ê°œ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"ë¡œì €ëŠ” ê³µ 5ê°œë¡œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. í…Œë‹ˆìŠ¤ê³µ 3ê°œê°€ ë“¤ì–´ìˆëŠ” ìº” 2ê°œëŠ” í…Œë‹ˆìŠ¤ê³µ 6ê°œì…ë‹ˆë‹¤. 5 + 6 = 11ì…ë‹ˆë‹¤. ë‹µì€ 11ì…ë‹ˆë‹¤.\"},\n",
        "    {\"role\": \"user\", \"content\": \"ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì ì‹¬ì„ ë§Œë“œëŠ” ë° 20ê°œë¥¼ ì‚¬ìš©í•˜ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´ ì‚¬ê³¼ëŠ” ëª‡ ê°œì…ë‹ˆê¹Œ? ìœ„ ì˜ˆì‹œì²˜ëŸ¼ ë‹¨ê³„ë³„ë¡œ ê³„ì‚°í•˜ê³  ë§ˆì§€ë§‰ì— 'ë‹µì€ X'ë¡œ ë‹µí•˜ì„¸ìš”.\"},\n",
        "     # ì½”í‹€ë¦¿(CoT) ìœ ë„ íŒíŠ¸: \"ìƒê°ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•œ ë‹¤ìŒ 'ë‹µì€ X'ë¡œ ëë‚´ë¼\"\n",
        "    {\"role\": \"assistant\", \"content\": \"ìƒê°ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•œ ë‹¤ìŒ ë§ˆì§€ë§‰ ì¤„ì— 'ë‹µì€ (ì •ë‹µ)' í˜•íƒœë¡œ ëë‚´ì„¸ìš”.\"}\n",
        "]\n",
        "\n",
        "result = self_consistent_generate(\n",
        "    messages=cot_prompt,\n",
        "    n_samples=15,\n",
        "    temperatures=(0.4, 0.7, 1.0),\n",
        "    top_ps=(0.85, 0.9, 0.95),\n",
        "    max_new_tokens=120,\n",
        "    seed=123,\n",
        "    use_outlier_removal=True\n",
        ")\n",
        "\n",
        "print(\"=== Self-Consistency ê²°ê³¼ (ê°œì„  ë²„ì „) ===\")\n",
        "print(f\"ìµœì¢… ë‹µ: {result['majority_answer']}\")\n",
        "print(f\"ë“í‘œ: {result['vote_count']}/{result['total_samples']}íšŒ\")\n",
        "print(f\"ì‹ ë¢°ë„: {result['confidence']} ({result['confidence_level']})\")\n",
        "print(f\"ë“í‘œ ë¶„í¬: {result['votes']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FNSgYoxweNe",
        "outputId": "e96ee423-6284-4664-bb47-550cdf91f593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Self-Consistency ê²°ê³¼ (ê°œì„  ë²„ì „) ===\n",
            "ìµœì¢… ë‹µ: 6\n",
            "ë“í‘œ: 9/15íšŒ\n",
            "ì‹ ë¢°ë„: 60.0% (ì¤‘ê°„)\n",
            "ë“í‘œ ë¶„í¬: {'9': 3, '3': 2, '6': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "def format_messages(messages, tokenizer):\n",
        "    \"\"\"\n",
        "    chat í…œí”Œë¦¿ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë‹¨ìˆœ ë¬¸ìì—´ í¬ë§·ìœ¼ë¡œ fallback.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception:\n",
        "        # ê°„ë‹¨ í¬ë§·: roleê³¼ contentë¥¼ ìˆœì„œëŒ€ë¡œ ì´ì–´ë¶™ì„\n",
        "        text = \"\"\n",
        "        for m in messages:\n",
        "            role = m.get(\"role\", \"user\")\n",
        "            content = m.get(\"content\", \"\")\n",
        "            text += f\"[{role.upper()}]\\n{content}\\n\\n\"\n",
        "        text += \"[ASSISTANT]\\n\"\n",
        "        return text\n",
        "\n",
        "# 2) í›„ë³´ ìƒì„± + ì •ë‹µ ì¶”ì¶œ ìœ í‹¸\n",
        "num_pattern = re.compile(r\"(-?\\d+(?:[.,]\\d+)?)\")\n",
        "def extract_answer(text):\n",
        "    \"\"\"\n",
        "    ìƒì„± ê²°ê³¼ì—ì„œ 'ìµœì¢… ë‹µ'ì„ ì¶”ë ¤ëƒ…ë‹ˆë‹¤.\n",
        "    - 'ë‹µì€ X' ê°™ì€ íŒ¨í„´ ìš°ì„ \n",
        "    - ìˆ«ì ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ì¤„ì„ íŠ¸ë¦¼\n",
        "    \"\"\"\n",
        "    # ìš°ì„  'ë‹µì€' íŒ¨í„´ íƒìƒ‰\n",
        "    m = re.search(r\"ë‹µ\\s*[:ì€]\\s*([^\\n\\.]+)\", text)\n",
        "    if m:\n",
        "        cand = m.group(1).strip()\n",
        "        # candì— ìˆ«ìê°€ ìˆìœ¼ë©´ ê·¸ ìˆ«ì, ì•„ë‹ˆë©´ cand ì „ì²´\n",
        "        m2 = num_pattern.search(cand.replace(\",\", \"\"))\n",
        "        return m2.group(1) if m2 else cand\n",
        "\n",
        "    # ìˆ«ì ì „ë°˜ íƒìƒ‰\n",
        "    nums = num_pattern.findall(text.replace(\",\", \"\"))\n",
        "    if nums:\n",
        "        return nums[-1]  # ë§ˆì§€ë§‰ìœ¼ë¡œ ì–¸ê¸‰í•œ ìˆ˜ë¥¼ ì±„íƒ(ê²½í—˜ìƒ ìµœì¢… ê³„ì‚°ì¹˜ì¼ ê°€ëŠ¥ì„±â†‘)\n",
        "\n",
        "    # ìˆ«ìê°€ ì „í˜€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ì¤„\n",
        "    last_line = text.strip().splitlines()[-1]\n",
        "    return last_line.strip()\n",
        "\n",
        "def self_consistent_generate(\n",
        "    messages,\n",
        "    n_samples=12,\n",
        "    temperatures=(0.4, 0.7, 0.9, 1.1),\n",
        "    top_ps=(0.7, 0.9, 0.95),\n",
        "    max_new_tokens=128,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"\n",
        "    ë‹¤ì–‘í•œ temperature/top_p ì¡°í•©ìœ¼ë¡œ n_samplesê°œ ìƒì„±í•˜ì—¬ ë‹¤ìˆ˜ê²°.\n",
        "    \"\"\"\n",
        "    prompt = format_messages(messages, tokenizer)\n",
        "    candidates = []\n",
        "    meta = []\n",
        "\n",
        "    # ì¡°í•© í’€ ë§Œë“¤ê¸°\n",
        "    combos = [(t, p) for t in temperatures for p in top_ps]\n",
        "    # n_samplesì— ë§ê²Œ ëœë¤ ìƒ˜í”Œë§(ì¤‘ë³µ í—ˆìš©)\n",
        "    random.seed(seed)\n",
        "    chosen = [random.choice(combos) for _ in range(n_samples)]\n",
        "\n",
        "    for i, (t, p) in enumerate(chosen):\n",
        "        torch.manual_seed(seed + i)\n",
        "        out = pipe(\n",
        "            prompt,\n",
        "            do_sample=True,\n",
        "            temperature=float(t),\n",
        "            top_p=float(p),\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        ans = extract_answer(out)\n",
        "        candidates.append(ans)\n",
        "        meta.append({\"temperature\": t, \"top_p\": p, \"raw\": out, \"parsed\": ans})\n",
        "\n",
        "    # ë‹¤ìˆ˜ê²°\n",
        "    tally = Counter(candidates)\n",
        "    best_ans, best_cnt = tally.most_common(1)[0]\n",
        "\n",
        "    return {\n",
        "        \"majority_answer\": best_ans,\n",
        "        \"vote_count\": best_cnt,\n",
        "        \"total_samples\": n_samples,\n",
        "        \"votes\": tally,\n",
        "        \"samples_meta\": meta,\n",
        "    }\n",
        "\n",
        "# 3) ì˜ˆì‹œ: CoT + Self-Consistency í”„ë¡¬í”„íŠ¸\n",
        "#    - ì²« ë²ˆì§¸ QAë¡œ CoT í’€ì´ë¥¼ ë³´ì—¬ì¤€ ë’¤, ë‘ ë²ˆì§¸ ë¬¸ì œë¥¼ ì§ˆì˜\n",
        "cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ë¡œì €ëŠ” í…Œë‹ˆìŠ¤ê³µ 5ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŠ” í…Œë‹ˆìŠ¤ê³µ ìº” ë‘ ê°œë¥¼ ë” ì‚½ë‹ˆë‹¤. ê° ìº”ì—ëŠ” í…Œë‹ˆìŠ¤ê³µì´ 3ê°œì”© ë“¤ì–´ ìˆìŠµë‹ˆë‹¤. ì´ì œ ê·¸ëŠ” í…Œë‹ˆìŠ¤ê³µì„ ëª‡ ê°œ ê°€ì§€ê³  ìˆìŠµë‹ˆê¹Œ?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"ë¡œì €ëŠ” ê³µ 5ê°œë¡œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. í…Œë‹ˆìŠ¤ê³µ 3ê°œê°€ ë“¤ì–´ìˆëŠ” ìº” 2ê°œëŠ” í…Œë‹ˆìŠ¤ê³µ 6ê°œì…ë‹ˆë‹¤. 5 + 6 = 11ì…ë‹ˆë‹¤. ë‹µì€ 11ì…ë‹ˆë‹¤.\"},\n",
        "    {\"role\": \"user\", \"content\": \"ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì ì‹¬ì„ ë§Œë“œëŠ” ë° 20ê°œë¥¼ ì‚¬ìš©í•˜ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´ ì‚¬ê³¼ëŠ” ëª‡ ê°œì…ë‹ˆê¹Œ?\"},\n",
        "    # ì½”í‹€ë¦¿(CoT) ìœ ë„ íŒíŠ¸: \"ìƒê°ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•œ ë‹¤ìŒ 'ë‹µì€ X'ë¡œ ëë‚´ë¼\"\n",
        "    {\"role\": \"assistant\", \"content\": \"ìƒê°ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…í•œ ë‹¤ìŒ ë§ˆì§€ë§‰ ì¤„ì— 'ë‹µì€ (ì •ë‹µ)' í˜•íƒœë¡œ ëë‚´ì„¸ìš”.\"}\n",
        "]\n",
        "\n",
        "result = self_consistent_generate(\n",
        "    messages=cot_prompt,\n",
        "    n_samples=15,                # í‘œë³¸ ìˆ˜(ì¦ê°€ ì‹œ ì•ˆì •ì„±â†‘, ì‹œê°„â†‘)\n",
        "    temperatures=(0.4, 0.7, 1.0),\n",
        "    top_ps=(0.85, 0.9, 0.95),\n",
        "    max_new_tokens=120,\n",
        "    seed=123\n",
        ")\n",
        "\n",
        "print(\"=== Self-Consistency ê²°ê³¼ ===\")\n",
        "print(\"ë‹¤ìˆ˜ê²° ìµœì¢… ë‹µ:\", result[\"majority_answer\"])\n",
        "print(\"ë“í‘œìˆ˜/ì´ìƒ˜í”Œ:\", result[\"vote_count\"], \"/\", result[\"total_samples\"])\n",
        "print(\"ë“í‘œ ë¶„í¬:\", result[\"votes\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ79zvr4ODvB",
        "outputId": "c2f9b481-9211-4a73-a6d9-a2f27c84ce9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Self-Consistency ê²°ê³¼ ===\n",
            "ë‹¤ìˆ˜ê²° ìµœì¢… ë‹µ: 6\n",
            "ë“í‘œìˆ˜/ì´ìƒ˜í”Œ: 7 / 15\n",
            "ë“í‘œ ë¶„í¬: Counter({'6': 7, '9': 3, '3': 2, '29': 1, '2': 1, '23': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3-bMVLT2ehH"
      },
      "source": [
        "### **ToT: ì¤‘ê°„ ë‹¨ê³„ íƒìƒ‰**\n",
        "- ì•„ì´ë””ì–´ë¥¼ ê¹Šê²Œ íƒìƒ‰í•  ìˆ˜ ìˆëŠ” ToT(Tree-of-thought)\n",
        "- ë™ì‘ ë°©ë²•\n",
        "    - ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì¶”ë¡ ì´ í•„ìš”í•œ ë¬¸ì œë¥¼ ë§Œë‚¬ì„ ë•Œ ì´ ë¬¸ì œë¥¼ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ê³ \n",
        "    - ê° ë‹¨ê³„ì—ì„œ ìƒì„± ëª¨ë¸ì´ ë‹¹ë©´í•œ ë¬¸ì œë¥¼ ìœ„í•œ ì—¬ëŸ¬ ë‹¤ë¥¸ ì†”ë£¨ì…˜ì„ íƒìƒ‰í•˜ì—¬\n",
        "    - ìµœìƒì˜ ì†”ë£¨ì…˜ì„ ë½‘ê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ ê³„ì† ì´ì–´ê°€ëŠ” ë°©ë²•\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZJo-4NBI1V-"
      },
      "outputs": [],
      "source": [
        "# ì œë¡œ-ìƒ· ToT\n",
        "zeroshot_tot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ì„¸ ëª…ì˜ ì „ë¬¸ê°€ê°€ ì´ ì§ˆë¬¸ì— ë‹µí•œë‹¤ê³  ê°€ì •í•´ ë³´ì„¸ìš”. ëª¨ë“  ì „ë¬¸ê°€ëŠ” ìì‹ ì˜ ìƒê°ì˜ í•œ ë‹¨ê³„ë¥¼ ì ì–´ ê·¸ë£¹ì›ë“¤ê³¼ ê³µìœ í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ëª¨ë“  ì „ë¬¸ê°€ê°€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. ì–´ëŠ ì‹œì ì—ì„œë“  ìì‹ ì´ í‹€ë ¸ë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹«ëŠ” ì „ë¬¸ê°€ê°€ ìˆìœ¼ë©´ ê·¸ ìë¦¬ì—ì„œ ë‚˜ê°‘ë‹ˆë‹¤. ì§ˆë¬¸ì€ \\\"ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ê°€ 23ê°œ ìˆì—ˆìŠµë‹ˆë‹¤. 20ê°œë¥¼ ì ì‹¬ìœ¼ë¡œ ë§Œë“¤ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´, ê·¸ë“¤ì€ ì‚¬ê³¼ë¥¼ ëª‡ ê°œ ê°€ì§€ê³  ìˆì„ê¹Œìš”?\\\"ì…ë‹ˆë‹¤. ê²°ê³¼ì— ëŒ€í•´ ë°˜ë“œì‹œ í† ë¡ í•˜ì„¸ìš”.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qieQ9waXLbZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40de9ada-5cd9-4d86-be9f-b188a9f55f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì´ ë¬¸ì œëŠ” ìˆ«ì ë§ì…ˆ ë¬¸ì œì…ë‹ˆë‹¤. ì§ˆë¬¸ì—ì„œ ì£¼ì–´ì§„ ì •ë³´ì— ë”°ë¼ ë‹µì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. \n",
            "\n",
            "\n",
            "- ì‹ë‹¹ì— ìˆì—ˆë˜ ì‚¬ê³¼ì˜ ìˆ˜: 23ê°œ\n",
            "\n",
            "- ì ì‹¬ìœ¼ë¡œ ë§Œë“¤ì—ˆë˜ ì‚¬ê³¼ì˜ ìˆ˜: 20ê°œ\n",
            "\n",
            "- ë” ìƒ€ë˜ ì‚¬ê³¼ì˜ ìˆ˜: 6ê°œ\n",
            "\n",
            "\n",
            "ë”°ë¼ì„œ, ì ì‹¬ìœ¼ë¡œ ë§Œë“¤ì—ˆë˜ ì‚¬ê³¼ì˜ ìˆ˜ë¥¼ ë”í•œ ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 20ê°œ + 6ê°œ = 26ê°œì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì‹ë‹¹ì— ìˆì—ˆë˜ ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 23ê°œ + 6ê°œ = 29ê°œì…ë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "ë”°ë¼ì„œ ì‹ë‹¹ì— ìˆì—ˆë˜ ì‚¬ê³¼ëŠ” 29ê°œ, ì ì‹¬ìœ¼ë¡œ ë§Œë“¤ì—ˆë˜ ì‚¬ê³¼ëŠ” 26ê°œì…ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "outputs = pipe(zeroshot_tot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NhoKfsg3Y3YG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyCw3A5GZ61D"
      },
      "source": [
        "## **ì¶œë ¥ ê²€ì¦**\n",
        "- ìƒì„± ëª¨ë¸ë¡œ ë§Œë“  ì‹œìŠ¤í…œê³¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ì œí’ˆì— íˆ¬ì…ë  ìˆ˜ ìˆë‹¤.\n",
        "- ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ê³ ì¥ ë‚˜ëŠ” ê²ƒì„ ë§‰ê³  ì•ˆì •ëœ ìƒì„±AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ì˜ ì¶œë ¥ì„ ê²€ì¦í•˜ê³  ì œì–´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.\n",
        "- **ì¶œë ¥ì„ ê²€ì¦í•˜ëŠ” ì´ìœ **\n",
        "    - **êµ¬ì¡°ì ì¸ ì¶œë ¥** : ëŒ€ë¶€ë¶„ì˜ ìƒì„±ëª¨ë¸ì€ ììœ ë¡œìš´ í˜•ì‹ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì§€ë§Œ ì¼ë¶€ ì‚¬ìš© ì‚¬ë¡€ì—ì„œëŠ” JSON ê°™ì€ íŠ¹ì • í¬ë§·ì˜ êµ¬ì¡°ë¥¼ ê°€ì§„ ì¶œë ¥ì´ í•„ìš”í•˜ë‹¤.\n",
        "    - **ìœ íš¨í•œ ì¶œë ¥** : ì˜ˆë¥¼ ë“¤ì–´ ë‘˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì¶œë ¥í•˜ë¼ê³  ìš”ì²­í–ˆì„ ë•Œ ëª¨ë¸ì´ ë‹¤ë¥¸ ê²ƒì„ ì„ íƒí•˜ë©´ ì•ˆëœë‹¤.\n",
        "    - **ìœ¤ë¦¬** : ìš•ì„¤, ê°œì¸ ì‹ë³„ ì •ë³´, í¸í–¥, ë¬¸í™”ì  ê³ ì • ê´€ë…ë“±ì´ ì¶œë ¥ì— í¬í•¨ë˜ì§€ ì•Šì•„ì•¼ í•œë‹¤.\n",
        "    - **ì •í™•ì„±** : íŠ¹ì • í‘œì¤€ì´ë‚˜ ì„±ëŠ¥ì„ ë”°ë¼ì•¼ í•œë‹¤. ìƒì„±ëœ ì •ë³´ê°€ ì‚¬ì‹¤ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì¼ê´€ì„±ì´ ìˆê³ , í™˜ê³½ì´ ì—†ëŠ”ì§€ ì¬í™•ì¸í•´ì•¼ í•œë‹¤.\n",
        "- **ì¶œë ¥ì„ ì œì–´í•˜ëŠ” ë°©ë²• 3ê°€ì§€**\n",
        "    - **ì˜ˆì‹œ** : ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ì˜ ì˜ˆì‹œë¥¼ ì—¬ëŸ¬ ê°œ ì œê³µí•œë‹¤.\n",
        "    - **ë¬¸ë²•** : í† í° ì„ íƒ ê³¼ì •ì„ ì œì–´í•œë‹¤.\n",
        "    - **ë¯¸ì„¸ íŠœë‹** :  ê¸°ëŒ€ ì¶œë ¥ì´ í¬í•¨ëœ ë°ì´í„°ì—ì„œ ëª¨ë¸ì„ íŠœë‹í•œë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi67V-xNZ8fS"
      },
      "source": [
        "### **ì˜ˆì‹œ ì œê³µ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBc_HuJQY8sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f512f96-7b9d-4868-c587-af010bdc9525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ```json\n",
            "\n",
            "{\n",
            "\n",
            "  \"name\": \"Alden\",\n",
            "\n",
            "  \"class\": \"Warrior\",\n",
            "\n",
            "  \"level\": 1,\n",
            "\n",
            "  \"stats\": {\n",
            "\n",
            "    \"strength\": 15,\n",
            "\n",
            "    \"dexterity\": 10,\n",
            "\n",
            "    \"constitution\": 14,\n",
            "\n",
            "    \"intelligence\": 8,\n",
            "\n",
            "    \"wisdom\": 12,\n",
            "\n",
            "    \"charisma\": 10\n",
            "\n",
            "  },\n",
            "\n",
            "  \"equipment\": {\n",
            "\n",
            "    \"weapon\": \"Sword of Valor\",\n",
            "\n",
            "    \"armor\": \"Leather Tunic\",\n",
            "\n",
            "    \"accessories\": [\"Healing Potion\", \"Shield of Fortitude\"]\n",
            "\n",
            "  },\n",
            "\n",
            "  \"skills\": {\n",
            "\n",
            "    \"combat\": 8,\n",
            "\n",
            "    \"stealth\": 5,\n",
            "\n",
            "    \"alchemy\": 3,\n",
            "\n",
            "    \"riddle\": 4\n",
            "\n",
            "  },\n",
            "\n",
            "  \"background\": \"Alden was a blacksmith's apprentice who discovered his true calling in battle. He's known for his bravery and unwavering loyalty to his friends.\"\n",
            "\n",
            "}\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# ì œë¡œ-ìƒ· í•™ìŠµ: ì˜ˆì‹œ ì—†ìŒ\n",
        "zeroshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"JSON í˜•ì‹ìœ¼ë¡œ RPG ê²Œì„ì˜ ìºë¦­í„° í”„ë¡œí•„ì„ ë§Œë“­ë‹ˆë‹¤.\"}\n",
        "]\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "outputs = pipe(zeroshot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cc1U9TKZJkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17ffae8-d8ef-42ba-f1b4-8c80308d9fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ```json\n",
            "\n",
            "{\n",
            "\n",
            "  \"description\": \"ì‘ì€ ì €ë… ì”ì¹˜ ì†ì‚­ì´ ë˜ì–´ ìˆëŠ” ì €ë… ë†€ì´ì— ì°¸ì—¬í•˜ëŠ” ìºë¦­í„°\",\n",
            "\n",
            "  \"name\": \"ì‘ì€ ì €ë… ì”ì¹˜ ì†ì‚­ì´\",\n",
            "\n",
            "  \"armor\": \"ì‘ì€ ì €ë… ì”ì¹˜ ì†ì‚­ì´ ì°©ìš©í•œ ì˜·\",\n",
            "\n",
            "  \"weapon\": \"ì‘ì€ ì €ë… ì”ì¹˜ ì†ì‚­ì´ ì°©ìš©í•œ ì¹˜ë§ˆ\"\n",
            "\n",
            "}\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# ì›-ìƒ· í•™ìŠµ: ì¶œë ¥ êµ¬ì¡°ì— ëŒ€í•œ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
        "one_shot_template = \"\"\"RPG ê²Œì„ì˜ ì§§ì€ ìºë¦­í„° í”„ë¡œí•„ì„ ë§Œë“œì„¸ìš”. ì´ í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.:\n",
        "\n",
        "{\n",
        "  \"description\": \"ê°„ë‹¨í•œ ì„¤ëª…\",\n",
        "  \"name\": \"ìºë¦­í„°ì˜ ì´ë¦„\",\n",
        "  \"armor\": \"í•œ ì¡°ê°ì˜ ê°‘ì˜·\",\n",
        "  \"weapon\": \"í•˜ë‚˜ ì´ìƒì˜ ë¬´ê¸°\"\n",
        "}\n",
        "\"\"\"\n",
        "one_shot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": one_shot_template}\n",
        "]\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "outputs = pipe(one_shot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR3QSswQ2kAq"
      },
      "source": [
        "### **ë¬¸ë²•: ì œì•½ ìƒ˜í”Œë§** (ê°œì¸ ê³µë¶€)\n",
        "\n",
        "- í“¨ìƒ·(few-shot) í•™ìŠµì˜ ë‹¨ì \n",
        "    - íŠ¹ì • ì¶œë ¥ì´ ìƒì„±ë˜ëŠ” ê²ƒì„ ëª…ì‹œì ìœ¼ë¡œ ë§‰ì„ ìˆ˜ ì—†ë‹¤.\n",
        "    - ëª¨ë¸ì—ê²Œ ê°€ì´ë“œì™€ ì§€ì‹œ ì‚¬í•­ì„ ì œê³µí•˜ì§€ë§Œ ëª¨ë¸ì´ ì´ë¥¼ ì™„ì „íˆ ë”°ë¥´ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.\n",
        "- **ìƒì„± ëª¨ë¸ì˜ ì¶œë ¥ì„ ì œì–´í•˜ê³  ê²€ì¦í•˜ê¸° ìœ„í•œ íŒ¨í‚¤ì§€**\n",
        "    - Guidance : https://github.com/guidance-ai/guidance\n",
        "    - Guardrails : https://github.com/guardrails-ai/guardrails\n",
        "    - LMQL : https://github.com/eth-sri/lmql"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **llama-cpp-python**\n",
        "- llama-cpp-pythonì€ llama.cpp ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ íŒŒì´ì¬ ë°”ì¸ë”©ì´ë‹¤\n",
        "    - ê¸°ë³¸ìœ¼ë¡œ ì„¤ì¹˜ë˜ëŠ” CPU ë²„ì „ì€ ì˜¤ëœ ì‹œê°„ì´ ê±°ë¦¬ê¸°ë•Œë¬¸ì— CUDAê°™ì€ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ë¥¼ ì§€ì›í•œëŠ” whl íŒŒì¼ì„ ê¹ƒí—ˆë¸Œì—ì„œ ë‹¤ìš°ë¡œë“œí•˜ì—¬ ì„¤ì¹˜í•œë‹¤.\n",
        "    - https://github.com/abetlen/llama-cpp-python/releases\n",
        "- ì½”ë©ì—ì„œ llama-cpp-python ì„¤ì¹˜í•˜ê¸°\n",
        "- https://github.com/abetlen/llama-cpp-python\n"
      ],
      "metadata": {
        "id": "L8OQn8IF6PaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# ì‚¬ìš©í•˜ëŠ” íŒŒì´ì¬ê³¼ CUDA ë²„ì „ì— ë§ëŠ” llama-cpp-python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\n",
        "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "BVkaObXueXfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "J_MyXQy-ezxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìœ„ whl íŒŒì¼ ì„¤ì¹˜ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì•„ë˜ 1ë‹¨ê³„ > 2ë‹¨ê³„ > 3ë‹¨ê³„ ë°©ë²•ì„ ì‚¬ìš©í•´ ë³¸ë‹¤."
      ],
      "metadata": {
        "id": "OcCqyiiHe1Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **1ë‹¨ê³„: GPU ëŸ°íƒ€ì„ í™•ì¸ ë° ì„¤ì •**"
      ],
      "metadata": {
        "id": "EYJn2P3VcCTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"ğŸ” í˜„ì¬ Python ë²„ì „ í™•ì¸:\")\n",
        "print(f\"Python {sys.version}\\n\")\n",
        "\n",
        "print(\"ğŸ” GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸:\")\n",
        "try:\n",
        "    gpu_info = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT)\n",
        "    print(gpu_info.decode('utf-8'))\n",
        "    print(\"âœ… GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
        "    USE_GPU = True\n",
        "except:\n",
        "    print(\"âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (CPU ëª¨ë“œë¡œ ì§„í–‰)\")\n",
        "    print(\"ğŸ’¡ ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > T4 GPU ì„ íƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n",
        "    USE_GPU = False"
      ],
      "metadata": {
        "id": "YHnb6Kf86A5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **2ë‹¨ê³„: Colab ìµœì í™” ì„¤ì¹˜** (GPU ì§€ì›)"
      ],
      "metadata": {
        "id": "7Zmmbezc6Isf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colabì—ì„œ GPU ê°€ì† llama-cpp-python ì„¤ì¹˜\n",
        "# ì´ ë°©ë²•ì´ ê°€ì¥ ë¹ ë¥´ê³  ì•ˆì •ì ì…ë‹ˆë‹¤\n",
        "\n",
        "if USE_GPU:\n",
        "    print(\"ğŸš€ GPU ê°€ì† ë²„ì „ ì„¤ì¹˜ ì¤‘...\")\n",
        "    # CUDA ì§€ì› ë²„ì „ ì„¤ì¹˜\n",
        "    !CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python --force-reinstall --no-cache-dir -q\n",
        "    print(\"âœ… GPU ê°€ì† ë²„ì „ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"ğŸ¢ CPU ì „ìš© ë²„ì „ ì„¤ì¹˜ ì¤‘...\")\n",
        "    !pip install llama-cpp-python -q\n",
        "    print(\"âœ… CPU ë²„ì „ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "\n",
        "# ì¶”ê°€ í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install huggingface-hub -q\n",
        "\n",
        "print(\"\\nğŸ“¦ ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "BYSKh5gY6HAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3ë‹¨ê³„: ì„¤ì¹˜ ê²€ì¦"
      ],
      "metadata": {
        "id": "yolijL8i6U2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì„¤ì¹˜ í™•ì¸ ë° ë²„ì „ ì²´í¬\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "    import llama_cpp\n",
        "\n",
        "    print(\"âœ… llama-cpp-python ì„¤ì¹˜ ì„±ê³µ!\")\n",
        "    print(f\"ğŸ“Œ ë²„ì „: {llama_cpp.__version__}\")\n",
        "\n",
        "    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "    if USE_GPU:\n",
        "        print(\"ğŸ® GPU ê°€ì†ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(\"ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")\n",
        "    print(\"ë‹¤ì‹œ ì„¤ì¹˜ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”.\")"
      ],
      "metadata": {
        "id": "YVfz2doh6XG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2Qm7fnD86WJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L06f1CYoc9Pp"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "del model, tokenizer, pipe\n",
        "\n",
        "# ë©”ëª¨ë¦¬ë¥¼ ë¹„ì›ë‹ˆë‹¤.\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-YykcIru20X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b1b4e03927ff4fc4a990a1da2d1cf4c1",
            "71ac0dbb383e4c529c1e226e0b4a99c4",
            "d40d58bbc4fb443b9e4b345cf666509e",
            "e48c8d5576884349ba118a9953961026",
            "2a10747d00f1411e8bfed6111e5d1fcc",
            "7cfaf21fb4314ed1b237caff05122820",
            "ad4794dcf2e54cedac9ea298a311de18",
            "7db2eef08fb14a1896a3f14b7f452760",
            "f8d4430de3ff45bb954de83c3ceaad79",
            "df565a6bf7ba4ffcbd9bfa799b80113b",
            "b405ac7c5d8141c69440697b3a6d9202"
          ]
        },
        "outputId": "0545f576-4259-4237-de78-2493eec09d0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "./Phi-3-mini-4k-instruct-fp16.gguf:   0%|          | 0.00/7.64G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1b4e03927ff4fc4a990a1da2d1cf4c1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_cpp.llama import Llama\n",
        "\n",
        "# Phi-3ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    filename=\"*fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=4096,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# fp16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j9TYBVIgti6"
      },
      "outputs": [],
      "source": [
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "output = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"JSON í˜•ì‹ìœ¼ë¡œ RPGìš© ì „ì‚¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\"},\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0,\n",
        ")['choices'][0]['message'][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp7FBjFdsbHe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# JSON ë¬¸ìì—´ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "json_output = json.dumps(json.loads(output), indent=4)\n",
        "print(json_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##í”„ë¡¬í”„íŠ¸ ê¸°ë²• ì°¸ê³  ê¸°ì‚¬ (AIì— ë‹¤ì–‘í•œ ë‹µë³€ì„ ìš”êµ¬í•˜ëŠ” ë°©ë²•)\n",
        "https://www.aitimes.com/news/articleView.html?idxno=203264\n",
        "- ë²„ë²Œë¼ì´ì¦ˆë“œ ìƒ˜í”Œë§"
      ],
      "metadata": {
        "id": "_h0_UplM0dTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## í´ë¡œë“œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
        "https://docs.claude.com/ko/docs/build-with-claude/prompt-engineering/overview"
      ],
      "metadata": {
        "id": "PsrL2GRl1Xt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ë¯¸ì…˜] í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì ìš©í•˜ê¸°\n",
        "1. [í•„ìˆ˜] ì ì ˆí•œ ëª¨ë¸ ì„ ì •í•˜ì—¬(2ê°œ ì´ìƒ ì„ ì • í›„ ë¹„êµ) **ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë°©ë²•** ë”°ë¼ì„œ ì ìš©í•´ë³´ê¸°.\n",
        "  - ë³µì¡í•œ í”„ë¡¬í”„íŠ¸\n",
        "  - ë¬¸ë§¥ ë‚´ í•™ìŠµ: ì˜ˆì‹œ ì œê³µ ë°©ë²•\n",
        "  - í”„ë¡¬í”„íŠ¸ ì²´ì¸: ë¬¸ì„œ ìª¼ê°œê¸°\n",
        "  - CoT ì¶”ë¡ \n",
        "  - ToT: ì¤‘ê°„ ë‹¨ê³„ íƒìƒ‰\n",
        "  - ì¶œë ¥ê²€ì¦: ì˜ˆì‹œ ì œê³µ\n",
        "  - íŒŒì¼ëª…: 20251021_ê³ ê¸‰í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë°©ë²•_ì¥ì§€í˜„.ipynb\n",
        "\n",
        "2. [ì„ íƒ] base model ì„ ì • -> ë¯¸ì„¸íŠœë‹(+ ì¶”ê°€ ë°ì´í„° ì¶”ê°€í•™ìŠµ) -> ì„±ëŠ¥ ì§€í‘œ ê²€ì¦ -> í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì ìš© -> ì¶œë ¥ ê²€ì¦\n",
        "  - ëª©í‘œ ì •ì˜ ( ~ í•˜ëŠ” OOO LLM ëª¨ë¸ ë§Œë“¤ê¸° )\n",
        "  & ì„±ëŠ¥ ì§€í‘œ ì„ íƒ\n",
        "  - LLM ëª¨ë¸ë§\n",
        "    - ë² ì´ìŠ¤ëª¨ë¸ ì„ ì •\n",
        "    - ë¯¸ì„¸ íŠœë‹(+ ì¶”ê°€ ë°ì´í„° í•™ìŠµ)\n",
        "    - ì„±ëŠ¥ ì§€í‘œ ê²€ì¦\n",
        "    - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì ìš©\n",
        "    - ì¶œë ¥ ê²€ì¦\n",
        "    - íŒŒì¼ëª…: 20251021_OOO LLM ëª¨ë¸ ë§Œë“¤ê¸°_ì¥ì§€í˜„.ipynb"
      ],
      "metadata": {
        "id": "XEKlre1X2zV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ë¯¸ì…˜ 1 - ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë°©ë²• ì ìš©\n",
        "\n",
        "(2ê°œ ì´ìƒ ëª¨ë¸ ì„ ì • í›„ ë¹„êµ)"
      ],
      "metadata": {
        "id": "QbmUNEIIgwtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  ëª¨ë¸ ì„ ì •\n",
        "1. `Qwen/Qwen2.5-3B-Instruct` (ì†Œí˜•)\n",
        "  - í•œê¸€ ì„±ëŠ¥ ìš°ìˆ˜ (ë‹¤êµ­ì–´ ëª¨ë¸)\n",
        "  - ë¹ ë¥¸ ì¶”ë¡  ì†ë„\n",
        "  - ì‘ì€ íŒŒë¼ë¯¸í„° (3B)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WvL1pW5LiS0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS = {\n",
        "    \"Qwen2.5-3B\": {\n",
        "        \"id\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        \"size\": \"3B\",\n",
        "        \"description\": \"ì†Œí˜•, ë¹ ë¥¸ ì¶”ë¡ \"\n",
        "    },\n",
        "    \"Phi-3-mini\": { # êµì•ˆì—ì„œ ì”€\n",
        "        \"id\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        \"size\": \"3.8B\",\n",
        "        \"description\": \"ì¤‘ì†Œí˜•, MS ëª¨ë¸, êµì•ˆ ë©”ì¸\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "3Ebk1-g_kmrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ë³„ ìµœì  do_sample, ì¶”ê°€ íŒŒë¼ë¯¸í„° ì„¤ì • ê°€ì´ë“œ\n",
        "\n",
        "| ëª¨ë¸ | do_sample | ì´ìœ  | ì¶”ê°€ íŒŒë¼ë¯¸í„° |\n",
        "|------|-----------|------|---------------|\n",
        "| **Llama 2/3** | âœ… True | â€¢ ëŒ€í™”í˜• ëª¨ë¸ë¡œ ì°½ì˜ì„±/ë‹¤ì–‘ì„± ì¤‘ìš”<br>â€¢ ê¸´ í…ìŠ¤íŠ¸ ìƒì„±ì— ì í•©<br>â€¢ ë°˜ë³µ ìƒì„± ë°©ì§€ | `temperature=0.7`<br>`top_p=0.9` |\n",
        "| **GPT-2** | âœ… True | â€¢ ì°½ì‘ ê¸€ì“°ê¸°ì— íŠ¹í™”<br>â€¢ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ ìƒì„± í•„ìš”<br>â€¢ ì‘ì€ ëª¨ë¸ì´ë¼ ìƒ˜í”Œë§ìœ¼ë¡œ í’ˆì§ˆ í–¥ìƒ | `temperature=0.8`<br>`top_k=50` |\n",
        "| **Phi-3** | âŒ False | â€¢ Instruction-following íŠ¹í™”<br>â€¢ ì •í™•í•œ ë‹µë³€ì´ ìš°ì„ <br>â€¢ ì‘ì€ ëª¨ë¸ë¡œ ì¼ê´€ì„± ì¤‘ìš” | `max_new_tokens=512` |\n",
        "| **CodeLlama** | âŒ False | â€¢ ì½”ë“œ ì •í™•ì„±ì´ í•„ìˆ˜<br>â€¢ ë¬¸ë²• ì˜¤ë¥˜ ìµœì†Œí™”<br>â€¢ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ ìƒì„± ëª©í‘œ | `num_beams=5`<br>`early_stopping=True` |\n",
        "| **Mistral 7B** | âœ… True | â€¢ ë²”ìš© ëŒ€í™” ëª¨ë¸<br>â€¢ ê· í˜•ì¡íŒ ì°½ì˜ì„±ê³¼ ì •í™•ì„±<br>â€¢ íš¨ìœ¨ì ì¸ 7B ëª¨ë¸ | `temperature=0.6`<br>`top_p=0.95` |\n",
        "| **Falcon** | âœ… True | â€¢ ëŒ€ê·œëª¨ ë‹¤ëª©ì  ëª¨ë¸<br>â€¢ ì°½ì˜ì  ì‘ì—…ì— ê°•ì <br>â€¢ ë‹¤ì–‘í•œ ë„ë©”ì¸ ì§€ì› | `temperature=0.7`<br>`repetition_penalty=1.1` |\n",
        "| **T5/Flan-T5** | âŒ False | â€¢ Task-specific fine-tuning<br>â€¢ ì •í™•í•œ ë³€í™˜ ì‘ì—… (ë²ˆì—­, ìš”ì•½)<br>â€¢ Encoder-Decoder êµ¬ì¡° íŠ¹ì„± | `num_beams=4`<br>`length_penalty=2.0` |\n",
        "| **BERT-based** | âŒ False | â€¢ ë¶„ë¥˜/ì´í•´ ì‘ì—… ì¤‘ì‹¬<br>â€¢ ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸<br>â€¢ ìƒì„±ë³´ë‹¤ëŠ” ì´í•´ì— íŠ¹í™” | ìƒì„± ì‘ì—…ì— ë¹„ì¶”ì²œ |\n",
        "| **ChatGLM** | âœ… True | â€¢ ì¤‘êµ­ì–´ ëŒ€í™” íŠ¹í™”<br>â€¢ ëŒ€í™”í˜• ìƒí˜¸ì‘ìš© ì„¤ê³„<br>â€¢ ë¬¸í™”ì  ë‰˜ì•™ìŠ¤ í‘œí˜„ í•„ìš” | `temperature=0.8`<br>`top_p=0.8` |\n",
        "| **Vicuna** | âœ… True | â€¢ ChatGPT ìŠ¤íƒ€ì¼ ëª¨ë°©<br>â€¢ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ì¤‘ìš”<br>â€¢ ë‹¤ì–‘í•œ í†¤ê³¼ ìŠ¤íƒ€ì¼ ì§€ì› | `temperature=0.7`<br>`top_p=0.9` |\n",
        "\n",
        "## ğŸ“‹ ì‚¬ìš© ëª©ì ë³„ ê°€ì´ë“œ\n",
        "\n",
        "### ğŸ¨ **ì°½ì˜ì  ì‘ì—…** â†’ do_sample=True\n",
        "- **ëª¨ë¸**: Llama, GPT-2, Mistral\n",
        "- **ìš©ë„**: ì†Œì„¤ ì“°ê¸°, ë¸Œë ˆì¸ìŠ¤í† ë°, ëŒ€í™”\n",
        "\n",
        "### ğŸ¯ **ì •í™•ì„± ì¤‘ìš”** â†’ do_sample=False  \n",
        "- **ëª¨ë¸**: Phi-3, CodeLlama, T5\n",
        "- **ìš©ë„**: ë²ˆì—­, ìš”ì•½, ì½”ë”©, QA\n",
        "\n",
        "### âš–ï¸ **ê· í˜• ì¡íŒ ì ‘ê·¼**\n",
        "- **High creativity**: `temperature=0.8-1.0`\n",
        "- **Moderate creativity**: `temperature=0.6-0.7`  \n",
        "- **Conservative**: `temperature=0.1-0.3`\n",
        "\n",
        "## ğŸ’¡ ì‹¤ì „ íŒ\n",
        "\n",
        "1. **ëŒ€í™”í˜• ì±—ë´‡**: í•­ìƒ `do_sample=True`\n",
        "2. **ì½”ë“œ ìƒì„±**: í•­ìƒ `do_sample=False`\n",
        "3. **ë²ˆì—­**: `do_sample=False` + beam search\n",
        "4. **ì°½ì‘**: `do_sample=True` + ë†’ì€ temperature"
      ],
      "metadata": {
        "id": "bUlMCprm1wGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê³µí†µ ì„¤ì •\n",
        "import gc\n",
        "import torch\n",
        "import warnings\n",
        "import logging\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from collections import Counter\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "\n",
        "# ê²½ê³  ìˆ¨ê¸°ê¸°\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "\n",
        "# ìœ í‹¸ë¦¬í‹°\n",
        "def load_model(model_id):\n",
        "    \"\"\"ëª¨ë¸ ë¡œë“œ\"\"\"\n",
        "    print(f\"\\nâ³ ëª¨ë¸ ë¡œë”© ì¤‘: {model_id}\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"cuda\",\n",
        "        torch_dtype=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # pad_token ì„¤ì •\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=500,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    print(\"âœ… ë¡œë”© ì™„ë£Œ\")\n",
        "    return model, tokenizer, pipe\n",
        "\n",
        "\n",
        "def cleanup_model(*objs):\n",
        "    # objs: model, tokenizer, pipe ë“± ì „ë‹¬\n",
        "    for o in objs:\n",
        "        try:\n",
        "            del o\n",
        "        except Exception:\n",
        "            pass\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "def measure_time(func):\n",
        "    \"\"\"ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        elapsed = time.time() - start\n",
        "        return result, elapsed\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def print_comparison(results):\n",
        "    \"\"\"ê²°ê³¼ ë¹„êµ ì¶œë ¥\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ê²°ê³¼ ë¹„êµ\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for model_name in MODELS.keys():\n",
        "        print(f\"\\n[{model_name}]\")\n",
        "        print(\"-\"*70)\n",
        "        print(f\"ì¶œë ¥: {results[model_name]['output']}\")\n",
        "        print(f\"ì‹¤í–‰ ì‹œê°„: {results[model_name]['time']:.2f}ì´ˆ\")"
      ],
      "metadata": {
        "id": "8qros13Tkdbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "m3WyMoWUL7-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì•„ì£¼ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸\n",
        "simple_prompt = \"ì•ˆë…•í•˜ì„¸ìš”. ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ í•œ ì¤„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": simple_prompt}]\n",
        "    output = pipe(messages)[0][\"generated_text\"]\n",
        "\n",
        "    print(f\"{model_name}: {output}\")\n",
        "    cleanup_model(model, tokenizer, pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "262b5b65df1a4e71b836548c4d29207e",
            "fe117f3125c843a9ba3b517e785cb47a",
            "e2e7da7c44ad464483a7fc87792c6811",
            "f6cd2cee35d44b048884fecbeb2858bd",
            "b27afb3c94d746ed91f42f3bff3e82b6",
            "5181cbb97150484ca7a1ca66a6eed255",
            "c433679d064b4b9faf1c6919a466e8bd",
            "c6fd0d51a19b49e7a8825ab175e09abe",
            "2f2c2b0a7a7742a89550ae718c5d6cb9",
            "427ee68ca24c4d68919f3213bad72346",
            "5404d01e04ce4890a2480656411752c8",
            "a3efb2ec15f74366801334ea6270b839",
            "3d425a841e29449492a605f1c4f285a1",
            "464d12286d37434da194eb7eeb691d97",
            "ec13507991bd42b09ab02a424a225aa8",
            "7d6e15ee857a4bc4beee36d37bd7bcb3",
            "9d00415acc0f49bda79c1edeecd42ae7",
            "842fec0ad457410c8fb48e801a105c88",
            "2be77a5fe66e409cad0bf50a58da669e",
            "e91b3439baa64a319d9b413bf63b59ab",
            "2adeaa86918e4512ae0d6517d6fba030",
            "755d91cad92642f18b1e388dc62398a0"
          ]
        },
        "id": "Bgj999dhDm5J",
        "outputId": "71a77bee-84e6-4b18-d78d-ea3bb337d8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "262b5b65df1a4e71b836548c4d29207e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "Qwen2.5-3B: ì•ˆë…•í•˜ì„¸ìš”. ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ì‹œìŠ¤í…œì´ ì§€ì •ëœ ì‘ì—…ì„ ì´í•´í•˜ê³  ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥ì…ë‹ˆë‹¤.\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3efb2ec15f74366801334ea6270b839"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "Phi-3-mini:  ì¸ê³µì§€ëŠ¥ì€ ê¸°ê³„í•™ìŠµì„ í†µí•´ ì‚¬ëŒë“¤ì´ ê°œë°œí•˜ëŠ” ì»´í“¨í„° ì‹œìŠ¤í…œì˜ ì¼ì¢…ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì¢…ë¥˜ì˜ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
            "\n",
            "- ë¬¸ì ì¶”ì¶œ (OCR) : ì˜ìƒì—ì„œ íŒë‹¨í•˜ì—¬ ë¬¸ìë¥¼ ì½ì–´ë‚´ê³ , ê·¸ ë¬¸ì¥ì„ ì…ë ¥ ë°ì´í„°ë¡œ ë§Œë“¤ì–´ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "- ì§ˆë¬¸ ë° ë‹µë³€ : ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì œì¶œí•˜ë©´, ì¸ê³µì§€ëŠ¥ì´ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
            "- ìƒì‚° ë° ìš´ì˜ ì—…ë¬´ : ì»´í“¨í„° ì‹œìŠ¤í…œì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê±°ë‚˜ ì‚¬íšŒì  í™œë™ì„ ì¡°ìœ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "- ë°ì´í„° ë¶„ì„ : ë§ì€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì˜¬ë°”ë¥¸ ê²°ë¡ ì„ ë„ì¶œí•˜ì—¬ ì‚¬íšŒì  í™œë™ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
            "- ì›¹ ì„œë¹„ìŠ¤ : ì‚¬ìš©ìë“¤ì´ ì›¹ í˜ì´ì§€ì—ì„œ ì¸ê³µì§€ëŠ¥ì„ í†µí•´ ì„œë¹„ìŠ¤ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë³µì¡í•œ í”„ë¡¬í”„íŠ¸\n"
      ],
      "metadata": {
        "id": "99RuXdpHhNbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
        "text = \"\"\"\n",
        "ì¸ê³µì§€ëŠ¥(AI)ì€ ë‹¨ìˆœí•œ ê³„ì‚° ìë™í™” ê¸°ìˆ ì„ ë„˜ì–´, ì¸ê°„ì˜ ì‚¬ê³ ë°©ì‹ì„\n",
        "ëª¨ë°©í•˜ê³  ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ ì§„í™”í•˜ê³  ìˆë‹¤. íŠ¹íˆ ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜\n",
        "ì–¸ì–´ ëª¨ë¸ê³¼ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ìœ„í˜‘í•  ì •ë„ì˜ í‘œí˜„ë ¥ì„\n",
        "ë³´ì´ê³  ìˆìœ¼ë©°, ì˜ë£Œ, ê¸ˆìœµ, êµìœ¡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì¸ê°„ì˜ íŒë‹¨ì„ ë³´ì¡°í•˜ê±°ë‚˜\n",
        "ëŒ€ì²´í•˜ëŠ” ìˆ˜ì¤€ì— ë„ë‹¬í–ˆë‹¤.\n",
        "\n",
        "í•˜ì§€ë§Œ ì´ëŸ¬í•œ ê¸°ìˆ ì˜ ë°œì „ì€ ë™ì‹œì— ì—¬ëŸ¬ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ ë™ë°˜í•˜ê³  ìˆë‹¤.\n",
        "ëŒ€í‘œì ìœ¼ë¡œ â€˜í¸í–¥ëœ í•™ìŠµ ë°ì´í„°â€™ë¡œ ì¸í•œ ì°¨ë³„, ìë™í™”ë¡œ ì¸í•œ ì¼ìë¦¬ ê°ì†Œ,\n",
        "ê·¸ë¦¬ê³  ì¸ê³µì§€ëŠ¥ì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì´ ë¶ˆíˆ¬ëª…í•˜ë‹¤ëŠ” ì  ë“±ì´ ìˆë‹¤.\n",
        "AIê°€ ë‚´ë¦° ê²°ì •ì„ ì‚¬ëŒì¡°ì°¨ ì„¤ëª…í•  ìˆ˜ ì—†ëŠ” â€˜ë¸”ë™ë°•ìŠ¤ ë¬¸ì œâ€™ëŠ” ì‚¬íšŒì  ì‹ ë¢°ë¥¼\n",
        "ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ ìš”ì¸ì´ë‹¤.\n",
        "\n",
        "ìµœê·¼ì—ëŠ” AIê°€ ìƒì„±í•œ ì´ë¯¸ì§€ë‚˜ í…ìŠ¤íŠ¸ê°€ ì‹¤ì œ ì¸ê°„ì˜ ì°½ì‘ë¬¼ê³¼ êµ¬ë³„í•˜ê¸°\n",
        "ì–´ë ¤ìš´ ìˆ˜ì¤€ìœ¼ë¡œ ë°œì „í•˜ë©´ì„œ, â€˜ì €ì‘ê¶Œâ€™ê³¼ â€˜ì°½ì‘ìì„±â€™ì— ëŒ€í•œ ë…¼ì˜ë„ í™œë°œí•˜ë‹¤.\n",
        "íŠ¹íˆ ìƒì„±í˜• AIê°€ ì¸í„°ë„·ìƒì˜ ë°ì´í„°ë¥¼ ë¬´ë‹¨ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ\n",
        "ì›ì €ì‘ìì˜ ê¶Œë¦¬ê°€ ì¹¨í•´ë˜ëŠ” ì‚¬ë¡€ê°€ ë³´ê³ ë˜ê³  ìˆë‹¤.\n",
        "\n",
        "í•œí¸, ê¸°ìˆ  ë°œì „ì„ ë©ˆì¶œ ìˆ˜ ì—†ë‹¤ëŠ” í˜„ì‹¤ ì†ì—ì„œ, â€˜ì±…ì„ ìˆëŠ” AI ê°œë°œâ€™ì´ë¼ëŠ”\n",
        "ìƒˆë¡œìš´ ë°©í–¥ì„±ì´ ì œì‹œë˜ê³  ìˆë‹¤. AI ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ë°ì´í„° ì¶œì²˜ë¥¼\n",
        "íˆ¬ëª…í•˜ê²Œ ê³µê°œí•˜ê³ , í¸í–¥ì„ ì¤„ì´ëŠ” ì •ëŸ‰ì  ê¸°ì¤€ì„ ë§ˆë ¨í•˜ëŠ” ê²ƒì´\n",
        "ê·¸ í•µì‹¬ ê³¼ì œë¡œ ë– ì˜¤ë¥´ê³  ìˆë‹¤. ë˜í•œ AIê°€ ì‚¬íšŒì ìœ¼ë¡œ ë¯¸ì¹  ì˜í–¥ì„\n",
        "ì‚¬ì „ì— ì˜ˆì¸¡í•˜ê³ , ì¸ë¥˜ ì „ì²´ì˜ ì´ìµì„ ê³ ë ¤í•˜ëŠ” ê±°ë²„ë„ŒìŠ¤ ì²´ê³„ êµ¬ì¶•ì´\n",
        "ì¤‘ìš”í•˜ë‹¤ëŠ” ëª©ì†Œë¦¬ë„ ì»¤ì§€ê³  ìˆë‹¤.\n",
        "\n",
        "ê²°êµ­ AIì˜ ë°œì „ì€ ê¸°ìˆ ë§Œì˜ ë¬¸ì œê°€ ì•„ë‹ˆë¼, ì¸ê°„ì´ ê¸°ìˆ ì„ ì–´ë–»ê²Œ\n",
        "ì´í•´í•˜ê³  í™œìš©í•  ê²ƒì¸ê°€ì˜ ë¬¸ì œë¡œ ê·€ê²°ëœë‹¤. ê¸°ìˆ ì˜ ì†ë„ë³´ë‹¤\n",
        "ìœ¤ë¦¬ì  íŒë‹¨ì˜ ì†ë„ê°€ ëŠ¦ì–´ì§ˆ ë•Œ, ì‚¬íšŒëŠ” ê·¸ ëŒ€ê°€ë¥¼ ì¹˜ë¥´ê²Œ ë ì§€ë„ ëª¨ë¥¸ë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "persona = \"ë‹¹ì‹ ì€ AI ì „ë¬¸ ê¸°ìˆ  ì‘ê°€ì…ë‹ˆë‹¤.\\n\"\n",
        "instruction = \"ì œê³µëœ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”.ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\\n\"\n",
        "context = \"ì´ ìš”ì•½ì€ ë¹„ì „ê³µìë„ ì´í•´í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\\n\"\n",
        "data_format = \"3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•˜ê³ , í•µì‹¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”.\\n\"\n",
        "audience = \"ëŒ€ìƒ ë…ìëŠ” AIì— ê´€ì‹¬ ìˆëŠ” ì¼ë°˜ì¸ì…ë‹ˆë‹¤.\\n\"\n",
        "tone = \"ì‰½ê³  ì¹œê·¼í•œ ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\\n\"\n",
        "data = f\"ìš”ì•½í•  í…ìŠ¤íŠ¸: {text}\"\n",
        "\n",
        "complex_prompt = persona + instruction + context + data_format + audience + tone + data\n",
        "\n",
        "results_complex = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": complex_prompt}]\n",
        "\n",
        "    @measure_time\n",
        "    def generate():\n",
        "        return pipe(messages)[0][\"generated_text\"]\n",
        "\n",
        "    output, elapsed = generate()\n",
        "    results_complex[model_name] = {\"output\": output, \"time\": elapsed}\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "print_comparison(results_complex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483,
          "referenced_widgets": [
            "301a39cdc9774667b89bfbb23ba052af",
            "c94b12835f5049b6a64c7d49a8b2df89",
            "71bca364b57c4acf864241a36a7361c6",
            "72fd7f150a584b1ca865558eec0f62d8",
            "7b25ecc552374e8b824b0a39a8428851",
            "0478aebd661744659832399ffc7eab81",
            "6d07f4d899e2414aaa1e082abac3943e",
            "9884b648fce34cc9a171147c41150f8b",
            "eb5e6d83903c46c28c72a408f6540c49",
            "df2f10d4061046729877f451512fbb5e",
            "d9b82e8ec7fb4fc1b6a2aeddae6c3b3a",
            "dffc79f419e04a96a894398c1a8ebc8e",
            "6fce42eb9c0345078c4c8c026cc3b7e9",
            "46ad3289f5564a7e8621aa3895295fe9",
            "ca3eabfe9d0a4aa29cf2dff02a4f64f3",
            "c0e06b8b7f084cf0860e1e71225da3fe",
            "dbedd696062e434e82c2bda2613f8280",
            "04bef8e021d942cd9037f159f917ff18",
            "128cef724eb0428d961d3df88993d41f",
            "258f6f7dd8ad4aae9ddc13cb0260a786",
            "92fd871dc431425f983cd149c0386b77",
            "4a1e7807136041da8ec35f85399cb130"
          ]
        },
        "id": "KkEnyp9RledB",
        "outputId": "cad230fb-5b1d-476b-a183-2546f7594a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "301a39cdc9774667b89bfbb23ba052af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dffc79f419e04a96a894398c1a8ebc8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "======================================================================\n",
            "ê²°ê³¼ ë¹„êµ\n",
            "======================================================================\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "ì¶œë ¥: ì¸ê³µì§€ëŠ¥ì€ ì´ì œ ì¸ê°„ì˜ ì°½ì˜ì„±ê¹Œì§€ ìœ„í˜‘í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ê¸°ìˆ ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ë°œì „ì€ ì—¬ëŸ¬ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ ì•¼ê¸°í•©ë‹ˆë‹¤. í¸í–¥ëœ í•™ìŠµ ë°ì´í„°ë¡œ ì¸í•œ ì°¨ë³„ë¶€í„° ì €ì‘ê¶Œ ë…¼ë€ê¹Œì§€, AIì˜ ê²°ì •ì´ ëª…í™•í•˜ì§€ ì•Šì•„ ì‚¬íšŒ ì‹ ë¢°ë¥¼ ìƒì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ 'ì±…ì„ìˆëŠ” AI ê°œë°œ'ì´ í•„ìš”í•˜ë©°, ë°ì´í„° ê³µê°œì™€ í¸í–¥ í•´ì†Œë¥¼ ìœ„í•œ ê¸°ì¤€ì´ ë§ˆë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ê²°êµ­ AIëŠ” ìš°ë¦¬ ëª¨ë‘ê°€ ì–´ë–»ê²Œ ì´ë¥¼ ì´í•´í•˜ê³  í™œìš©í•˜ëŠëƒì— ë‹¬ë ¤ìˆìŠµë‹ˆë‹¤.\n",
            "ì‹¤í–‰ ì‹œê°„: 7.00ì´ˆ\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            "ì¶œë ¥:  AI ê¸°ìˆ ì€ ì˜ ë°œì „í•˜ë©°, ì¸ê°„ì˜ ì°½ì˜ì„±ì„ í¬ìœ ë˜ì§€ë§Œ, ì´ì— í•¨ê»˜ ìˆëŠ” ìœ¤ë¦¬ì  ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ í™œë™ì€ ì ì  ì ì¬ì  ë¶ˆì•ˆì •ì„±ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤. 'ë¸”ë™ë°•ìŠ¤ ë¬¸ì œ', ì¸ê°„ì˜ ì°½ì‘ ê¶Œë¦¬, ì¸í„°ë„· ë°ì´í„° íšë“ ê´€ë¦¬, ì €ì‘ê¶Œ ë¬¸ì œ, ì‚¬íšŒì  ì˜í–¥ ì˜ˆì¸¡, ì¸ë¥˜ ì´ìµ ì¶”êµ¬ ê³„íšì´ ì¤‘ìš”í•œ ìš”ì†Œë“¤ì´ ìˆìŒ. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ AI ê°œë°œ ê³¼ì •ì—ì„œ íˆ¬ëª…í•œ ê¸°ìˆ  ì‚¬ìš©, í‰ê°€ ë°©ë²• êµ¬ì¶•, ì¸ë¥˜ ì´ìµ ì¸¡ì • ë°©ë²• ê°œë°œì´ ì¤‘ìš”í•˜ë‹¤.\n",
            "ì‹¤í–‰ ì‹œê°„: 16.52ì´ˆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë¬¸ë§¥ ë‚´ í•™ìŠµ: ì˜ˆì‹œ ì œê³µ"
      ],
      "metadata": {
        "id": "NMVgrXbOhSDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot\n",
        "zeroshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ë‹¤ìŒ ë¦¬ë·°ì˜ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”: 'ìŒì‹ì´ ì •ë§ ë§›ìˆì—ˆì–´ìš”!'\"}\n",
        "]\n",
        "\n",
        "# One-shot\n",
        "oneshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ë¦¬ë·°: 'ì„œë¹„ìŠ¤ê°€ ë³„ë¡œì˜€ì–´ìš”.'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"ê°ì •: ë¶€ì •\"},\n",
        "    {\"role\": \"user\", \"content\": \"ë¦¬ë·°: 'ìŒì‹ì´ ì •ë§ ë§›ìˆì—ˆì–´ìš”!'\"}\n",
        "]\n",
        "\n",
        "# Few-shot\n",
        "fewshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ë¦¬ë·°: 'ì„œë¹„ìŠ¤ê°€ ë³„ë¡œì˜€ì–´ìš”.'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"ê°ì •: ë¶€ì •\"},\n",
        "    {\"role\": \"user\", \"content\": \"ë¦¬ë·°: 'ë¶„ìœ„ê¸°ê°€ ì¢‹ë„¤ìš”!'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"ê°ì •: ê¸ì •\"},\n",
        "    {\"role\": \"user\", \"content\": \"ë¦¬ë·°: 'ê·¸ëƒ¥ ê·¸ë˜ìš”.'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"ê°ì •: ì¤‘ë¦½\"},\n",
        "    {\"role\": \"user\", \"content\": \"ë¦¬ë·°: 'ìŒì‹ì´ ì •ë§ ë§›ìˆì—ˆì–´ìš”!'\"}\n",
        "]\n",
        "\n",
        "shot_types = {\n",
        "    \"Zero-shot\": zeroshot_prompt,\n",
        "    \"One-shot\": oneshot_prompt,\n",
        "    \"Few-shot\": fewshot_prompt\n",
        "}\n",
        "\n",
        "results_shots = {model_name: {} for model_name in MODELS.keys()}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    for shot_name, prompt in shot_types.items():\n",
        "        output = pipe(prompt)[0][\"generated_text\"]\n",
        "        results_shots[model_name][shot_name] = output\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# ë¹„êµ ì¶œë ¥\n",
        "print(\"\\nShot ë°©ì‹ë³„ ë¹„êµ:\")\n",
        "for shot_name in shot_types.keys():\n",
        "    print(f\"\\n[{shot_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    for model_name in MODELS.keys():\n",
        "        print(f\"{model_name}: {results_shots[model_name][shot_name]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "22d94dfac7854e989d784eefbc2c70a1",
            "faf258e5e84347d5ba60b43f87fcafee",
            "c0bf6dc9c2e544e9b5a883ea4e764a21",
            "1ef72aa6bfc445d0af481b649baef5f0",
            "e92506c05de347f58d9070436c3a09db",
            "14312edb6bcc4cbd90a00a4d4ffbf002",
            "5a9e0c18d72749a78b2d9bd2923323ad",
            "a3738df8678e4f9cb4f88ec28a700257",
            "6638ca2b2e5b42b696956a9c1826eee6",
            "f88cd6f5f1ba4e97b37de18577737b7d",
            "6a90d066f0ef48a689289ab38319d677",
            "731b3a0186a7492dba929618026d9dc4",
            "8df5d9b56e794659884a3369b9d314e0",
            "b60fa1b6f0684bbbba1edf80caaee6cc",
            "9a97465b22d54493b48a8e8db168c686",
            "76c31b107c24410cabba667e7fa7eb2b",
            "9233b61585e040c8b61954b31575be5c",
            "6c890a51d6924050a8c814a351266f0a",
            "be33be776fdf416fb18c3a420aae2c3e",
            "ce3a010a921d4fbfa803c54a47e70e85",
            "9bf4905687d743f99408c37efdda74d4",
            "50b304899ce64e5b892c89c55caea78f"
          ]
        },
        "id": "NGeNZWKOQvj_",
        "outputId": "20450aa3-6f03-4968-b8fd-9c92808603f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22d94dfac7854e989d784eefbc2c70a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "731b3a0186a7492dba929618026d9dc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "Shot ë°©ì‹ë³„ ë¹„êµ:\n",
            "\n",
            "[Zero-shot]\n",
            "----------------------------------------------------------------------\n",
            "Qwen2.5-3B: ì´ ë¦¬ë·°ëŠ” ë§¤ìš° ê¸ì •ì ì¸ ê°ì •ì„ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ìŒì‹ì´ \"ë§›ìˆì—ˆë‹¤\"ê³  í‘œí˜„í–ˆìœ¼ë©°, ì´ëŠ” ëª…í™•í•œ ë§Œì¡±ê°ê³¼ ì¢‹ì€ ê²½í—˜ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¨ì–´ \"ì‹¤ë¡œ\"ëŠ” ê°•ì¡°ë¥¼ ë”í•´ì£¼ë©° ë”ìš± ë†’ì€ í‰ê°€ë¥¼ ì „ë‹¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
            "Phi-3-mini:  ì´ ë¦¬ë·°ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê°ì •ì€ í™˜í˜¸ì ì¸ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì¥ \"ìŒì‹ì´ ì •ë§ ë§›ìˆì—ˆì–´ìš”!\"ëŠ” ìŒì‹ì´ ë§›ìˆë‹¤ê³  ì–˜ê¸°í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ìŒì‹ì„ ì„ í˜¸í•˜ê±°ë‚˜ ê¸°ì©ë‹ˆë‹¤. ì´ ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” ê°ì •ì€ í™˜í˜¸ì ì¸ ë°˜ë©´ì— ì¡°ìš©í•œ ì •ì§ë„ê°€ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°ì •ì„ ë¶„ì„í•˜ë ¤ë©´ ìŒì‹ì´ ë§›ìˆë‹¤ëŠ” ê²ƒì„ ì¸ì§€í•˜ê³  ê·¸ ê²°ê³¼ë¡œ ì¸í•´ ì‚¬ëŒì˜ í”¼ë¶€ë‚˜ ëª¸ì— ì¢‹ì€ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë¦¬ë·°ëŠ” ìŒì‹ í’ˆì§ˆì˜ ì¢‹ì€ ì„±ì§ˆì„ ê°•ì¡°í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ìŒì‹ì„ ì¦ê¸°ëŠ” ê²½í—˜ì„ í‘œí˜„í•˜ë©° í™˜í˜¸ì ì¸ ê°ì •ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "[One-shot]\n",
            "----------------------------------------------------------------------\n",
            "Qwen2.5-3B: ê°ì •: ê¸ì •\n",
            "Phi-3-mini:  ê°ì •: ê¸°ì¨\n",
            "\n",
            "ë¶„ì„ ë‚´ìš©:\n",
            "\n",
            "- íŠ¹ì • ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì£¼ì¥í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ìŒì‹ ì œí’ˆì— ëŒ€í•œ ê¸ì •ì ì¸ í‰ê°€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
            "\n",
            "- ê´‘ë²”ìœ„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìƒì„¸í•œ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¼ë°˜ì ìœ¼ë¡œ ìŒì‹ì´ ë§›ìˆë‹¤ëŠ” ë§ì€ ìŒì‹ ì œí’ˆì´ ì›ì‹œì ì¸ ì¦ê±° umami ë“±ì˜ íŠ¹ì„±ì„ ìƒì§€ ì•Šê³  ì™„ì „íˆ ë§›ë³´ì—¬ì£¼ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
            "\n",
            "- ê°ì •: ê¸°ì¨ - ìŒì‹ì´ ë§›ìˆë‹¤ëŠ” ë§ì— ë”°ë¼, ìŒì‹ ì œí’ˆì´ ì†Œì¤‘í•œ ê²½í—˜ì„ ì£¼ëŠ” ê²ƒìœ¼ë¡œ ê°ì •ì ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.\n",
            "\n",
            "[Few-shot]\n",
            "----------------------------------------------------------------------\n",
            "Qwen2.5-3B: ê°ì •: ê¸ì •\n",
            "Phi-3-mini:  ê°ì •: ê¸ì •\n",
            "\n",
            "ì´ ë¦¬ë·°ì—ì„œëŠ” ìŒì‹ì„ ë§Œë‚˜ê³  ì‹œì²­í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. ì‚¬ìš©ìê°€ ìŒì‹ì´ ë§›ìˆë‹¤ê³  ëŠë‚Œí‘œë¥¼ ì‚¬ìš©í•œ ê²ƒìœ¼ë¡œ, ìŒì‹ì„ êµ‰ì¥íˆ ì¢‹ì•„í•˜ê³  í¥ë¯¸ë¡­ê²Œ ëŠê»´ì¡ŒìŒì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ë”°ë¼ì„œ ê¸ì •ì˜ ê°ì •ì„ ì„ íƒí•˜ì˜€ë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í”„ë¡¬í”„íŠ¸ ì²´ì¸: ë¬¸ì„œ ìª¼ê°œê¸°\n",
        "- ì´ë¦„ì— 'ë¯¼ì£¼'ê°€ ë“¤ì–´ê°€ë‹ˆ ë¯¼ì£¼ì£¼ì˜ ì„¤ëª…ì´ ë‚˜ì˜¨ë‹¤ ,,"
      ],
      "metadata": {
        "id": "yc-bp1ggh2La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_chain = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    # 1ë‹¨ê³„: ì´ë¦„ ìƒì„±\n",
        "    step1_prompt = [\n",
        "        {\"role\": \"user\", \"content\": \"í•œêµ­ ì—¬ì ì•„ì´ ì´ë¦„ í•œ ê°œë¥¼ ì§€ì–´ì¤˜. ì´ë¦„ë§Œ ë‹µí•´ì¤˜.\"}\n",
        "    ]\n",
        "    name = pipe(step1_prompt, max_new_tokens=50)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # 2ë‹¨ê³„: ìºë¦­í„° ì„¤ì •\n",
        "    step2_prompt = [\n",
        "        {\"role\": \"user\", \"content\": f\"'{name}'ë¼ëŠ” ì´ë¦„ì˜ íŒíƒ€ì§€ ì†Œì„¤ ì£¼ì¸ê³µ ì„¤ì •ì„ 2-3ì¤„ë¡œ ë§Œë“¤ì–´ì¤˜.\"}\n",
        "    ]\n",
        "    character = pipe(step2_prompt, max_new_tokens=200)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # 3ë‹¨ê³„: í™ë³´ ë¬¸êµ¬\n",
        "    step3_prompt = [\n",
        "        {\"role\": \"user\", \"content\": f\"ë‹¤ìŒ ìºë¦­í„°ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ í™ë³´ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì¤˜:\\n\\n{character}\"}\n",
        "    ]\n",
        "    promo = pipe(step3_prompt, max_new_tokens=200)[0][\"generated_text\"].strip()\n",
        "\n",
        "    results_chain[model_name] = {\n",
        "        \"step1_name\": name,\n",
        "        \"step2_character\": character,\n",
        "        \"step3_promo\": promo\n",
        "    }\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# ë¹„êµ ì¶œë ¥\n",
        "print(\"\\ní”„ë¡¬í”„íŠ¸ ì²´ì¸ ê²°ê³¼:\")\n",
        "for model_name, chain in results_chain.items():\n",
        "    print(f\"\\n[{model_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"1ë‹¨ê³„ (ì´ë¦„): {chain['step1_name']}\")\n",
        "    print(f\"2ë‹¨ê³„ (ìºë¦­í„°): {chain['step2_character']}\")\n",
        "    print(f\"3ë‹¨ê³„ (í™ë³´): {chain['step3_promo']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "56ee5ab57a354641b4c04dcd1a5bf75e",
            "8b7fcbfbebc348b5894831ecf07d6b12",
            "2b2c6a82b0224f4788d9ff06cf81513c",
            "2c87a00bd33f4dee84549c744ffc8a79",
            "9da147912ef24a4baeb4d3cad28d4920",
            "9729f5ed735b406d9e7255c7a3beda57",
            "cc3cb453600b4cbe950298d1be649a9e",
            "8904393e326a421d91f40446cd15e2ba",
            "4b187dc3118149ba90857c817ee35eee",
            "24a2f4e909954fef92b6eaad9148c9da",
            "527b1347918f4d3cbe709f20ba26ab9d",
            "d1eb8e0963524bf295efa6330c69dfd2",
            "fe4dd619adac4d318d2170d4b65a12ef",
            "663202f3e0644055a2fd40b28702ebfd",
            "2df03c83d26a49bba52f997b0be9beab",
            "73552af3ad734864b83591c33aa3f4b1",
            "4617d513c57744fb986b878f69c0b6e9",
            "dc3251f417814716ae51306310024136",
            "d8a671938fdb4f668a42ada93e3e17e0",
            "2f7afae2f44049808273ba6ffc0fef54",
            "d6f9b22a0d724c819fbc07d41be00ef5",
            "6a9159c5e0f8452f873c36b3878d329f"
          ]
        },
        "id": "sJRbnbZcRyf9",
        "outputId": "cfd80ddb-c8bd-4191-f6af-f2bdcf04901a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ee5ab57a354641b4c04dcd1a5bf75e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1eb8e0963524bf295efa6330c69dfd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "í”„ë¡¬í”„íŠ¸ ì²´ì¸ ê²°ê³¼:\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "1ë‹¨ê³„ (ì´ë¦„): ì„œì—°\n",
            "2ë‹¨ê³„ (ìºë¦­í„°): 'ì„œì—°', ê·¸ë¦¼ ê°™ì€ íˆ¬ëª…í•œ í”¼ë¶€ì™€ ê²€ì€ ëˆˆë™ìì— ë¹›ë‚˜ëŠ” íŒŒë€ ë¨¸ë¦¬ì¹´ë½ì´ ë‹ë³´ì´ëŠ” ì†Œë…€. ì„œì—°ì€ ì„¸ìƒì—ì„œ ê°€ì¥ ìˆœìˆ˜í•˜ê³  ë”°ëœ»í•œ ì¡´ì¬ë¡œ, ì–¸ì œë‚˜ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì„ ìœ„í•´ í–‰ë™í•˜ëŠ” ì²œì‚¬ì²˜ëŸ¼ ì•„ë¦„ë‹¤ìš´ ê·¸ë…€ëŠ” íŒíƒ€ì§€ ì„¸ê³„ë¥¼ ë”ìš± ë°ê²Œ ë¹„ì¶”ëŠ” ë¹›ì´ë‹¤.\n",
            "3ë‹¨ê³„ (í™ë³´): \"ì„œì—°: ì²œì‚¬ì˜ ë¯¸ì†Œ, ìˆœìˆ˜í•¨ì˜ íƒ„ìƒ\"\n",
            "ì´ ê´‘ê³  ë¬¸êµ¬ëŠ” ì„œì—°ì´ë¼ëŠ” ìºë¦­í„°ê°€ ê°€ì§€ê³  ìˆëŠ” íŠ¹ì§•ì„ ì˜ ì „ë‹¬í•©ë‹ˆë‹¤. 'ì²œì‚¬ì˜ ë¯¸ì†Œ'ëŠ” ê·¸ë…€ì˜ ë”°ëœ»í•¨ê³¼ ì‚¬ë‘ìŠ¤ëŸ¬ì›€ì„ í‘œí˜„í•˜ë©°, 'ìˆœìˆ˜í•¨ì˜ íƒ„ìƒ'ì€ ê·¸ë…€ê°€ ì„¸ìƒì— ê°€ì ¸ë‹¤ì£¼ëŠ” ì˜í–¥ë ¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ë˜í•œ, \"íŒíƒ€ì§€ ì„¸ê³„ë¥¼ ë”ìš± ë°ê²Œ ë¹„ì¶”ëŠ” ë¹›\"ì´ë¼ëŠ” í‘œí˜„ì€ ì„œì—°ì´ ì£¼ëŠ” ê¸ì •ì ì¸ ì—ë„ˆì§€ë¥¼ ìƒì§•ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            "1ë‹¨ê³„ (ì´ë¦„): ë¯¼ì£¼\n",
            "\n",
            "\n",
            "(Note: The provided solution is a common Korean name, Minju, which means \"democracy\". It's chosen as an example of simplicity and directness in response to the instruction.)\n",
            "2ë‹¨ê³„ (ìºë¦­í„°): ë¯¼ì£¼ëŠ” ê²°ì •ì ì¸ ëª…ì‚¬ë¡œì„œ ë¯¼ì£¼ì£¼ì˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ëª…ì‚¬ì…ë‹ˆë‹¤. í•œêµ­ì—ì„œ ì¼ë³¸ ì–‘í™” ë¬¸ì œì— ëŒ€í•œ ê°œë°©ì ì¸ í‰í™”ì™€ ììœ ë¥¼ ê°€ì§„ ë¯¼ì£¼ì£¼ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ëŸ¬í•œ ì´ë¦„ì„ ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤. ë¯¼ì£¼ì£¼ì˜ì˜ ì¤‘ìš”ì„±ê³¼ ì—­ì‚¬ì  ì˜í–¥ì„ ë‹´ê³  ìˆëŠ” ì´ë¦„ì€ í˜„ëŒ€ ë¬¸í™”ì—ì„œë„ ê·¸ë¦¬ìŠ¤ì–´ ì˜ë¬¸ì—ì„œë„ ë¯¼ì£¼ì£¼ì˜ë¥¼ ì˜ë¯¸í•˜ëŠ” ë‹¨ì–´ì¸ 'D\n",
            "3ë‹¨ê³„ (í™ë³´): \"ë¯¼ì£¼ì£¼ì˜ëŠ” ìš°ë¦¬ ëª¨ë‘ì—ê²Œ í—Œì‹ ê³¼ ë¶ˆì•ˆê±°ë‚˜ ë¶ˆë§Œì„ ì”ë“¤ì§€ ì•ŠëŠ” ë° ì¤‘ìš”í•œ ì›ì¹™ì´ ìˆìŠµë‹ˆë‹¤.\"\n",
            "\n",
            "\n",
            "\"ë¯¼ì£¼ì£¼ì˜ëŠ” êµ­ë¯¼ì˜ í¬ë§ê³¼ í‰í™”ë¥¼ ìœ„í•œ ê¸°ë³¸ì ì¸ ë¬¸í™”ì  ì•„ì´ë””ì–´ì…ë‹ˆë‹¤.\"\n",
            "\n",
            "\n",
            "\"ë¯¼ì£¼ì£¼ì˜ëŠ” ê°œì¸ì˜ ì¡´ê²½ê³¼ ê³µë™ì²´ì˜ ê· í˜•ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ì¤‘ìš”í•œ ëª©ì ì…ë‹ˆë‹¤.\"\n",
            "\n",
            "\n",
            "\"ë¯¼ì£¼ì£¼ì˜ëŠ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoT ì¶”ë¡ \n",
        "\n",
        "- ê²°ê³¼: zeroshot ë³´ë‹¤ one shotì´ ë” íš¨ê³¼ì \n",
        "\n"
      ],
      "metadata": {
        "id": "iJ4aplOWh5U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot CoT\n",
        "zeroshot_cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
        "20ê°œë¥¼ ì ì‹¬ ë§Œë“œëŠ” ë° ì‚¬ìš©í•˜ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´,\n",
        "ë‚¨ì€ ì‚¬ê³¼ëŠ” ëª‡ ê°œì¼ê¹Œìš”?\n",
        "\n",
        "ê³„ì‚° ê³¼ì • ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ ë´…ì‹œë‹¤.\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "# Few-shot CoT\n",
        "fewshot_cot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"ë¡œì €ëŠ” í…Œë‹ˆìŠ¤ê³µ 5ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìº” 2ê°œë¥¼ ë” ìƒ€ê³ , ê° ìº”ì—ëŠ” ê³µì´ 3ê°œì”© ë“¤ì–´ìˆìŠµë‹ˆë‹¤. ì´ì œ ëª‡ ê°œì¼ê¹Œìš”?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"ë¡œì €ëŠ” ê³µ 5ê°œë¡œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ìº” 2ê°œ Ã— ê³µ 3ê°œ = 6ê°œì…ë‹ˆë‹¤. 5 + 6 = 11ê°œì…ë‹ˆë‹¤. ë‹µì€ 11ê°œì…ë‹ˆë‹¤.\"},\n",
        "    {\"role\": \"user\", \"content\": \"ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. 20ê°œë¥¼ ì ì‹¬ ë§Œë“œëŠ” ë° ì‚¬ìš©í•˜ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´, ë‚¨ì€ ì‚¬ê³¼ëŠ” ëª‡ ê°œì¼ê¹Œìš”?\"}\n",
        "]\n",
        "\n",
        "cot_types = {\n",
        "    \"Zero-shot CoT\": zeroshot_cot_prompt,\n",
        "    \"Few-shot CoT\": fewshot_cot_prompt\n",
        "}\n",
        "\n",
        "results_cot = {model_name: {} for model_name in MODELS.keys()}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    for cot_name, prompt in cot_types.items():\n",
        "        output = pipe(prompt)[0][\"generated_text\"]\n",
        "        results_cot[model_name][cot_name] = output\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# ë¹„êµ ì¶œë ¥\n",
        "print(\"\\nCoT ë°©ì‹ë³„ ë¹„êµ:\")\n",
        "for cot_name in cot_types.keys():\n",
        "    print(f\"\\n[{cot_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    for model_name in MODELS.keys():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(results_cot[model_name][cot_name][:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969,
          "referenced_widgets": [
            "23f39583e4a543bab895f9c898c543cc",
            "ca09f57e6cb24659a392b3dfb6c29740",
            "68139d154bd6456281340ebf800b3390",
            "a6a30eb4f5fa4845830d88b312b905b3",
            "210ca520ee6641b29f8c53642ebf9eda",
            "d16d036e031747ebab8591737a577eff",
            "fcb631bc71ec4cb5b44b7ec3545ff219",
            "b33868980f6a4d7d97aca6c20fb4a88a",
            "b273b8bf2a9845ca99a7b77cc7ec4868",
            "00f2835b66864eb9959e5e62538f1b95",
            "a1c1a1867e874fc695ce1b754c52608d",
            "e667d651b80d4ce3b38950d0157f494a",
            "92e11363029a4c78bdea19d7c3464b9d",
            "2dd795abcbfe4690a2d80722b8e565d0",
            "dbba9d3d33eb4a879c281105af6c2e13",
            "ac0d3a0cf1064855b259b332d88feec5",
            "f26bdd4ec2e9463f9de99908bcbdbcb4",
            "3028a7ee8a8748aa9bec65cc15baef16",
            "b52e9815155f471dbded150d9a31bfdd",
            "2a31aeaf557f43f1b180706d0f2fc51f",
            "58c2e66f8a7146f29caa95bb4ae813a2",
            "63f159d65d0d4ad1802031232802eb92"
          ]
        },
        "id": "mIzykiFrSmfG",
        "outputId": "5fcb173f-bc34-4315-8702-c60351608bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23f39583e4a543bab895f9c898c543cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e667d651b80d4ce3b38950d0157f494a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "CoT ë°©ì‹ë³„ ë¹„êµ:\n",
            "\n",
            "[Zero-shot CoT]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Qwen2.5-3B:\n",
            "ë¬¼ë¡ ì…ë‹ˆë‹¤! ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í•´ê²°í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
            "\n",
            "1) **ì´ˆê¸° ìƒíƒœ**: ì‹ë‹¹ì— ìˆëŠ” ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 23ê°œì…ë‹ˆë‹¤.\n",
            "   \n",
            "2) **ì‚¬ê³¼ ì‚¬ìš©**: 20ê°œë¥¼ ì ì‹¬ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
            "   - ë‚¨ì•„ìˆëŠ” ì‚¬ê³¼ì˜ ìˆ˜ = ì´ˆê¸° ìƒíƒœì˜ ì‚¬ê³¼ ìˆ˜ - ì‚¬ìš©í•œ ì‚¬ê³¼ ìˆ˜\n",
            "   - ë‚¨ì•„ìˆëŠ” ì‚¬ê³¼ì˜ ìˆ˜ = 23 - 20 = 3ê°œ\n",
            "   \n",
            "3) **ì¶”ê°€ êµ¬ì…**: ë‚¨ì•„ìˆëŠ” 3ê°œ...\n",
            "\n",
            "Phi-3-mini:\n",
            " 1. ì‹ë‹¹ì—ì„œ ì²˜ìŒ ë“¤ì–´ì™”ì„ ë•Œ ì‚¬ê³¼ì˜ ìˆ˜: 23ê°œ\n",
            "\n",
            "2. ì ì‹¬ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©í–ˆë˜ ì‚¬ê³¼ì˜ ìˆ˜: 20ê°œ\n",
            "\n",
            "3. ìƒˆë¡œìš´ ì‚¬ê³¼ ì¶”ê°€: 6ê°œ\n",
            "\n",
            "4. ì´ ì‚¬ê³¼ì˜ ìˆ˜ ê³„ì‚°: 23ê°œ (ì²« ì¸ë±ìŠ¤) + 6ê°œ (ë‘ ë²ˆì§¸ ì¸ë±ìŠ¤) = 29ê°œ\n",
            "\n",
            "5. ì‚¬ê³¼ê°€ ì‚¬ìš©ëœ ì‚¬ê³¼ ìˆ˜: 20ê°œ\n",
            "\n",
            "6. ë‚¨ì€ ì‚¬ê³¼ì˜ ìˆ˜ ê³„ì‚°: 29ê°œ (ì´ ì‚¬ê³¼ ìˆ˜) - 20ê°œ (ì‚¬ìš©ëœ ì‚¬ê³¼ ìˆ˜) = 9...\n",
            "\n",
            "[Few-shot CoT]\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Qwen2.5-3B:\n",
            "ì²˜ìŒìœ¼ë¡œ ì‹ë‹¹ì— ìˆëŠ” ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 23ê°œì˜€ìŠµë‹ˆë‹¤. ì ì‹¬ì„ ìœ„í•´ 20ê°œë¥¼ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ ë‚¨ì€ ì‚¬ê³¼ëŠ” 23 - 20 = 3ê°œê°€ ë©ë‹ˆë‹¤.\n",
            "\n",
            "ê·¸ ë‹¤ìŒì— 6ê°œë¥¼ ì¶”ê°€ë¡œ ìƒ€ê¸° ë•Œë¬¸ì—, í˜„ì¬ ë‚¨ì•„ìˆëŠ” ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 3 + 6 = 9ê°œê°€ ë©ë‹ˆë‹¤.\n",
            "\n",
            "ë”°ë¼ì„œ, ì§€ê¸ˆ ë‚¨ì•„ìˆëŠ” ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 9ê°œì…ë‹ˆë‹¤....\n",
            "\n",
            "Phi-3-mini:\n",
            " ì‹ë‹¹ì—ì„œ ì´‰êµ¬í•œ ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 23ê°œì˜€ìŠµë‹ˆë‹¤. ì ì‹¬ ë§Œë“¤ê¸°ì— ì‚¬ìš©í•  ì‚¬ê³¼ëŠ” 20ê°œì…ë‹ˆë‹¤. ë‚¨ì€ ì‚¬ê³¼ëŠ” 23 - 20 = 3ê°œì…ë‹ˆë‹¤. ê·¸ëŸ°ë° 6ê°œë¥¼ ë” ìƒ€ìœ¼ë¯€ë¡œ, 3 + 6 = 9ê°œì…ë‹ˆë‹¤. ë‹µì€ 9ê°œì…ë‹ˆë‹¤....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìê¸° ì¼ê´€ì„± (Self-Consistency)"
      ],
      "metadata": {
        "id": "HqYmxLHgbIku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë‹µë³€ ì¶”ì¶œ í•¨ìˆ˜\n",
        "def extract_answer(text):\n",
        "    \"\"\"ì‘ë‹µì—ì„œ ìˆ«ì ì¶”ì¶œ\"\"\"\n",
        "    numbers = re.findall(r'\\d+', text)\n",
        "    # ë¬¸ì œì˜ ìˆ«ì ì œì™¸\n",
        "    filtered = [int(n) for n in numbers if int(n) not in [23, 20, 6]]\n",
        "    return filtered[-1] if filtered else None\n",
        "\n",
        "question = \"\"\"\n",
        "ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
        "20ê°œë¥¼ ì ì‹¬ ë§Œë“œëŠ” ë° ì‚¬ìš©í•˜ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´,\n",
        "ë‚¨ì€ ì‚¬ê³¼ëŠ” ëª‡ ê°œì¼ê¹Œìš”?\n",
        "\"\"\"\n",
        "\n",
        "self_consistency_prompt = [\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "results_consistency = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    answers = []\n",
        "    n_samples = 7\n",
        "\n",
        "    print(f\"\\n[{model_name}] {n_samples}ë²ˆ ìƒ˜í”Œë§ ì¤‘...\")\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        output = pipe(\n",
        "            self_consistency_prompt,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            max_new_tokens=300\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        answer = extract_answer(output)\n",
        "        if answer:\n",
        "            answers.append(answer)\n",
        "            print(f\"  ì‹œë„ {i+1}: {answer}ê°œ\")\n",
        "\n",
        "    # ë‹¤ìˆ˜ê²°\n",
        "    if answers:\n",
        "        vote_counts = Counter(answers)\n",
        "        final_answer, count = vote_counts.most_common(1)[0]\n",
        "        confidence = count / n_samples\n",
        "\n",
        "        results_consistency[model_name] = {\n",
        "            \"answers\": answers,\n",
        "            \"final\": final_answer,\n",
        "            \"confidence\": confidence,\n",
        "            \"votes\": dict(vote_counts)\n",
        "        }\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# ë¹„êµ ì¶œë ¥\n",
        "print(\"\\nìê¸° ì¼ê´€ì„± ê²°ê³¼:\")\n",
        "for model_name, result in results_consistency.items():\n",
        "    print(f\"\\n[{model_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"ìˆ˜ì§‘ëœ ë‹µë³€: {result['answers']}\")\n",
        "    print(f\"ìµœì¢… ë‹µë³€: {result['final']}ê°œ\")\n",
        "    print(f\"ì‹ ë¢°ë„: {result['confidence']:.1%}\")\n",
        "    print(f\"ë“í‘œ ë¶„í¬: {result['votes']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810,
          "referenced_widgets": [
            "c32a1cbee91c494dbbedf6bf81e5d338",
            "d11e9781cbcc401ea49b0b34dbdc936f",
            "366c7bfd6bc842f0b0847273315c5f95",
            "1e7a2e965cb94784b4f9087e732a36cd",
            "9e99dc6d4c914f4a9fab92b51275e897",
            "a13a236e289c4111b5fad062455ae121",
            "f3c36200f95240c095ab85c853243a60",
            "de503b5c20e547a7bac12659d9c6c066",
            "4c863f7ab5c24641a164a77f5c93bbb3",
            "2b6f3a7d24c34259814010d2077cfcad",
            "6914f35b01354a6a988457d3ba2a8f9b",
            "ec71cdb6d8464bc1bc0c5e53d3113f03",
            "40f4dcb4b56d4865910c8060d9dd8c27",
            "ed512287f63c4e52b0ca277dfadb5170",
            "aa114300a2f64d7ea7434590dd643538",
            "9803b13f95f5470c803b93df334405b2",
            "4dd20ab2e8cf437687bbe67a6a2bf199",
            "3db310c49d4f4fffa8cbcf9afa73b844",
            "14206984b9f74e51b5cac8fe7f7ee1d3",
            "f3d67459ff404231af3074d110fa95be",
            "c47f2e9a031545ec9624c8c8873e9998",
            "c408f8bf11d14fb4aa28be5ae187b447"
          ]
        },
        "id": "1kQREUAxbMDp",
        "outputId": "08c38dcd-cd92-46b9-8b55-5653aae42d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c32a1cbee91c494dbbedf6bf81e5d338"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "\n",
            "[Qwen2.5-3B] 7ë²ˆ ìƒ˜í”Œë§ ì¤‘...\n",
            "  ì‹œë„ 1: 9ê°œ\n",
            "  ì‹œë„ 2: 9ê°œ\n",
            "  ì‹œë„ 3: 9ê°œ\n",
            "  ì‹œë„ 4: 9ê°œ\n",
            "  ì‹œë„ 5: 9ê°œ\n",
            "  ì‹œë„ 6: 9ê°œ\n",
            "  ì‹œë„ 7: 3ê°œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec71cdb6d8464bc1bc0c5e53d3113f03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "\n",
            "[Phi-3-mini] 7ë²ˆ ìƒ˜í”Œë§ ì¤‘...\n",
            "  ì‹œë„ 1: 9ê°œ\n",
            "  ì‹œë„ 2: 16ê°œ\n",
            "  ì‹œë„ 3: 26ê°œ\n",
            "  ì‹œë„ 4: 29ê°œ\n",
            "  ì‹œë„ 5: 26ê°œ\n",
            "  ì‹œë„ 6: 9ê°œ\n",
            "  ì‹œë„ 7: 3ê°œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "ìê¸° ì¼ê´€ì„± ê²°ê³¼:\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "ìˆ˜ì§‘ëœ ë‹µë³€: [9, 9, 9, 9, 9, 9, 3]\n",
            "ìµœì¢… ë‹µë³€: 9ê°œ\n",
            "ì‹ ë¢°ë„: 85.7%\n",
            "ë“í‘œ ë¶„í¬: {9: 6, 3: 1}\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            "ìˆ˜ì§‘ëœ ë‹µë³€: [9, 16, 26, 29, 26, 9, 3]\n",
            "ìµœì¢… ë‹µë³€: 9ê°œ\n",
            "ì‹ ë¢°ë„: 28.6%\n",
            "ë“í‘œ ë¶„í¬: {9: 2, 16: 1, 26: 2, 29: 1, 3: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ToT: ì¤‘ê°„ ë‹¨ê³„ íƒìƒ‰"
      ],
      "metadata": {
        "id": "6ywCmJfkh9YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "ì„¸ ëª…ì˜ ì „ë¬¸ê°€ê°€ ì´ ì§ˆë¬¸ì— ë‹µí•œë‹¤ê³  ê°€ì •í•´ ë³´ì„¸ìš”.\n",
        "ëª¨ë“  ì „ë¬¸ê°€ëŠ” ìì‹ ì˜ ìƒê°ì˜ í•œ ë‹¨ê³„ë¥¼ ì ì–´ ê·¸ë£¹ì›ë“¤ê³¼ ê³µìœ í•©ë‹ˆë‹¤.\n",
        "ê·¸ëŸ° ë‹¤ìŒ ëª¨ë“  ì „ë¬¸ê°€ê°€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "ì–´ëŠ ì‹œì ì—ì„œë“  ìì‹ ì´ í‹€ë ¸ë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹«ëŠ” ì „ë¬¸ê°€ê°€ ìˆìœ¼ë©´ ê·¸ ìë¦¬ì—ì„œ ë‚˜ê°‘ë‹ˆë‹¤.\n",
        "\n",
        "ì§ˆë¬¸: \"ì‹ë‹¹ì—ëŠ” ì‚¬ê³¼ 23ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. 20ê°œë¥¼ ì ì‹¬ìœ¼ë¡œ ë§Œë“¤ê³  6ê°œë¥¼ ë” ìƒ€ë‹¤ë©´,\n",
        "ì‚¬ê³¼ëŠ” ëª‡ ê°œì¼ê¹Œìš”?\"\n",
        "\n",
        "ê° ì „ë¬¸ê°€ì˜ ì¶”ë¡  ê³¼ì •ì„ ë³´ì—¬ì£¼ê³ , ìµœì¢… ë‹µì„ ë„ì¶œí•˜ì„¸ìš”.\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "results_tot = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    output = pipe(tot_prompt, max_new_tokens=800)[0][\"generated_text\"]\n",
        "    results_tot[model_name] = output\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# ë¹„êµ ì¶œë ¥\n",
        "print(\"\\nToT ê²°ê³¼:\")\n",
        "for model_name, output in results_tot.items():\n",
        "    print(f\"\\n[{model_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    print(output[:400] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830,
          "referenced_widgets": [
            "4e97cd6aa0f44a50974d52d028c4046c",
            "952ba02866fd408d85471f1d147dbb8a",
            "2cd44d7a303f45ff96e038db98152fce",
            "1882151c4bf84311b51f9ee7c440f0e4",
            "fd63743ce83a428caaa07753a267a87c",
            "5bbb9fe3ea3447ff8814c8fce00bad24",
            "7cb26e8adb0b4612b20c86bbceb7b354",
            "b5cf03b18ee44ef099e4de8e7670c44a",
            "fb994784efeb4cbfb38e6a6ff4d98eb0",
            "d9a80d7c8cdc4d14b7fa5b96f08d505e",
            "808f84685e5e4f2daef6caae18d179b9",
            "2a7c98fe49f14170ae80126a755192b0",
            "a06fb77d893e4bdcba00348ad17fee32",
            "41da21dff34641318068c9727266fa56",
            "6c1bd9b82a684896821724ebbc208131",
            "d99ffa12bc574d3eb26febfccc872ab7",
            "18c05f799a6f45258a40bfcd09fee076",
            "fc6d1561bda9434d9ea5067c7b69d5e0",
            "78f17161bf724a2995a776175c316813",
            "8e7f41bd5ff04168818442854eca6d18",
            "8d3dc042fb5f400fb4bc7af4bc2ae9a4",
            "eebeb97cadd64a2da5dd01b392e08141"
          ]
        },
        "id": "f8T0JiCYUoBM",
        "outputId": "43b17634-7eb0-4fcd-aaa1-7701ed6b6c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e97cd6aa0f44a50974d52d028c4046c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a7c98fe49f14170ae80126a755192b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "ToT ê²°ê³¼:\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "ë„¤, ê° ì „ë¬¸ê°€ê°€ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì„ ë”°ë¼ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
            "\n",
            "### ì „ë¬¸ê°€ A\n",
            "\n",
            "**ë‹¨ê³„ 1:** ì´ˆê¸° ìƒíƒœì—ì„œ ì‚¬ê³¼ëŠ” 23ê°œì…ë‹ˆë‹¤.\n",
            "**ë‹¨ê³„ 2:** 20ê°œë¥¼ ì ì‹¬ìœ¼ë¡œ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ ë‚¨ì€ ìˆ˜ëŸ‰ì€ 23 - 20 = 3ê°œì…ë‹ˆë‹¤.\n",
            "**ë‹¨ê³„ 3:** ê·¸ë¦¬ê³  6ê°œë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ìƒ€ìœ¼ë¯€ë¡œ, í˜„ì¬ì˜ ì‚¬ê³¼ì˜ ì´ ìˆ˜ëŠ” 3 + 6 = 9ê°œì…ë‹ˆë‹¤.\n",
            "\n",
            "**ê²°ë¡ :** ì‚¬ê³¼ëŠ” 9ê°œì…ë‹ˆë‹¤.\n",
            "\n",
            "---\n",
            "\n",
            "### ì „ë¬¸ê°€ B\n",
            "\n",
            "**ë‹¨ê³„ 1:** ì´ˆê¸° ìƒíƒœì—ì„œ ì‚¬ê³¼ëŠ” 23ê°œì…ë‹ˆë‹¤.\n",
            "**ë‹¨ê³„ 2:** 20ê°œë¥¼ ì ì‹¬ìœ¼ë¡œ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ ë‚¨ì€ ìˆ˜ëŸ‰ì€ 23 - 20 = 3ê°œì…ë‹ˆë‹¤.\n",
            "**ë‹¨ê³„ 3:** ê·¸ë¦¬ê³  6ê°œë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ìƒ€ìœ¼ë¯€ë¡œ, í˜„ì¬ì˜ ì‚¬ê³¼ì˜ ì´ ìˆ˜ëŠ” 3 + 6 = 9ê°œì…ë‹ˆë‹¤.\n",
            "\n",
            "**ê²°è®ºï¼ˆæ³¨æ„ï¼šåŸæ–‡ä¸­â€œçµè«–â€åº”ä¸ºâ€œç»“è®ºâ€ï¼‰:** ì‚¬ê³¼ëŠ” 9ê°œì…ë‹ˆë‹¤....\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            " ì „ë¬¸ê°€1: ë¨¼ì € 20ê°œë¥¼ ì ì‹¬ìœ¼ë¡œ ë§Œë“¤ì—ˆìœ¼ë¯€ë¡œ, ë‚¨ì€ ì‚¬ê³¼ëŠ” 23 - 20 = 3ê°œê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ 6ê°œë¥¼ ë” ì¤€ ê²ƒì´ë¯€ë¡œ, 3 + 6 = 9ê°œê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¹ì‹œ ì‹ë‹¹ì— ìˆë˜ ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 9ê°œì˜€ìŠµë‹ˆë‹¤.\n",
            "\n",
            "ì „ë¬¸ê°€2: ì ì‹¬ ë¨¹ê¸° ìœ„í•´ 20ê°œë¥¼ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ, ë‚¨ì€ ì‚¬ê³¼ëŠ” 23 - 20 = 3ê°œê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ 6ê°œë¥¼ ë” ì¤€ ê²ƒì´ë¯€ë¡œ, 3 + 6 = 9ê°œê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¹ì‹œ ì‹ë‹¹ì— ìˆë˜ ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 9ê°œì˜€ìŠµë‹ˆë‹¤.\n",
            "\n",
            "ì „ë¬¸ê°€3: ì ì‹¬ ë¨¹ê¸° ì „ì— 23ê°œì˜ ì‚¬ê³¼ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. 20ê°œë¥¼ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ, ë‚¨ì€ ì‚¬ê³¼ëŠ” 23 - 20 = 3ê°œê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ 6ê°œë¥¼ ë” ì¤€ ê²ƒì´ë¯€ë¡œ, 3 + 6 = 9ê°œê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¹ì‹œ ì‹ë‹¹ì— ìˆë˜ ì‚¬ê³¼ì˜ ìˆ˜ëŠ” 9ê°œì˜€ìŠµë‹ˆë‹¤.\n",
            "\n",
            "ë”°ë¼ì„œ ëª¨ë“  ì „ë¬¸ê°€ëŠ” ë‹µì€ ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì¶œë ¥ ê²€ì¦: ì˜ˆì‹œ ì œê³µ"
      ],
      "metadata": {
        "id": "M_m84Z3DiKKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "json_template = \"\"\"\n",
        "RPG ê²Œì„ì˜ ì§§ì€ ìºë¦­í„° í”„ë¡œí•„ì„ ë§Œë“œì„¸ìš”.\n",
        "ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”:\n",
        "\n",
        "{\n",
        "  \"name\": \"ìºë¦­í„° ì´ë¦„\",\n",
        "  \"class\": \"ì§ì—…\",\n",
        "  \"weapon\": \"ë¬´ê¸°\",\n",
        "  \"skill\": \"íŠ¹ìˆ˜ ëŠ¥ë ¥\"\n",
        "}\n",
        "\n",
        "ì˜ˆì‹œ:\n",
        "{\n",
        "  \"name\": \"ì•„ë¦¬ì•„\",\n",
        "  \"class\": \"ë§ˆë²•ì‚¬\",\n",
        "  \"weapon\": \"ì§€íŒ¡ì´\",\n",
        "  \"skill\": \"í™”ì—¼êµ¬\"\n",
        "}\n",
        "\n",
        "ì´ì œ ì „ì‚¬ ìºë¦­í„°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n",
        "\"\"\"\n",
        "\n",
        "validation_prompt = [\n",
        "    {\"role\": \"user\", \"content\": json_template}\n",
        "]\n",
        "\n",
        "def extract_and_parse_json(output):\n",
        "    \"\"\"ê°œì„ ëœ JSON ì¶”ì¶œ ë° íŒŒì‹±\"\"\"\n",
        "\n",
        "    # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ\n",
        "    patterns = [\n",
        "        r'```(?:json)?\\s*(\\{.*?\\})\\s*```',  # ```json { ... } ```\n",
        "        r'```\\s*(\\{.*?\\})\\s*```',           # ``` { ... } ```\n",
        "        r'\\{.*\\}',                          # ì§ì ‘ JSON ê°ì²´\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, output, re.DOTALL)\n",
        "        if match:\n",
        "            json_candidate = match.group(1) if match.groups() else match.group()\n",
        "\n",
        "            try:\n",
        "                parsed = json.loads(json_candidate.strip())\n",
        "                return parsed, \"âœ… JSON íŒŒì‹± ì„±ê³µ\"\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "    return None, \"âŒ ìœ íš¨í•œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\"\n",
        "\n",
        "# ê²€ì¦ ì½”ë“œ ìˆ˜ì •\n",
        "results_validation = {}\n",
        "\n",
        "for model_name, model_info in MODELS.items():\n",
        "    model, tokenizer, pipe = load_model(model_info[\"id\"])\n",
        "\n",
        "    output = pipe(validation_prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "\n",
        "    # ê°œì„ ëœ JSON íŒŒì‹±\n",
        "    parsed, validation_msg = extract_and_parse_json(output)\n",
        "    is_valid = parsed is not None\n",
        "\n",
        "    if is_valid:\n",
        "        # í•„ìˆ˜ í•„ë“œ ê²€ì¦\n",
        "        required = [\"name\", \"class\", \"weapon\", \"skill\"]\n",
        "        missing = [f for f in required if f not in parsed]\n",
        "        if missing:\n",
        "            validation_msg += f\" (ëˆ„ë½ í•„ë“œ: {missing})\"\n",
        "\n",
        "    results_validation[model_name] = {\n",
        "        \"output\": output,\n",
        "        \"valid\": is_valid,\n",
        "        \"parsed\": parsed,\n",
        "        \"message\": validation_msg\n",
        "    }\n",
        "\n",
        "    cleanup_model(model, tokenizer, pipe)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\nê°œì„ ëœ ì¶œë ¥ ê²€ì¦ ê²°ê³¼:\")\n",
        "for model_name, result in results_validation.items():\n",
        "    print(f\"\\n[{model_name}]\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"ì›ë³¸ ì¶œë ¥:\\n{result['output']}\")\n",
        "    print(f\"\\nê²€ì¦: {result['message']}\")\n",
        "    if result['parsed']:\n",
        "        print(f\"íŒŒì‹± ë°ì´í„°:\\n{json.dumps(result['parsed'], indent=2, ensure_ascii=False)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a659e7f08ec14b538c4d4e36a8105b72",
            "73d7b6b100c74064b04b118bcc24ec9c",
            "f551cfa22fca45109b7f8f83850d7850",
            "6fe342cdb083400daf41789246dd0619",
            "7cfd02dba44b4b8793a880d730fc931f",
            "7b26a1ba95e04eeaade59c1336bed1f4",
            "acd00bc9350146378b11fee6ee8cfa60",
            "74ac541cecb2424e8a5537308037d6ef",
            "79b41b215dcf4015a369ce522788339e",
            "f8e6895efe0b44a9897cd20644f889a5",
            "0b734aacb88b40e096825bb9a3709fb1",
            "e9b67d899c344f6888550ce02c5cf045",
            "5bc51e547f0841a0949a65c57aabf213",
            "5978a1f9cbcb42989710a513a6be4c10",
            "aa67328e95a542658a33fdfcd4dd6865",
            "d5c3e15992c74ad6bf1101e67223312f",
            "ea70375dd1f64bdfaa48cccddb1741d8",
            "39c5d883e68f4d1da3c9999a105003bf",
            "7da94fbe61134809b520599bc1803274",
            "f99aa74291c14444a439f09ee2acd555",
            "5a4d7b9cf49b4aa298878d5a5ab99208",
            "4b9dfd933da1453298654b8b94766614"
          ]
        },
        "id": "H87k60khWc_L",
        "outputId": "af604989-3dbd-402f-8fb7-f7561ffa8fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: Qwen/Qwen2.5-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a659e7f08ec14b538c4d4e36a8105b72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "â³ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9b67d899c344f6888550ce02c5cf045"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ\n",
            "ğŸ‘Œ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
            "\n",
            "ê°œì„ ëœ ì¶œë ¥ ê²€ì¦ ê²°ê³¼:\n",
            "\n",
            "[Qwen2.5-3B]\n",
            "----------------------------------------------------------------------\n",
            "ì›ë³¸ ì¶œë ¥:\n",
            "{\n",
            "  \"name\": \"ë¸Œë£¨ìŠ¤\",\n",
            "  \"class\": \"ì „ì‚¬\",\n",
            "  \"weapon\": \"ê²€\",\n",
            "  \"skill\": \"ëŒ€í˜• ì¹¼ë‚  ë˜ì§ˆê¸°\"\n",
            "}\n",
            "\n",
            "ê²€ì¦: âœ… JSON íŒŒì‹± ì„±ê³µ\n",
            "íŒŒì‹± ë°ì´í„°:\n",
            "{\n",
            "  \"name\": \"ë¸Œë£¨ìŠ¤\",\n",
            "  \"class\": \"ì „ì‚¬\",\n",
            "  \"weapon\": \"ê²€\",\n",
            "  \"skill\": \"ëŒ€í˜• ì¹¼ë‚  ë˜ì§ˆê¸°\"\n",
            "}\n",
            "\n",
            "[Phi-3-mini]\n",
            "----------------------------------------------------------------------\n",
            "ì›ë³¸ ì¶œë ¥:\n",
            " ```json\n",
            "\n",
            "{\n",
            "\n",
            "  \"name\": \"ì „ì‚¬ ë¯¼ìˆ˜\",\n",
            "  \"class\": \"ì „ì‚¬\",\n",
            "  \"weapon\": \"ì§±êµ¬\",\n",
            "  \"skill\": \"ë°©ì–´ìœµí•©\"\n",
            "\n",
            "}\n",
            "\n",
            "```\n",
            "\n",
            "ê²€ì¦: âœ… JSON íŒŒì‹± ì„±ê³µ\n",
            "íŒŒì‹± ë°ì´í„°:\n",
            "{\n",
            "  \"name\": \"ì „ì‚¬ ë¯¼ìˆ˜\",\n",
            "  \"class\": \"ì „ì‚¬\",\n",
            "  \"weapon\": \"ì§±êµ¬\",\n",
            "  \"skill\": \"ë°©ì–´ìœµí•©\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ji3sGC5NhihD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ìµœì¢… ì¢…í•© ë¹„êµ\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_table = {\n",
        "    \"ëª¨ë¸\": list(MODELS.keys()),\n",
        "    \"í¬ê¸°\": [MODELS[m][\"size\"] for m in MODELS.keys()],\n",
        "    \"ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ ì†ë„\": [f\"{results_complex[m]['time']:.2f}ì´ˆ\" for m in MODELS.keys()],\n",
        "    \"ìê¸°ì¼ê´€ì„± ì‹ ë¢°ë„\": [f\"{results_consistency[m]['confidence']:.1%}\" for m in MODELS.keys()],\n",
        "    \"JSON ê²€ì¦\": [results_validation[m]['message'] for m in MODELS.keys()],\n",
        "}\n",
        "\n",
        "# í‘œ í˜•ì‹ ì¶œë ¥\n",
        "print(\"\\n\")\n",
        "for key, values in comparison_table.items():\n",
        "    print(f\"{key:20s}\", end=\"\")\n",
        "    for v in values:\n",
        "        print(f\"{str(v):25s}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ê²°ë¡ \")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ì†ë„ ë¹„êµ\n",
        "faster_model = min(MODELS.keys(), key=lambda m: results_complex[m]['time'])\n",
        "print(f\"ì†ë„: {faster_model} ìš°ì„¸\")\n",
        "\n",
        "# ì‹ ë¢°ë„ ë¹„êµ\n",
        "reliable_model = max(MODELS.keys(), key=lambda m: results_consistency[m]['confidence'])\n",
        "print(f\"ì¼ê´€ì„±: {reliable_model} ìš°ì„¸\")\n",
        "\n",
        "print(\"\\nê° ëª¨ë¸ì˜ íŠ¹ì§•:\")\n",
        "for model_name, info in MODELS.items():\n",
        "    print(f\"  â€¢ {model_name}: {info['description']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiAUNvDe4Jc9",
        "outputId": "bfc882f0-e58e-41fd-ed1b-f5ceb8f1e04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìµœì¢… ì¢…í•© ë¹„êµ\n",
            "======================================================================\n",
            "\n",
            "\n",
            "ëª¨ë¸                  Qwen2.5-3B               Phi-3-mini               \n",
            "í¬ê¸°                  3B                       3.8B                     \n",
            "ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ ì†ë„         7.00ì´ˆ                    16.52ì´ˆ                   \n",
            "ìê¸°ì¼ê´€ì„± ì‹ ë¢°ë„           85.7%                    28.6%                    \n",
            "JSON ê²€ì¦             âœ… JSON íŒŒì‹± ì„±ê³µ             âœ… JSON íŒŒì‹± ì„±ê³µ             \n",
            "\n",
            "======================================================================\n",
            "ê²°ë¡ \n",
            "======================================================================\n",
            "ì†ë„: Qwen2.5-3B ìš°ì„¸\n",
            "ì¼ê´€ì„±: Qwen2.5-3B ìš°ì„¸\n",
            "\n",
            "ê° ëª¨ë¸ì˜ íŠ¹ì§•:\n",
            "  â€¢ Qwen2.5-3B: ì†Œí˜•, ë¹ ë¥¸ ì¶”ë¡ \n",
            "  â€¢ Phi-3-mini: ì¤‘ì†Œí˜•, MS ëª¨ë¸, êµì•ˆ ë©”ì¸\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ëª¨ë¸ ë¹„êµ ë¶„ì„ ê²°ê³¼**\n",
        "\n",
        "## ì„±ëŠ¥ ë¹„êµ\n",
        "\n",
        "| í•­ëª© | Qwen2.5-3B | Phi-3-mini |\n",
        "|------|------------|------------|\n",
        "| í¬ê¸° | 3B | 3.8B |\n",
        "| ì†ë„ | 7.00ì´ˆ | 16.52ì´ˆ |\n",
        "| ì¼ê´€ì„± | 85.7% | 28.6% |\n",
        "| JSON ê²€ì¦ | ì„±ê³µ | ì„±ê³µ |\n",
        "\n",
        "---\n",
        "\n",
        "## ì£¼ìš” ë°œê²¬\n",
        "\n",
        "### ì†ë„\n",
        "- Qwenì´ 2.36ë°° ë¹ ë¦„\n",
        "- ì‘ì€ ëª¨ë¸ì´ ë” ë¹ ë¥¸ ì—­ì„¤\n",
        "\n",
        "### ì¼ê´€ì„±\n",
        "- Qwen: 7íšŒ ì‹œë„ ì¤‘ 6íšŒ ì¼ì¹˜\n",
        "- Phi-3: 7íšŒ ì¤‘ 2íšŒë§Œ ì¼ì¹˜\n",
        "- ì‹ ë¢°ë„ ì°¨ì´ 3ë°°\n",
        "\n",
        "### êµ¬ì¡°í™” ì¶œë ¥\n",
        "- ë‘˜ ë‹¤ JSON ìƒì„± ì„±ê³µ\n",
        "\n",
        "---\n",
        "\n",
        "## ì›ì¸ ë¶„ì„\n",
        "\n",
        "### Qwen ìš°ì„¸ ì´ìœ \n",
        "- ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì € (í•œêµ­ì–´ ìµœì í™”)\n",
        "- íš¨ìœ¨ì  ì•„í‚¤í…ì²˜\n",
        "- ì•ˆì •ì  í•™ìŠµ ë°ì´í„°\n",
        "\n",
        "### Phi-3 ë¶€ì§„ ì´ìœ \n",
        "- ì˜ì–´ ì¤‘ì‹¬ ì„¤ê³„\n",
        "- í•œêµ­ì–´ í† í° ì²˜ë¦¬ ë¹„íš¨ìœ¨\n",
        "- ì¶œë ¥ ë³€ë™ì„± ë†’ìŒ\n",
        "\n",
        "---\n",
        "\n",
        "## í…ŒìŠ¤íŠ¸ í•œê³„\n",
        "\n",
        "### í•œêµ­ì–´ ì¤‘ì‹¬ í‰ê°€\n",
        "- Phi-3ëŠ” ë³¸ë˜ ì˜ì–´ì— ê°•í•¨\n",
        "- ì–¸ì–´ ì°¨ì´ë¡œ ì¸í•œ ë¶ˆë¦¬\n",
        "- ì˜ì–´ í…ŒìŠ¤íŠ¸ ì‹œ ë‹¤ë¥¸ ê²°ê³¼ ê°€ëŠ¥\n",
        "\n",
        "---\n",
        "\n",
        "## ê²°ë¡ \n",
        "\n",
        "### Qwen2.5-3B ì í•© ìƒí™©\n",
        "- ì˜ì–´ ì™¸ ë‹¤êµ­ì–´ ì„œë¹„ìŠ¤(í•œêµ­ì–´ í¬í•¨)\n",
        "- ë¹ ë¥¸ ì‘ë‹µ ì†ë„ í•„ìš”\n",
        "- ì¼ê´€ëœ ì¶œë ¥ í•„ìˆ˜\n",
        "- ë¹„ìš© íš¨ìœ¨ ì¤‘ì‹œ\n",
        "\n",
        "### Phi-3-mini ì í•© ìƒí™©\n",
        "- ì˜ì–´ ì„œë¹„ìŠ¤\n",
        "- MS ìƒíƒœê³„\n",
        "- êµìœ¡/ì—°êµ¬\n",
        "\n",
        "---\n",
        "\n",
        "## ìš”ì•½\n",
        "\n",
        "í•œêµ­ì–´ íƒœìŠ¤í¬ì—ì„œ Qwen2.5-3Bê°€ ì†ë„ì™€ ì¼ê´€ì„± ëª¨ë‘ ìš°ì„¸.\n",
        "ë‹¨, ì˜ì–´ ì„œë¹„ìŠ¤ì—ì„œëŠ” Phi-3ë„ ê²½ìŸë ¥ ìˆì„ ìˆ˜ ìˆìŒ."
      ],
      "metadata": {
        "id": "jYJsIZ9YcpC-"
      }
    }
  ]
}